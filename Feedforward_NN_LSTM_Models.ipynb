{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook two versions of the feedforward NN / MLP and the LSTM model are applied. \n",
    "The feedforward NN versions comprise a model which only uses one taxi district as input to predict the future demand (referred to as \"SingleMLP\") and a model which uses multiple taxi districts as input (referred to as \"ComplexMLP\"). The same concept applies to the LSTM models: The \"SingleLSTM\" only processes data of a particular taxi district while the \"MultivarLSTM\" processes multiple districts at the same time to make predictions for multiple areas. The classes of the models which contain all methods necessary to process the data and train the models are imported.\n",
    "\n",
    "The idea of each class is to put all required preprocessing steps and the training of the model at one place. Additionally, pre-trained models can be \"loaded\" into the class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T20:48:17.810955Z",
     "start_time": "2019-10-24T20:48:15.852047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#import keras specific functions for storing and loading models\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "#load custom deep Models (LSTM, MLP)\n",
    "from custom_deepmodels import SingleLSTM\n",
    "\n",
    "from multivar_lstm import MultivariateLSTM \n",
    "\n",
    "from complex_mlp import ComplexMLP\n",
    "\n",
    "from single_mlp import SingleMLP\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T20:48:21.345299Z",
     "start_time": "2019-10-24T20:48:18.088194Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date  1  2  3    4  5  6   7  8  9  ...  254  255  256  257  \\\n",
      "0 2009-01-01 05:00:00  0  0  0   91  0  0  30  0  0  ...    0   50   39    3   \n",
      "1 2009-01-01 06:00:00  1  0  0  105  0  0  62  0  0  ...    0   77   67    5   \n",
      "2 2009-01-01 07:00:00  0  0  0   96  0  0  79  0  0  ...    0   90   83    4   \n",
      "3 2009-01-01 08:00:00  0  0  0   91  0  0  84  0  0  ...    0   54   77    3   \n",
      "4 2009-01-01 09:00:00  2  0  0   82  0  0  85  0  1  ...    0   66   54    4   \n",
      "\n",
      "   258  259  260  261  262  263  \n",
      "0    1    0    3   52  127  326  \n",
      "1    0    0   15   65  166  476  \n",
      "2    0    0   19   39  125  460  \n",
      "3    1    0   19   54   79  313  \n",
      "4    0    0   13   24   47  224  \n",
      "\n",
      "[5 rows x 264 columns]\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "TRAIN_PATH = '/media/...'\n",
    "Store_PATH = '/media/...'\n",
    "file_final = 'preprocessed_taxidemand.csv'\n",
    "\n",
    "df_m = pd.read_csv(TRAIN_PATH + file_final, header=0)\n",
    "\n",
    "#convert to datetime format:\n",
    "df_m['date'] = pd.to_datetime(df_m['date'], utc=True)\n",
    "df_m['date'] = df_m['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_m['date'] = pd.to_datetime(df_m['date'])\n",
    "#df_m = df_m.set_index(\"date\") -> set index later, since we need \"date\" column to find highest demand columns..\n",
    "print(df_m.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T20:48:25.466937Z",
     "start_time": "2019-10-24T20:48:25.457612Z"
    }
   },
   "outputs": [],
   "source": [
    "'''filter areas with highest demand '''\n",
    "#get time series with highest \"demand patterns\":\n",
    "\n",
    "#function filters nlargest areas:\n",
    "def get_nlargest_areas(nlargest, org_dataset = df_m):\n",
    "    \n",
    "    #get time series with highest \"demand patterns\":\n",
    "    df_sum = org_dataset.copy(deep=True).drop(columns=[\"date\"],axis=1)\n",
    "    df_sum = df_sum.sum(axis=0,numeric_only=True)\n",
    "\n",
    "    #store nlargest values:\n",
    "    df_sum = df_sum.nlargest(nlargest) \n",
    "    idx_filter = list(df_sum.index.values)\n",
    "    #append \"date\" column\n",
    "    idx_filter.append(\"date\")\n",
    "\n",
    "    del df_sum\n",
    "    \n",
    "    #filter columns with largest values:\n",
    "    ts_largest = org_dataset[idx_filter].copy(deep=True)\n",
    "    ts_largest = ts_largest.set_index(\"date\")\n",
    "\n",
    "    #shift datetimeindex to use local NYC time not UTC:\n",
    "    ts_largest.index = ts_largest.index.shift(-5,freq='H')\n",
    "\n",
    "    return ts_largest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T20:48:26.450304Z",
     "start_time": "2019-10-24T20:48:26.016661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 largest areas:  (83231, 20)\n",
      "10 largest areas:  (83231, 10)\n"
     ]
    }
   ],
   "source": [
    "ts_20largest = get_nlargest_areas(20)\n",
    "ts_10largest = get_nlargest_areas(10)\n",
    "\n",
    "print('20 largest areas: ', ts_20largest.shape)\n",
    "print('10 largest areas: ', ts_10largest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T20:48:37.685777Z",
     "start_time": "2019-10-24T20:48:37.674571Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model_from_disk(model_type):\n",
    "    \n",
    "    '''\n",
    "    Returns pre-trained model from disk\n",
    "    '''\n",
    "\n",
    "\n",
    "    model_PATH = '/media/...'\n",
    "\n",
    "    #complex MLP:\n",
    "    model_architecture_complex_MLP_PATH = '/media/...'\n",
    "    complex_MLP_model_file = 'complex_MLP_early_stopping_W168_20areas__y2012.json'\n",
    "    complex_MLP_weights = 'complex_MLP_early_stopping_W168_20areas__y2012_weights.h5'\n",
    "    \n",
    "    \n",
    "    #multivar LSTM without additional features:\n",
    "    model_architecture_multivar_LSTM_PATH = '/media/...'\n",
    "    multivar_LSTM_file = 'multivar_LSTM_W168_20areas__y2012.json'\n",
    "    multivar_LSTM_weights = 'multivar_LSTM_W168_20areas__y2012_weights.h5'\n",
    "\n",
    "\n",
    "    \n",
    "    instances_dict = {'SingleMLP': (),\n",
    "                  'SingleLSTM': (),\n",
    "                  'ComplexMLP': (complex_MLP_model_file, model_architecture_complex_MLP_PATH, complex_MLP_weights),\n",
    "                  'MultivarLSTM': (multivar_LSTM_file, model_architecture_multivar_LSTM_PATH, multivar_LSTM_weights)\n",
    "\n",
    "                 }\n",
    "    \n",
    "       \n",
    "    \n",
    "\n",
    "    #load complexMLP model 20largest areas:\n",
    "    json_file = open(model_PATH + instances_dict[model_type][0], 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    prediction_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    #load weights of best model:\n",
    "    prediction_model.load_weights(instances_dict[model_type][1] + instances_dict[model_type][2])\n",
    "\n",
    "\n",
    "    \n",
    "    return prediction_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SingleLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:01:43.819170Z",
     "start_time": "2019-10-24T22:22:25.369563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years selected:\n",
      "start_validation_set_year  2011\n",
      "start_test_set_year  2012\n",
      "end_validation_set_year  None\n",
      "end_test_set_year  None\n",
      "##\n",
      "ts diff shape:  (52415, 169)\n",
      "Train/Test Split...\n",
      "generate data..\n",
      "start_validation_set_year:  2011\n",
      "end_validation_set_year:  2011\n",
      "start_test_set_year:  2012\n",
      "end_test_set_year:  2012\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Reshape data for LSTM model...\n",
      "X_train shape before modeling:  (17351, 168, 1)\n",
      "X_valid shape before modeling:  (8760, 168, 1)\n",
      "X_test shape before modeling:  (8784, 168, 1)\n",
      "y_train shape before modeling:  (17351,)\n",
      "y_valid shape before modeling:  (8760,)\n",
      "y_test shape before modeling:  (8784,)\n",
      "scaler type:  MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
      "create stacked LSTM 2 layer non-stateful model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Regular 2H-LSTM Model is created...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 17s 976us/step - loss: 0.0420 - mean_absolute_error: 0.1507 - val_loss: 0.0428 - val_mean_absolute_error: 0.1490\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04276, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0352 - mean_absolute_error: 0.1320 - val_loss: 0.0376 - val_mean_absolute_error: 0.1355\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04276 to 0.03759, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 15s 856us/step - loss: 0.0322 - mean_absolute_error: 0.1284 - val_loss: 0.0355 - val_mean_absolute_error: 0.1322\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03759 to 0.03547, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0305 - mean_absolute_error: 0.1269 - val_loss: 0.0318 - val_mean_absolute_error: 0.1255\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03547 to 0.03182, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0295 - mean_absolute_error: 0.1253 - val_loss: 0.0311 - val_mean_absolute_error: 0.1239\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03182 to 0.03107, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 15s 876us/step - loss: 0.0282 - mean_absolute_error: 0.1228 - val_loss: 0.0321 - val_mean_absolute_error: 0.1289\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03107\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0255 - mean_absolute_error: 0.1161 - val_loss: 0.0267 - val_mean_absolute_error: 0.1179\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03107 to 0.02667, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0232 - mean_absolute_error: 0.1120 - val_loss: 0.0286 - val_mean_absolute_error: 0.1230\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02667\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0221 - mean_absolute_error: 0.1082 - val_loss: 0.0243 - val_mean_absolute_error: 0.1116\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02667 to 0.02435, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0220 - mean_absolute_error: 0.1073 - val_loss: 0.0217 - val_mean_absolute_error: 0.1092\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02435 to 0.02170, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0201 - mean_absolute_error: 0.1036 - val_loss: 0.0208 - val_mean_absolute_error: 0.1052\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02170 to 0.02076, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0191 - mean_absolute_error: 0.1010 - val_loss: 0.0203 - val_mean_absolute_error: 0.1040\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02076 to 0.02034, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0188 - mean_absolute_error: 0.0997 - val_loss: 0.0195 - val_mean_absolute_error: 0.1013\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02034 to 0.01949, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0191 - mean_absolute_error: 0.0998 - val_loss: 0.0208 - val_mean_absolute_error: 0.1071\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01949\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0182 - mean_absolute_error: 0.0980 - val_loss: 0.0191 - val_mean_absolute_error: 0.0996\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01949 to 0.01913, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 15s 854us/step - loss: 0.0176 - mean_absolute_error: 0.0962 - val_loss: 0.0186 - val_mean_absolute_error: 0.0976\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01913 to 0.01858, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0176 - mean_absolute_error: 0.0959 - val_loss: 0.0181 - val_mean_absolute_error: 0.0989\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01858 to 0.01813, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0166 - mean_absolute_error: 0.0933 - val_loss: 0.0179 - val_mean_absolute_error: 0.0991\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01813 to 0.01791, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0163 - mean_absolute_error: 0.0922 - val_loss: 0.0175 - val_mean_absolute_error: 0.0979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01791 to 0.01746, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0154 - mean_absolute_error: 0.0906 - val_loss: 0.0162 - val_mean_absolute_error: 0.0934\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01746 to 0.01623, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0148 - mean_absolute_error: 0.0882 - val_loss: 0.0155 - val_mean_absolute_error: 0.0914\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01623 to 0.01548, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0144 - mean_absolute_error: 0.0871 - val_loss: 0.0157 - val_mean_absolute_error: 0.0909\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01548\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0139 - mean_absolute_error: 0.0854 - val_loss: 0.0144 - val_mean_absolute_error: 0.0882\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01548 to 0.01445, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 15s 876us/step - loss: 0.0143 - mean_absolute_error: 0.0866 - val_loss: 0.0148 - val_mean_absolute_error: 0.0913\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01445\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0134 - mean_absolute_error: 0.0840 - val_loss: 0.0130 - val_mean_absolute_error: 0.0846\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01445 to 0.01303, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0128 - mean_absolute_error: 0.0819 - val_loss: 0.0130 - val_mean_absolute_error: 0.0843\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01303\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0126 - mean_absolute_error: 0.0809 - val_loss: 0.0118 - val_mean_absolute_error: 0.0805\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01303 to 0.01183, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0126 - mean_absolute_error: 0.0804 - val_loss: 0.0122 - val_mean_absolute_error: 0.0807\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01183\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0117 - mean_absolute_error: 0.0776 - val_loss: 0.0110 - val_mean_absolute_error: 0.0768\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01183 to 0.01102, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0114 - mean_absolute_error: 0.0769 - val_loss: 0.0107 - val_mean_absolute_error: 0.0746\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01102 to 0.01067, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 15s 853us/step - loss: 0.0117 - mean_absolute_error: 0.0770 - val_loss: 0.0106 - val_mean_absolute_error: 0.0749\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01067 to 0.01062, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0116 - mean_absolute_error: 0.0770 - val_loss: 0.0120 - val_mean_absolute_error: 0.0818\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01062\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0114 - mean_absolute_error: 0.0761 - val_loss: 0.0108 - val_mean_absolute_error: 0.0756\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.01062\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0104 - mean_absolute_error: 0.0728 - val_loss: 0.0099 - val_mean_absolute_error: 0.0728\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01062 to 0.00991, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0104 - mean_absolute_error: 0.0724 - val_loss: 0.0100 - val_mean_absolute_error: 0.0735\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00991\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0105 - mean_absolute_error: 0.0728 - val_loss: 0.0101 - val_mean_absolute_error: 0.0737\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00991\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0102 - mean_absolute_error: 0.0720 - val_loss: 0.0094 - val_mean_absolute_error: 0.0713\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00991 to 0.00942, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0101 - mean_absolute_error: 0.0714 - val_loss: 0.0100 - val_mean_absolute_error: 0.0724\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00942\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0111 - mean_absolute_error: 0.0752 - val_loss: 0.0092 - val_mean_absolute_error: 0.0703\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00942 to 0.00923, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0100 - mean_absolute_error: 0.0715 - val_loss: 0.0102 - val_mean_absolute_error: 0.0747\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00923\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0096 - mean_absolute_error: 0.0706 - val_loss: 0.0093 - val_mean_absolute_error: 0.0705\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00923\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0096 - mean_absolute_error: 0.0699 - val_loss: 0.0105 - val_mean_absolute_error: 0.0748\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00923\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0099 - mean_absolute_error: 0.0709 - val_loss: 0.0095 - val_mean_absolute_error: 0.0712\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00923\n",
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 15s 879us/step - loss: 0.0092 - mean_absolute_error: 0.0688 - val_loss: 0.0095 - val_mean_absolute_error: 0.0703\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00923\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0090 - mean_absolute_error: 0.0677 - val_loss: 0.0089 - val_mean_absolute_error: 0.0697\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00923 to 0.00895, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 15s 878us/step - loss: 0.0094 - mean_absolute_error: 0.0687 - val_loss: 0.0093 - val_mean_absolute_error: 0.0701\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00895\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0096 - mean_absolute_error: 0.0696 - val_loss: 0.0105 - val_mean_absolute_error: 0.0728\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00895\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0094 - mean_absolute_error: 0.0691 - val_loss: 0.0095 - val_mean_absolute_error: 0.0729\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00895\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 15s 858us/step - loss: 0.0090 - mean_absolute_error: 0.0677 - val_loss: 0.0095 - val_mean_absolute_error: 0.0706\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00895\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0092 - mean_absolute_error: 0.0680 - val_loss: 0.0087 - val_mean_absolute_error: 0.0669\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00895 to 0.00866, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0090 - mean_absolute_error: 0.0672 - val_loss: 0.0085 - val_mean_absolute_error: 0.0670\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00866 to 0.00845, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0087 - mean_absolute_error: 0.0663 - val_loss: 0.0087 - val_mean_absolute_error: 0.0676\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00845\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0088 - mean_absolute_error: 0.0665 - val_loss: 0.0085 - val_mean_absolute_error: 0.0670\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00845\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0087 - mean_absolute_error: 0.0666 - val_loss: 0.0084 - val_mean_absolute_error: 0.0668\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00845 to 0.00841, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 15s 858us/step - loss: 0.0085 - mean_absolute_error: 0.0654 - val_loss: 0.0085 - val_mean_absolute_error: 0.0675\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00841\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0085 - mean_absolute_error: 0.0654 - val_loss: 0.0088 - val_mean_absolute_error: 0.0677\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00841\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 15s 857us/step - loss: 0.0088 - mean_absolute_error: 0.0663 - val_loss: 0.0085 - val_mean_absolute_error: 0.0672\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00841\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0085 - mean_absolute_error: 0.0653 - val_loss: 0.0082 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00841 to 0.00820, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0087 - mean_absolute_error: 0.0663 - val_loss: 0.0086 - val_mean_absolute_error: 0.0677\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00820\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 15s 875us/step - loss: 0.0084 - mean_absolute_error: 0.0645 - val_loss: 0.0083 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00820\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0084 - mean_absolute_error: 0.0651 - val_loss: 0.0082 - val_mean_absolute_error: 0.0659\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00820 to 0.00818, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0083 - mean_absolute_error: 0.0644 - val_loss: 0.0076 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00818 to 0.00764, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0082 - mean_absolute_error: 0.0637 - val_loss: 0.0082 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00764\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0082 - mean_absolute_error: 0.0641 - val_loss: 0.0077 - val_mean_absolute_error: 0.0633\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00764\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0086 - mean_absolute_error: 0.0648 - val_loss: 0.0080 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00764\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 15s 857us/step - loss: 0.0082 - mean_absolute_error: 0.0641 - val_loss: 0.0083 - val_mean_absolute_error: 0.0665\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00764\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0080 - mean_absolute_error: 0.0637 - val_loss: 0.0080 - val_mean_absolute_error: 0.0649\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00764\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 15s 874us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0088 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00764\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0080 - mean_absolute_error: 0.0633 - val_loss: 0.0077 - val_mean_absolute_error: 0.0633\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00764\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 15s 859us/step - loss: 0.0079 - mean_absolute_error: 0.0634 - val_loss: 0.0075 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00764 to 0.00749, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0079 - mean_absolute_error: 0.0630 - val_loss: 0.0083 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00749\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0080 - mean_absolute_error: 0.0636 - val_loss: 0.0083 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00749\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0080 - mean_absolute_error: 0.0636 - val_loss: 0.0079 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00749\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0077 - mean_absolute_error: 0.0623 - val_loss: 0.0082 - val_mean_absolute_error: 0.0655\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00749\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0078 - mean_absolute_error: 0.0626 - val_loss: 0.0076 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00749\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0080 - mean_absolute_error: 0.0634 - val_loss: 0.0078 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00749\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0077 - mean_absolute_error: 0.0622 - val_loss: 0.0074 - val_mean_absolute_error: 0.0624\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00749 to 0.00739, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0077 - mean_absolute_error: 0.0621 - val_loss: 0.0072 - val_mean_absolute_error: 0.0614\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.00739 to 0.00722, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0077 - mean_absolute_error: 0.0616 - val_loss: 0.0077 - val_mean_absolute_error: 0.0641\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00722\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0075 - mean_absolute_error: 0.0612 - val_loss: 0.0075 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00722\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0076 - mean_absolute_error: 0.0618 - val_loss: 0.0080 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00722\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 15s 859us/step - loss: 0.0076 - mean_absolute_error: 0.0612 - val_loss: 0.0074 - val_mean_absolute_error: 0.0618\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00722\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0075 - mean_absolute_error: 0.0611 - val_loss: 0.0076 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00722\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0075 - mean_absolute_error: 0.0613 - val_loss: 0.0076 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00722\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0077 - mean_absolute_error: 0.0622 - val_loss: 0.0075 - val_mean_absolute_error: 0.0625\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00722\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0077 - mean_absolute_error: 0.0615 - val_loss: 0.0071 - val_mean_absolute_error: 0.0608\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00722 to 0.00711, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0079 - mean_absolute_error: 0.0628 - val_loss: 0.0092 - val_mean_absolute_error: 0.0682\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00711\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0077 - mean_absolute_error: 0.0616 - val_loss: 0.0076 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00711\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0075 - mean_absolute_error: 0.0609 - val_loss: 0.0084 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00711\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0074 - mean_absolute_error: 0.0611 - val_loss: 0.0076 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00711\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0073 - mean_absolute_error: 0.0606 - val_loss: 0.0077 - val_mean_absolute_error: 0.0628\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00711\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 15s 874us/step - loss: 0.0073 - mean_absolute_error: 0.0602 - val_loss: 0.0076 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00711\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0072 - mean_absolute_error: 0.0600 - val_loss: 0.0078 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00711\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0073 - mean_absolute_error: 0.0605 - val_loss: 0.0072 - val_mean_absolute_error: 0.0610\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00711\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 15s 859us/step - loss: 0.0072 - mean_absolute_error: 0.0602 - val_loss: 0.0072 - val_mean_absolute_error: 0.0609\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00711\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0073 - mean_absolute_error: 0.0602 - val_loss: 0.0074 - val_mean_absolute_error: 0.0626\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00711\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0072 - val_mean_absolute_error: 0.0613\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00711\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 15s 857us/step - loss: 0.0073 - mean_absolute_error: 0.0603 - val_loss: 0.0073 - val_mean_absolute_error: 0.0619\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00711\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0069 - val_mean_absolute_error: 0.0600\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.00711 to 0.00693, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0072 - mean_absolute_error: 0.0600 - val_loss: 0.0074 - val_mean_absolute_error: 0.0627\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00693\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0107 - mean_absolute_error: 0.0656 - val_loss: 0.0887 - val_mean_absolute_error: 0.2501\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00693\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 15s 875us/step - loss: 0.0302 - mean_absolute_error: 0.1245 - val_loss: 0.0134 - val_mean_absolute_error: 0.0841\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00693\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0120 - mean_absolute_error: 0.0784 - val_loss: 0.0105 - val_mean_absolute_error: 0.0747\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00693\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 15s 872us/step - loss: 0.0100 - mean_absolute_error: 0.0716 - val_loss: 0.0083 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00693\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0089 - mean_absolute_error: 0.0671 - val_loss: 0.0083 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00693\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0086 - mean_absolute_error: 0.0662 - val_loss: 0.0081 - val_mean_absolute_error: 0.0663\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00693\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 15s 880us/step - loss: 0.0085 - mean_absolute_error: 0.0656 - val_loss: 0.0082 - val_mean_absolute_error: 0.0659\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00693\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0082 - mean_absolute_error: 0.0645 - val_loss: 0.0078 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00693\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0083 - mean_absolute_error: 0.0646 - val_loss: 0.0078 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00693\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0081 - mean_absolute_error: 0.0639 - val_loss: 0.0076 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00693\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 15s 874us/step - loss: 0.0079 - mean_absolute_error: 0.0629 - val_loss: 0.0086 - val_mean_absolute_error: 0.0670\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00693\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 15s 874us/step - loss: 0.0081 - mean_absolute_error: 0.0634 - val_loss: 0.0083 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00693\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 15s 855us/step - loss: 0.0079 - mean_absolute_error: 0.0628 - val_loss: 0.0083 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00693\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0076 - mean_absolute_error: 0.0619 - val_loss: 0.0077 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00693\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 15s 851us/step - loss: 0.0077 - mean_absolute_error: 0.0620 - val_loss: 0.0082 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00693\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0077 - mean_absolute_error: 0.0619 - val_loss: 0.0081 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00693\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0075 - mean_absolute_error: 0.0612 - val_loss: 0.0075 - val_mean_absolute_error: 0.0625\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00693\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 15s 865us/step - loss: 0.0074 - mean_absolute_error: 0.0608 - val_loss: 0.0074 - val_mean_absolute_error: 0.0619\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00693\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 15s 857us/step - loss: 0.0074 - mean_absolute_error: 0.0608 - val_loss: 0.0075 - val_mean_absolute_error: 0.0625\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00693\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0075 - mean_absolute_error: 0.0615 - val_loss: 0.0085 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00693\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 15s 852us/step - loss: 0.0072 - mean_absolute_error: 0.0601 - val_loss: 0.0074 - val_mean_absolute_error: 0.0623\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00693\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0072 - mean_absolute_error: 0.0600 - val_loss: 0.0079 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00693\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 15s 873us/step - loss: 0.0073 - mean_absolute_error: 0.0603 - val_loss: 0.0075 - val_mean_absolute_error: 0.0625\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00693\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 15s 858us/step - loss: 0.0072 - mean_absolute_error: 0.0595 - val_loss: 0.0075 - val_mean_absolute_error: 0.0623\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00693\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 15s 873us/step - loss: 0.0072 - mean_absolute_error: 0.0599 - val_loss: 0.0071 - val_mean_absolute_error: 0.0602\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00693\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0071 - mean_absolute_error: 0.0595 - val_loss: 0.0078 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00693\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0073 - mean_absolute_error: 0.0597 - val_loss: 0.0075 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00693\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0071 - mean_absolute_error: 0.0595 - val_loss: 0.0083 - val_mean_absolute_error: 0.0655\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00693\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 15s 877us/step - loss: 0.0072 - mean_absolute_error: 0.0593 - val_loss: 0.0070 - val_mean_absolute_error: 0.0600\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00693\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0099 - mean_absolute_error: 0.0668 - val_loss: 0.0094 - val_mean_absolute_error: 0.0721\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00693\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 15s 862us/step - loss: 0.0111 - mean_absolute_error: 0.0747 - val_loss: 0.0083 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00693\n",
      "Epoch 132/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0084 - mean_absolute_error: 0.0653 - val_loss: 0.0080 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00693\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 15s 854us/step - loss: 0.0076 - mean_absolute_error: 0.0615 - val_loss: 0.0084 - val_mean_absolute_error: 0.0653\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00693\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0073 - mean_absolute_error: 0.0606 - val_loss: 0.0074 - val_mean_absolute_error: 0.0619\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00693\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 15s 868us/step - loss: 0.0072 - mean_absolute_error: 0.0600 - val_loss: 0.0080 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00693\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 15s 863us/step - loss: 0.0071 - mean_absolute_error: 0.0596 - val_loss: 0.0071 - val_mean_absolute_error: 0.0607\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00693\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 15s 861us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0070 - val_mean_absolute_error: 0.0598\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00693\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 15s 864us/step - loss: 0.0070 - mean_absolute_error: 0.0590 - val_loss: 0.0073 - val_mean_absolute_error: 0.0611\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00693\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 15s 871us/step - loss: 0.0071 - mean_absolute_error: 0.0597 - val_loss: 0.0075 - val_mean_absolute_error: 0.0616\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00693\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 15s 876us/step - loss: 0.0071 - mean_absolute_error: 0.0590 - val_loss: 0.0076 - val_mean_absolute_error: 0.0622\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00693\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0071 - mean_absolute_error: 0.0587 - val_loss: 0.0073 - val_mean_absolute_error: 0.0613\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00693\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0070 - mean_absolute_error: 0.0587 - val_loss: 0.0076 - val_mean_absolute_error: 0.0622\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00693\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0069 - mean_absolute_error: 0.0583 - val_loss: 0.0076 - val_mean_absolute_error: 0.0626\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00693\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 15s 870us/step - loss: 0.0069 - mean_absolute_error: 0.0584 - val_loss: 0.0072 - val_mean_absolute_error: 0.0607\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00693\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 15s 866us/step - loss: 0.0070 - mean_absolute_error: 0.0583 - val_loss: 0.0071 - val_mean_absolute_error: 0.0602\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00693\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0068 - mean_absolute_error: 0.0580 - val_loss: 0.0078 - val_mean_absolute_error: 0.0626\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00693\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 15s 869us/step - loss: 0.0068 - mean_absolute_error: 0.0582 - val_loss: 0.0075 - val_mean_absolute_error: 0.0623\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00693\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 15s 860us/step - loss: 0.0068 - mean_absolute_error: 0.0583 - val_loss: 0.0085 - val_mean_absolute_error: 0.0655\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00693\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 15s 875us/step - loss: 0.0070 - mean_absolute_error: 0.0584 - val_loss: 0.0077 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00693\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 15s 867us/step - loss: 0.0068 - mean_absolute_error: 0.0580 - val_loss: 0.0073 - val_mean_absolute_error: 0.0611\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00693\n",
      "8760/8760 [==============================] - 41s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[-169.37537]\n",
      " [-129.24057]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "Shape of org. dataset after shift:  (8760,)\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    344.624634\n",
      "2011-01-01 01:00:00    200.759430\n",
      "2011-01-01 02:00:00    299.798325\n",
      "2011-01-01 03:00:00    231.223885\n",
      "2011-01-01 04:00:00    113.465820\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: results_2011: 77.38925327444356\n",
      "8784/8784 [==============================] - 41s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[-183.96545]\n",
      " [-146.72714]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "Shape of org. dataset after shift:  (8784,)\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    362.034546\n",
      "2012-01-01 01:00:00    305.272858\n",
      "2012-01-01 02:00:00    411.671707\n",
      "2012-01-01 03:00:00    274.193951\n",
      "2012-01-01 04:00:00    110.269382\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: results_2012: 78.56382814142985\n"
     ]
    }
   ],
   "source": [
    "#create instance of single LSTM model\n",
    "snglelSTM = SingleLSTM()\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "\n",
    "#slice single area of input data:\n",
    "single_ts_series = ts_20largest.loc['2009':'2014'].iloc[:,0]\n",
    "\n",
    "\n",
    "#create full model for single area:\n",
    "#returns predictions for all single area for given years (validation year & test year) while model is trained on data of 2009 & 2010\n",
    "results_i = snglelSTM.create_full_pred_model(single_ts_series, start_train_year, \n",
    "                                            last_train_set_year, validation_set_year, \n",
    "                                            test_set_year, 'TEST_singleLSTM', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SingleMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:02:22.133570Z",
     "start_time": "2019-10-24T23:01:43.900613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate data..\n",
      "start_validation_set_year:  2011\n",
      "end_validation_set_year:  None\n",
      "start_test_set_year:  2012\n",
      "end_test_set_year:  None\n",
      "Train/Test Split...\n",
      "# dates adjusted:\n",
      "start_validation_set_year:  2011\n",
      "end_validation_set_year:  2011\n",
      "start_test_set_year:  2012\n",
      "end_test_set_year:  2012\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Reshape data for MLP model...\n",
      "X_train shape before modeling:  (16847, 172)\n",
      "X_valid shape before modeling:  (8760, 172)\n",
      "X_test shape before modeling:  (8784, 172)\n",
      "y_train shape before modeling:  (16847,)\n",
      "y_valid shape before modeling:  (8760,)\n",
      "y_test shape before modeling:  (8784,)\n",
      "scaler type:  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 16847 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "16847/16847 [==============================] - 1s 59us/step - loss: 0.7721 - mean_absolute_error: 0.6510 - val_loss: 0.3158 - val_mean_absolute_error: 0.4139\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31581, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 2/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.3619 - mean_absolute_error: 0.4366 - val_loss: 0.2626 - val_mean_absolute_error: 0.3699\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.31581 to 0.26263, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 3/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.3033 - mean_absolute_error: 0.3953 - val_loss: 0.2537 - val_mean_absolute_error: 0.3618\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26263 to 0.25365, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 4/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2826 - mean_absolute_error: 0.3821 - val_loss: 0.2369 - val_mean_absolute_error: 0.3466\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25365 to 0.23687, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 5/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2630 - mean_absolute_error: 0.3655 - val_loss: 0.2286 - val_mean_absolute_error: 0.3418\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23687 to 0.22864, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 6/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2579 - mean_absolute_error: 0.3618 - val_loss: 0.2194 - val_mean_absolute_error: 0.3362\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.22864 to 0.21937, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 7/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2419 - mean_absolute_error: 0.3499 - val_loss: 0.2175 - val_mean_absolute_error: 0.3349\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.21937 to 0.21749, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 8/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2356 - mean_absolute_error: 0.3461 - val_loss: 0.2115 - val_mean_absolute_error: 0.3285\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.21749 to 0.21154, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 9/150\n",
      "16847/16847 [==============================] - 0s 14us/step - loss: 0.2319 - mean_absolute_error: 0.3442 - val_loss: 0.2094 - val_mean_absolute_error: 0.3284\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.21154 to 0.20945, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 10/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2277 - mean_absolute_error: 0.3399 - val_loss: 0.1918 - val_mean_absolute_error: 0.3132\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20945 to 0.19184, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 11/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2163 - mean_absolute_error: 0.3323 - val_loss: 0.1908 - val_mean_absolute_error: 0.3115\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.19184 to 0.19076, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 12/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2154 - mean_absolute_error: 0.3311 - val_loss: 0.1968 - val_mean_absolute_error: 0.3180\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19076\n",
      "Epoch 13/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2098 - mean_absolute_error: 0.3253 - val_loss: 0.1863 - val_mean_absolute_error: 0.3094\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.19076 to 0.18625, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 14/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2127 - mean_absolute_error: 0.3275 - val_loss: 0.1947 - val_mean_absolute_error: 0.3152\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.18625\n",
      "Epoch 15/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2054 - mean_absolute_error: 0.3215 - val_loss: 0.1802 - val_mean_absolute_error: 0.3038\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.18625 to 0.18024, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 16/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2068 - mean_absolute_error: 0.3233 - val_loss: 0.1759 - val_mean_absolute_error: 0.3005\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.18024 to 0.17594, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 17/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.2005 - mean_absolute_error: 0.3183 - val_loss: 0.1732 - val_mean_absolute_error: 0.2976\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.17594 to 0.17324, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 18/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1962 - mean_absolute_error: 0.3159 - val_loss: 0.1742 - val_mean_absolute_error: 0.2990\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.17324\n",
      "Epoch 19/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1941 - mean_absolute_error: 0.3159 - val_loss: 0.1793 - val_mean_absolute_error: 0.3042\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.17324\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1950 - mean_absolute_error: 0.3147 - val_loss: 0.1714 - val_mean_absolute_error: 0.2973\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.17324 to 0.17137, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 21/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1882 - mean_absolute_error: 0.3093 - val_loss: 0.1711 - val_mean_absolute_error: 0.2967\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.17137 to 0.17112, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 22/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1939 - mean_absolute_error: 0.3148 - val_loss: 0.1718 - val_mean_absolute_error: 0.2966\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.17112\n",
      "Epoch 23/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1847 - mean_absolute_error: 0.3063 - val_loss: 0.1698 - val_mean_absolute_error: 0.2952\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.17112 to 0.16979, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 24/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1870 - mean_absolute_error: 0.3063 - val_loss: 0.1677 - val_mean_absolute_error: 0.2936\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.16979 to 0.16770, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 25/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1828 - mean_absolute_error: 0.3065 - val_loss: 0.1704 - val_mean_absolute_error: 0.2965\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.16770\n",
      "Epoch 26/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1814 - mean_absolute_error: 0.3029 - val_loss: 0.1636 - val_mean_absolute_error: 0.2892\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.16770 to 0.16356, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 27/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1803 - mean_absolute_error: 0.3033 - val_loss: 0.1665 - val_mean_absolute_error: 0.2920\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.16356\n",
      "Epoch 28/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1795 - mean_absolute_error: 0.3020 - val_loss: 0.1639 - val_mean_absolute_error: 0.2898\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.16356\n",
      "Epoch 29/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1771 - mean_absolute_error: 0.3007 - val_loss: 0.1664 - val_mean_absolute_error: 0.2926\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.16356\n",
      "Epoch 30/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1737 - mean_absolute_error: 0.2984 - val_loss: 0.1631 - val_mean_absolute_error: 0.2891\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.16356 to 0.16313, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 31/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1720 - mean_absolute_error: 0.2971 - val_loss: 0.1637 - val_mean_absolute_error: 0.2891\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.16313\n",
      "Epoch 32/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1751 - mean_absolute_error: 0.2974 - val_loss: 0.1660 - val_mean_absolute_error: 0.2927\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16313\n",
      "Epoch 33/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1715 - mean_absolute_error: 0.2966 - val_loss: 0.1613 - val_mean_absolute_error: 0.2881\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.16313 to 0.16128, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 34/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1696 - mean_absolute_error: 0.2937 - val_loss: 0.1643 - val_mean_absolute_error: 0.2923\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.16128\n",
      "Epoch 35/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1658 - mean_absolute_error: 0.2918 - val_loss: 0.1606 - val_mean_absolute_error: 0.2872\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.16128 to 0.16062, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 36/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1693 - mean_absolute_error: 0.2932 - val_loss: 0.1577 - val_mean_absolute_error: 0.2843\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.16062 to 0.15772, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 37/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1677 - mean_absolute_error: 0.2925 - val_loss: 0.1575 - val_mean_absolute_error: 0.2846\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.15772 to 0.15754, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 38/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1660 - mean_absolute_error: 0.2899 - val_loss: 0.1571 - val_mean_absolute_error: 0.2839\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.15754 to 0.15715, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 39/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1647 - mean_absolute_error: 0.2901 - val_loss: 0.1588 - val_mean_absolute_error: 0.2844\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15715\n",
      "Epoch 40/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1617 - mean_absolute_error: 0.2880 - val_loss: 0.1525 - val_mean_absolute_error: 0.2798\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.15715 to 0.15248, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 41/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1639 - mean_absolute_error: 0.2892 - val_loss: 0.1511 - val_mean_absolute_error: 0.2770\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.15248 to 0.15113, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 42/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1592 - mean_absolute_error: 0.2859 - val_loss: 0.1541 - val_mean_absolute_error: 0.2800\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15113\n",
      "Epoch 43/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1609 - mean_absolute_error: 0.2859 - val_loss: 0.1568 - val_mean_absolute_error: 0.2828\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15113\n",
      "Epoch 44/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1626 - mean_absolute_error: 0.2870 - val_loss: 0.1523 - val_mean_absolute_error: 0.2777\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.15113\n",
      "Epoch 45/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1588 - mean_absolute_error: 0.2854 - val_loss: 0.1600 - val_mean_absolute_error: 0.2863\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.15113\n",
      "Epoch 46/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1613 - mean_absolute_error: 0.2863 - val_loss: 0.1558 - val_mean_absolute_error: 0.2825\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.15113\n",
      "Epoch 47/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1553 - mean_absolute_error: 0.2820 - val_loss: 0.1549 - val_mean_absolute_error: 0.2821\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.15113\n",
      "Epoch 48/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1567 - mean_absolute_error: 0.2818 - val_loss: 0.1540 - val_mean_absolute_error: 0.2813\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.15113\n",
      "Epoch 49/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1543 - mean_absolute_error: 0.2813 - val_loss: 0.1573 - val_mean_absolute_error: 0.2832\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.15113\n",
      "Epoch 50/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1536 - mean_absolute_error: 0.2803 - val_loss: 0.1524 - val_mean_absolute_error: 0.2783\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.15113\n",
      "Epoch 51/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1522 - mean_absolute_error: 0.2792 - val_loss: 0.1497 - val_mean_absolute_error: 0.2753\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.15113 to 0.14967, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 52/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1513 - mean_absolute_error: 0.2778 - val_loss: 0.1494 - val_mean_absolute_error: 0.2760\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.14967 to 0.14938, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 53/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1541 - mean_absolute_error: 0.2796 - val_loss: 0.1488 - val_mean_absolute_error: 0.2746\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.14938 to 0.14881, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 54/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1482 - mean_absolute_error: 0.2763 - val_loss: 0.1498 - val_mean_absolute_error: 0.2758\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.14881\n",
      "Epoch 55/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1484 - mean_absolute_error: 0.2768 - val_loss: 0.1525 - val_mean_absolute_error: 0.2797\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.14881\n",
      "Epoch 56/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1462 - mean_absolute_error: 0.2742 - val_loss: 0.1503 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.14881\n",
      "Epoch 57/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1481 - mean_absolute_error: 0.2760 - val_loss: 0.1560 - val_mean_absolute_error: 0.2811\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.14881\n",
      "Epoch 58/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1516 - mean_absolute_error: 0.2787 - val_loss: 0.1495 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.14881\n",
      "Epoch 59/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1466 - mean_absolute_error: 0.2742 - val_loss: 0.1530 - val_mean_absolute_error: 0.2797\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.14881\n",
      "Epoch 60/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1470 - mean_absolute_error: 0.2731 - val_loss: 0.1513 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.14881\n",
      "Epoch 61/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1425 - mean_absolute_error: 0.2707 - val_loss: 0.1485 - val_mean_absolute_error: 0.2738\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.14881 to 0.14847, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 62/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1439 - mean_absolute_error: 0.2716 - val_loss: 0.1478 - val_mean_absolute_error: 0.2732\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.14847 to 0.14780, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 63/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1409 - mean_absolute_error: 0.2703 - val_loss: 0.1548 - val_mean_absolute_error: 0.2812\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.14780\n",
      "Epoch 64/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1402 - mean_absolute_error: 0.2693 - val_loss: 0.1491 - val_mean_absolute_error: 0.2749\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.14780\n",
      "Epoch 65/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1404 - mean_absolute_error: 0.2698 - val_loss: 0.1515 - val_mean_absolute_error: 0.2772\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.14780\n",
      "Epoch 66/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1433 - mean_absolute_error: 0.2710 - val_loss: 0.1516 - val_mean_absolute_error: 0.2778\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.14780\n",
      "Epoch 67/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1397 - mean_absolute_error: 0.2675 - val_loss: 0.1548 - val_mean_absolute_error: 0.2809\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.14780\n",
      "Epoch 68/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1396 - mean_absolute_error: 0.2693 - val_loss: 0.1489 - val_mean_absolute_error: 0.2757\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.14780\n",
      "Epoch 69/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1384 - mean_absolute_error: 0.2680 - val_loss: 0.1485 - val_mean_absolute_error: 0.2743\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.14780\n",
      "Epoch 70/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1347 - mean_absolute_error: 0.2639 - val_loss: 0.1488 - val_mean_absolute_error: 0.2751\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.14780\n",
      "Epoch 71/150\n",
      "16847/16847 [==============================] - 0s 12us/step - loss: 0.1367 - mean_absolute_error: 0.2658 - val_loss: 0.1494 - val_mean_absolute_error: 0.2752\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.14780\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1400 - mean_absolute_error: 0.2681 - val_loss: 0.1575 - val_mean_absolute_error: 0.2842\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.14780\n",
      "Epoch 73/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1361 - mean_absolute_error: 0.2653 - val_loss: 0.1491 - val_mean_absolute_error: 0.2756\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.14780\n",
      "Epoch 74/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1384 - mean_absolute_error: 0.2666 - val_loss: 0.1565 - val_mean_absolute_error: 0.2819\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.14780\n",
      "Epoch 75/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1348 - mean_absolute_error: 0.2647 - val_loss: 0.1477 - val_mean_absolute_error: 0.2739\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.14780 to 0.14767, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 76/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1352 - mean_absolute_error: 0.2644 - val_loss: 0.1470 - val_mean_absolute_error: 0.2729\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.14767 to 0.14704, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 77/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1334 - mean_absolute_error: 0.2640 - val_loss: 0.1488 - val_mean_absolute_error: 0.2750\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.14704\n",
      "Epoch 78/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1373 - mean_absolute_error: 0.2640 - val_loss: 0.1540 - val_mean_absolute_error: 0.2802\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.14704\n",
      "Epoch 79/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1318 - mean_absolute_error: 0.2611 - val_loss: 0.1592 - val_mean_absolute_error: 0.2842\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.14704\n",
      "Epoch 80/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1344 - mean_absolute_error: 0.2620 - val_loss: 0.1575 - val_mean_absolute_error: 0.2817\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.14704\n",
      "Epoch 81/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1319 - mean_absolute_error: 0.2603 - val_loss: 0.1488 - val_mean_absolute_error: 0.2756\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.14704\n",
      "Epoch 82/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1312 - mean_absolute_error: 0.2610 - val_loss: 0.1548 - val_mean_absolute_error: 0.2806\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.14704\n",
      "Epoch 83/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1290 - mean_absolute_error: 0.2592 - val_loss: 0.1470 - val_mean_absolute_error: 0.2726\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.14704 to 0.14696, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 84/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1302 - mean_absolute_error: 0.2583 - val_loss: 0.1513 - val_mean_absolute_error: 0.2772\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.14696\n",
      "Epoch 85/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1272 - mean_absolute_error: 0.2577 - val_loss: 0.1514 - val_mean_absolute_error: 0.2782\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.14696\n",
      "Epoch 86/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1271 - mean_absolute_error: 0.2569 - val_loss: 0.1495 - val_mean_absolute_error: 0.2755\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.14696\n",
      "Epoch 87/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1281 - mean_absolute_error: 0.2566 - val_loss: 0.1543 - val_mean_absolute_error: 0.2790\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.14696\n",
      "Epoch 88/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1289 - mean_absolute_error: 0.2590 - val_loss: 0.1513 - val_mean_absolute_error: 0.2771\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.14696\n",
      "Epoch 89/150\n",
      "16847/16847 [==============================] - 0s 14us/step - loss: 0.1301 - mean_absolute_error: 0.2583 - val_loss: 0.1494 - val_mean_absolute_error: 0.2754\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.14696\n",
      "Epoch 90/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1255 - mean_absolute_error: 0.2535 - val_loss: 0.1499 - val_mean_absolute_error: 0.2756\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.14696\n",
      "Epoch 91/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1272 - mean_absolute_error: 0.2582 - val_loss: 0.1461 - val_mean_absolute_error: 0.2714\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.14696 to 0.14612, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_bestmodel.h5\n",
      "Epoch 92/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1243 - mean_absolute_error: 0.2551 - val_loss: 0.1477 - val_mean_absolute_error: 0.2741\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.14612\n",
      "Epoch 93/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1237 - mean_absolute_error: 0.2532 - val_loss: 0.1539 - val_mean_absolute_error: 0.2822\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.14612\n",
      "Epoch 94/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1258 - mean_absolute_error: 0.2548 - val_loss: 0.1515 - val_mean_absolute_error: 0.2775\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.14612\n",
      "Epoch 95/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1215 - mean_absolute_error: 0.2517 - val_loss: 0.1510 - val_mean_absolute_error: 0.2765\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.14612\n",
      "Epoch 96/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1243 - mean_absolute_error: 0.2546 - val_loss: 0.1505 - val_mean_absolute_error: 0.2764\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.14612\n",
      "Epoch 97/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1230 - mean_absolute_error: 0.2514 - val_loss: 0.1508 - val_mean_absolute_error: 0.2763\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.14612\n",
      "Epoch 98/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1223 - mean_absolute_error: 0.2515 - val_loss: 0.1512 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.14612\n",
      "Epoch 99/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1203 - mean_absolute_error: 0.2499 - val_loss: 0.1479 - val_mean_absolute_error: 0.2735\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.14612\n",
      "Epoch 100/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1243 - mean_absolute_error: 0.2535 - val_loss: 0.1486 - val_mean_absolute_error: 0.2738\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.14612\n",
      "Epoch 101/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1200 - mean_absolute_error: 0.2502 - val_loss: 0.1495 - val_mean_absolute_error: 0.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.14612\n",
      "Epoch 102/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1213 - mean_absolute_error: 0.2505 - val_loss: 0.1489 - val_mean_absolute_error: 0.2737\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.14612\n",
      "Epoch 103/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1211 - mean_absolute_error: 0.2517 - val_loss: 0.1551 - val_mean_absolute_error: 0.2817\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.14612\n",
      "Epoch 104/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1229 - mean_absolute_error: 0.2510 - val_loss: 0.1502 - val_mean_absolute_error: 0.2762\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.14612\n",
      "Epoch 105/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1207 - mean_absolute_error: 0.2513 - val_loss: 0.1499 - val_mean_absolute_error: 0.2760\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.14612\n",
      "Epoch 106/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1187 - mean_absolute_error: 0.2484 - val_loss: 0.1498 - val_mean_absolute_error: 0.2760\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.14612\n",
      "Epoch 107/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1211 - mean_absolute_error: 0.2512 - val_loss: 0.1510 - val_mean_absolute_error: 0.2767\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.14612\n",
      "Epoch 108/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1187 - mean_absolute_error: 0.2505 - val_loss: 0.1572 - val_mean_absolute_error: 0.2815\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.14612\n",
      "Epoch 109/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1204 - mean_absolute_error: 0.2512 - val_loss: 0.1535 - val_mean_absolute_error: 0.2794\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.14612\n",
      "Epoch 110/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1193 - mean_absolute_error: 0.2494 - val_loss: 0.1535 - val_mean_absolute_error: 0.2792\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.14612\n",
      "Epoch 111/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1182 - mean_absolute_error: 0.2474 - val_loss: 0.1511 - val_mean_absolute_error: 0.2768\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.14612\n",
      "Epoch 112/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1145 - mean_absolute_error: 0.2434 - val_loss: 0.1508 - val_mean_absolute_error: 0.2765\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.14612\n",
      "Epoch 113/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1144 - mean_absolute_error: 0.2436 - val_loss: 0.1504 - val_mean_absolute_error: 0.2765\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.14612\n",
      "Epoch 114/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1160 - mean_absolute_error: 0.2460 - val_loss: 0.1552 - val_mean_absolute_error: 0.2813\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.14612\n",
      "Epoch 115/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1153 - mean_absolute_error: 0.2455 - val_loss: 0.1552 - val_mean_absolute_error: 0.2800\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.14612\n",
      "Epoch 116/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1157 - mean_absolute_error: 0.2447 - val_loss: 0.1502 - val_mean_absolute_error: 0.2764\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.14612\n",
      "Epoch 117/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1151 - mean_absolute_error: 0.2437 - val_loss: 0.1527 - val_mean_absolute_error: 0.2795\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.14612\n",
      "Epoch 118/150\n",
      "16847/16847 [==============================] - 0s 12us/step - loss: 0.1138 - mean_absolute_error: 0.2439 - val_loss: 0.1507 - val_mean_absolute_error: 0.2758\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.14612\n",
      "Epoch 119/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1155 - mean_absolute_error: 0.2448 - val_loss: 0.1487 - val_mean_absolute_error: 0.2745\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.14612\n",
      "Epoch 120/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1149 - mean_absolute_error: 0.2442 - val_loss: 0.1505 - val_mean_absolute_error: 0.2763\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.14612\n",
      "Epoch 121/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1146 - mean_absolute_error: 0.2438 - val_loss: 0.1553 - val_mean_absolute_error: 0.2792\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.14612\n",
      "Epoch 122/150\n",
      "16847/16847 [==============================] - 0s 12us/step - loss: 0.1138 - mean_absolute_error: 0.2449 - val_loss: 0.1538 - val_mean_absolute_error: 0.2790\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.14612\n",
      "Epoch 123/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1145 - mean_absolute_error: 0.2436 - val_loss: 0.1543 - val_mean_absolute_error: 0.2799\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.14612\n",
      "Epoch 124/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1148 - mean_absolute_error: 0.2438 - val_loss: 0.1528 - val_mean_absolute_error: 0.2783\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.14612\n",
      "Epoch 125/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1119 - mean_absolute_error: 0.2415 - val_loss: 0.1536 - val_mean_absolute_error: 0.2791\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.14612\n",
      "Epoch 126/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1103 - mean_absolute_error: 0.2395 - val_loss: 0.1532 - val_mean_absolute_error: 0.2778\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.14612\n",
      "Epoch 127/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1135 - mean_absolute_error: 0.2414 - val_loss: 0.1576 - val_mean_absolute_error: 0.2829\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.14612\n",
      "Epoch 128/150\n",
      "16847/16847 [==============================] - 0s 14us/step - loss: 0.1143 - mean_absolute_error: 0.2438 - val_loss: 0.1503 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.14612\n",
      "Epoch 129/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1130 - mean_absolute_error: 0.2421 - val_loss: 0.1491 - val_mean_absolute_error: 0.2751\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.14612\n",
      "Epoch 130/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1147 - mean_absolute_error: 0.2432 - val_loss: 0.1533 - val_mean_absolute_error: 0.2783\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.14612\n",
      "Epoch 131/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1102 - mean_absolute_error: 0.2405 - val_loss: 0.1495 - val_mean_absolute_error: 0.2751\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.14612\n",
      "Epoch 132/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1099 - mean_absolute_error: 0.2386 - val_loss: 0.1514 - val_mean_absolute_error: 0.2766\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.14612\n",
      "Epoch 133/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1117 - mean_absolute_error: 0.2391 - val_loss: 0.1500 - val_mean_absolute_error: 0.2744\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.14612\n",
      "Epoch 134/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1132 - mean_absolute_error: 0.2421 - val_loss: 0.1530 - val_mean_absolute_error: 0.2791\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.14612\n",
      "Epoch 135/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1117 - mean_absolute_error: 0.2418 - val_loss: 0.1570 - val_mean_absolute_error: 0.2835\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.14612\n",
      "Epoch 136/150\n",
      "16847/16847 [==============================] - 0s 14us/step - loss: 0.1115 - mean_absolute_error: 0.2421 - val_loss: 0.1491 - val_mean_absolute_error: 0.2746\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.14612\n",
      "Epoch 137/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1094 - mean_absolute_error: 0.2396 - val_loss: 0.1525 - val_mean_absolute_error: 0.2782\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.14612\n",
      "Epoch 138/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1119 - mean_absolute_error: 0.2414 - val_loss: 0.1536 - val_mean_absolute_error: 0.2781\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.14612\n",
      "Epoch 139/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1131 - mean_absolute_error: 0.2403 - val_loss: 0.1535 - val_mean_absolute_error: 0.2799\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.14612\n",
      "Epoch 140/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1104 - mean_absolute_error: 0.2397 - val_loss: 0.1537 - val_mean_absolute_error: 0.2793\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.14612\n",
      "Epoch 141/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1070 - mean_absolute_error: 0.2364 - val_loss: 0.1508 - val_mean_absolute_error: 0.2761\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.14612\n",
      "Epoch 142/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1081 - mean_absolute_error: 0.2376 - val_loss: 0.1516 - val_mean_absolute_error: 0.2771\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.14612\n",
      "Epoch 143/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1095 - mean_absolute_error: 0.2381 - val_loss: 0.1521 - val_mean_absolute_error: 0.2791\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.14612\n",
      "Epoch 144/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1101 - mean_absolute_error: 0.2393 - val_loss: 0.1533 - val_mean_absolute_error: 0.2789\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.14612\n",
      "Epoch 145/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1108 - mean_absolute_error: 0.2378 - val_loss: 0.1566 - val_mean_absolute_error: 0.2817\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.14612\n",
      "Epoch 146/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1087 - mean_absolute_error: 0.2380 - val_loss: 0.1498 - val_mean_absolute_error: 0.2748\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.14612\n",
      "Epoch 147/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1086 - mean_absolute_error: 0.2370 - val_loss: 0.1564 - val_mean_absolute_error: 0.2812\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.14612\n",
      "Epoch 148/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1078 - mean_absolute_error: 0.2377 - val_loss: 0.1518 - val_mean_absolute_error: 0.2772\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.14612\n",
      "Epoch 149/150\n",
      "16847/16847 [==============================] - 0s 12us/step - loss: 0.1093 - mean_absolute_error: 0.2378 - val_loss: 0.1526 - val_mean_absolute_error: 0.2782\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.14612\n",
      "Epoch 150/150\n",
      "16847/16847 [==============================] - 0s 13us/step - loss: 0.1082 - mean_absolute_error: 0.2382 - val_loss: 0.1560 - val_mean_absolute_error: 0.2815\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.14612\n",
      "8760/8760 [==============================] - 0s 50us/step\n",
      "First 2 scaled predictions\n",
      "[[-93.442795]\n",
      " [-93.474846]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "Shape of org. dataset after shift:  (8760,)\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    420.557205\n",
      "2011-01-01 01:00:00    236.525154\n",
      "2011-01-01 02:00:00    296.361839\n",
      "2011-01-01 03:00:00    194.007874\n",
      "2011-01-01 04:00:00     89.208221\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: results_2011: 73.80187038140505\n",
      "8784/8784 [==============================] - 0s 19us/step\n",
      "First 2 scaled predictions\n",
      "[[-68.62956 ]\n",
      " [-89.194275]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "Shape of org. dataset after shift:  (8784,)\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    477.370438\n",
      "2012-01-01 01:00:00    362.805725\n",
      "2012-01-01 02:00:00    352.771271\n",
      "2012-01-01 03:00:00    260.881302\n",
      "2012-01-01 04:00:00    146.841803\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: results_2012: 75.5870478039305\n"
     ]
    }
   ],
   "source": [
    "#test_lagged features:\n",
    "sngleMLP = SingleMLP()\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "\n",
    "#slice a sample of input data for a single area:\n",
    "single_ts_series = ts_20largest.loc['2009':'2014'].iloc[:,0]\n",
    "\n",
    "\n",
    "#create full model for single area:\n",
    "#returns predictions for all single area for given years (validation year & test year) while model is trained on data of 2009 & 2010\n",
    "results_i = sngleMLP.create_full_pred_model(single_ts_series, start_train_year, \n",
    "                                            last_train_set_year, validation_set_year, \n",
    "                                            test_set_year, 'TEST_singleMLP', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplexMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T23:15:28.987391Z",
     "start_time": "2019-10-24T23:02:22.205034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate data..\n",
      "start_train_year:  2009\n",
      "last_train_set_year:  2010\n",
      "start_validation_set_year:  2011\n",
      "start_test_set_year:  2012\n",
      "end_validation_set_year:  None\n",
      "end_test_set_year:  None\n",
      "# adjusted dates..\n",
      "start_train_year:  2009\n",
      "last_train_set_year:  2010\n",
      "start_validation_set_year:  2011\n",
      "start_test_set_year:  2012\n",
      "end_validation_set_year:  2011\n",
      "end_test_set_year:  2012\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16847, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (8760, 203)\n",
      "X_test shape of area237 before concat with other areas:  (8784, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16847,)\n",
      "y_valid shape of area237 before concat with other areas:  (8760,)\n",
      "y_test shape of area237 before concat with other areas:  (8784,)\n",
      "final concatenated shape of X_train :  (336940, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336940 samples, validate on 175200 samples\n",
      "Epoch 1/150\n",
      "336940/336940 [==============================] - 6s 17us/step - loss: 0.3684 - mean_absolute_error: 0.4354 - val_loss: 0.2886 - val_mean_absolute_error: 0.3831\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28862, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 2/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2902 - mean_absolute_error: 0.3849 - val_loss: 0.2657 - val_mean_absolute_error: 0.3657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28862 to 0.26568, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 3/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2731 - mean_absolute_error: 0.3725 - val_loss: 0.2535 - val_mean_absolute_error: 0.3558\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.26568 to 0.25349, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 4/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2621 - mean_absolute_error: 0.3645 - val_loss: 0.2459 - val_mean_absolute_error: 0.3495\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25349 to 0.24588, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 5/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2547 - mean_absolute_error: 0.3590 - val_loss: 0.2401 - val_mean_absolute_error: 0.3445\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24588 to 0.24011, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 6/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2503 - mean_absolute_error: 0.3554 - val_loss: 0.2388 - val_mean_absolute_error: 0.3442\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24011 to 0.23880, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 7/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2451 - mean_absolute_error: 0.3522 - val_loss: 0.2410 - val_mean_absolute_error: 0.3477\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23880\n",
      "Epoch 8/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2417 - mean_absolute_error: 0.3495 - val_loss: 0.2315 - val_mean_absolute_error: 0.3384\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23880 to 0.23151, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 9/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2378 - mean_absolute_error: 0.3468 - val_loss: 0.2307 - val_mean_absolute_error: 0.3377\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23151 to 0.23067, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 10/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2366 - mean_absolute_error: 0.3457 - val_loss: 0.2311 - val_mean_absolute_error: 0.3388\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23067\n",
      "Epoch 11/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2344 - mean_absolute_error: 0.3445 - val_loss: 0.2281 - val_mean_absolute_error: 0.3349\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23067 to 0.22809, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 12/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2323 - mean_absolute_error: 0.3430 - val_loss: 0.2261 - val_mean_absolute_error: 0.3338\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.22809 to 0.22606, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 13/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2298 - mean_absolute_error: 0.3412 - val_loss: 0.2242 - val_mean_absolute_error: 0.3320\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22606 to 0.22421, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 14/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2294 - mean_absolute_error: 0.3412 - val_loss: 0.2249 - val_mean_absolute_error: 0.3328\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.22421\n",
      "Epoch 15/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2278 - mean_absolute_error: 0.3402 - val_loss: 0.2233 - val_mean_absolute_error: 0.3314\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.22421 to 0.22327, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 16/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2261 - mean_absolute_error: 0.3387 - val_loss: 0.2254 - val_mean_absolute_error: 0.3341\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.22327\n",
      "Epoch 17/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2251 - mean_absolute_error: 0.3382 - val_loss: 0.2221 - val_mean_absolute_error: 0.3307\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.22327 to 0.22214, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2235 - mean_absolute_error: 0.3370 - val_loss: 0.2272 - val_mean_absolute_error: 0.3368\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.22214\n",
      "Epoch 19/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2232 - mean_absolute_error: 0.3367 - val_loss: 0.2245 - val_mean_absolute_error: 0.3329\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.22214\n",
      "Epoch 20/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2224 - mean_absolute_error: 0.3366 - val_loss: 0.2246 - val_mean_absolute_error: 0.3342\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.22214\n",
      "Epoch 21/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2209 - mean_absolute_error: 0.3352 - val_loss: 0.2253 - val_mean_absolute_error: 0.3354\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.22214\n",
      "Epoch 22/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2205 - mean_absolute_error: 0.3350 - val_loss: 0.2237 - val_mean_absolute_error: 0.3326\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.22214\n",
      "Epoch 23/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2199 - mean_absolute_error: 0.3348 - val_loss: 0.2243 - val_mean_absolute_error: 0.3337\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.22214\n",
      "Epoch 24/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2196 - mean_absolute_error: 0.3346 - val_loss: 0.2199 - val_mean_absolute_error: 0.3295\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.22214 to 0.21992, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 25/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2185 - mean_absolute_error: 0.3336 - val_loss: 0.2215 - val_mean_absolute_error: 0.3306\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.21992\n",
      "Epoch 26/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2182 - mean_absolute_error: 0.3336 - val_loss: 0.2221 - val_mean_absolute_error: 0.3325\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.21992\n",
      "Epoch 27/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2182 - mean_absolute_error: 0.3338 - val_loss: 0.2192 - val_mean_absolute_error: 0.3289\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.21992 to 0.21921, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 28/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2171 - mean_absolute_error: 0.3331 - val_loss: 0.2222 - val_mean_absolute_error: 0.3317\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.21921\n",
      "Epoch 29/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2160 - mean_absolute_error: 0.3323 - val_loss: 0.2203 - val_mean_absolute_error: 0.3303\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.21921\n",
      "Epoch 30/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2161 - mean_absolute_error: 0.3321 - val_loss: 0.2191 - val_mean_absolute_error: 0.3290\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.21921 to 0.21906, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 31/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2157 - mean_absolute_error: 0.3316 - val_loss: 0.2222 - val_mean_absolute_error: 0.3316\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.21906\n",
      "Epoch 32/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2155 - mean_absolute_error: 0.3321 - val_loss: 0.2198 - val_mean_absolute_error: 0.3302\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.21906\n",
      "Epoch 33/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2154 - mean_absolute_error: 0.3317 - val_loss: 0.2207 - val_mean_absolute_error: 0.3301\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.21906\n",
      "Epoch 34/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2152 - mean_absolute_error: 0.3315 - val_loss: 0.2217 - val_mean_absolute_error: 0.3318\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.21906\n",
      "Epoch 35/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2141 - mean_absolute_error: 0.3311 - val_loss: 0.2236 - val_mean_absolute_error: 0.3349\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.21906\n",
      "Epoch 36/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2137 - mean_absolute_error: 0.3308 - val_loss: 0.2192 - val_mean_absolute_error: 0.3292\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.21906\n",
      "Epoch 37/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2142 - mean_absolute_error: 0.3307 - val_loss: 0.2237 - val_mean_absolute_error: 0.3351\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.21906\n",
      "Epoch 38/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2135 - mean_absolute_error: 0.3308 - val_loss: 0.2205 - val_mean_absolute_error: 0.3315\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.21906\n",
      "Epoch 39/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2133 - mean_absolute_error: 0.3305 - val_loss: 0.2222 - val_mean_absolute_error: 0.3322\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.21906\n",
      "Epoch 40/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2130 - mean_absolute_error: 0.3302 - val_loss: 0.2196 - val_mean_absolute_error: 0.3292\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.21906\n",
      "Epoch 41/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2130 - mean_absolute_error: 0.3303 - val_loss: 0.2173 - val_mean_absolute_error: 0.3263\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.21906 to 0.21731, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 42/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2121 - mean_absolute_error: 0.3297 - val_loss: 0.2172 - val_mean_absolute_error: 0.3273\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.21731 to 0.21724, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 43/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2122 - mean_absolute_error: 0.3298 - val_loss: 0.2177 - val_mean_absolute_error: 0.3275\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.21724\n",
      "Epoch 44/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2121 - mean_absolute_error: 0.3297 - val_loss: 0.2209 - val_mean_absolute_error: 0.3310\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.21724\n",
      "Epoch 45/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2120 - mean_absolute_error: 0.3293 - val_loss: 0.2208 - val_mean_absolute_error: 0.3310\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.21724\n",
      "Epoch 46/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2112 - mean_absolute_error: 0.3290 - val_loss: 0.2197 - val_mean_absolute_error: 0.3298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21724\n",
      "Epoch 47/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2115 - mean_absolute_error: 0.3295 - val_loss: 0.2171 - val_mean_absolute_error: 0.3276\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.21724 to 0.21708, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 48/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2105 - mean_absolute_error: 0.3288 - val_loss: 0.2183 - val_mean_absolute_error: 0.3280\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21708\n",
      "Epoch 49/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2108 - mean_absolute_error: 0.3288 - val_loss: 0.2208 - val_mean_absolute_error: 0.3309\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21708\n",
      "Epoch 50/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2111 - mean_absolute_error: 0.3292 - val_loss: 0.2195 - val_mean_absolute_error: 0.3310\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21708\n",
      "Epoch 51/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2109 - mean_absolute_error: 0.3290 - val_loss: 0.2165 - val_mean_absolute_error: 0.3270\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.21708 to 0.21654, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 52/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2113 - mean_absolute_error: 0.3290 - val_loss: 0.2171 - val_mean_absolute_error: 0.3270\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21654\n",
      "Epoch 53/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2097 - mean_absolute_error: 0.3282 - val_loss: 0.2219 - val_mean_absolute_error: 0.3318\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21654\n",
      "Epoch 54/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2100 - mean_absolute_error: 0.3286 - val_loss: 0.2204 - val_mean_absolute_error: 0.3316\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21654\n",
      "Epoch 55/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2102 - mean_absolute_error: 0.3283 - val_loss: 0.2170 - val_mean_absolute_error: 0.3272\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21654\n",
      "Epoch 56/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2092 - mean_absolute_error: 0.3279 - val_loss: 0.2196 - val_mean_absolute_error: 0.3298\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21654\n",
      "Epoch 57/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2090 - mean_absolute_error: 0.3278 - val_loss: 0.2208 - val_mean_absolute_error: 0.3311\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21654\n",
      "Epoch 58/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2088 - mean_absolute_error: 0.3277 - val_loss: 0.2170 - val_mean_absolute_error: 0.3279\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21654\n",
      "Epoch 59/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2086 - mean_absolute_error: 0.3276 - val_loss: 0.2180 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21654\n",
      "Epoch 60/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2079 - mean_absolute_error: 0.3272 - val_loss: 0.2200 - val_mean_absolute_error: 0.3302\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.21654\n",
      "Epoch 61/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2081 - mean_absolute_error: 0.3275 - val_loss: 0.2187 - val_mean_absolute_error: 0.3295\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.21654\n",
      "Epoch 62/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2085 - mean_absolute_error: 0.3273 - val_loss: 0.2168 - val_mean_absolute_error: 0.3268\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.21654\n",
      "Epoch 63/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2082 - mean_absolute_error: 0.3274 - val_loss: 0.2172 - val_mean_absolute_error: 0.3272\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.21654\n",
      "Epoch 64/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2074 - mean_absolute_error: 0.3265 - val_loss: 0.2189 - val_mean_absolute_error: 0.3298\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.21654\n",
      "Epoch 65/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2073 - mean_absolute_error: 0.3266 - val_loss: 0.2187 - val_mean_absolute_error: 0.3288\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.21654\n",
      "Epoch 66/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2083 - mean_absolute_error: 0.3274 - val_loss: 0.2176 - val_mean_absolute_error: 0.3269\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.21654\n",
      "Epoch 67/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2073 - mean_absolute_error: 0.3267 - val_loss: 0.2189 - val_mean_absolute_error: 0.3294\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.21654\n",
      "Epoch 68/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2071 - mean_absolute_error: 0.3268 - val_loss: 0.2203 - val_mean_absolute_error: 0.3310\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.21654\n",
      "Epoch 69/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2068 - mean_absolute_error: 0.3264 - val_loss: 0.2195 - val_mean_absolute_error: 0.3297\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.21654\n",
      "Epoch 70/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2072 - mean_absolute_error: 0.3267 - val_loss: 0.2164 - val_mean_absolute_error: 0.3265\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.21654 to 0.21635, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 71/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2076 - mean_absolute_error: 0.3266 - val_loss: 0.2170 - val_mean_absolute_error: 0.3272\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.21635\n",
      "Epoch 72/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2068 - mean_absolute_error: 0.3267 - val_loss: 0.2170 - val_mean_absolute_error: 0.3275\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.21635\n",
      "Epoch 73/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2065 - mean_absolute_error: 0.3263 - val_loss: 0.2166 - val_mean_absolute_error: 0.3264\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.21635\n",
      "Epoch 74/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2064 - mean_absolute_error: 0.3264 - val_loss: 0.2158 - val_mean_absolute_error: 0.3250\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.21635 to 0.21576, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 75/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2066 - mean_absolute_error: 0.3262 - val_loss: 0.2172 - val_mean_absolute_error: 0.3269\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.21576\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2067 - mean_absolute_error: 0.3265 - val_loss: 0.2169 - val_mean_absolute_error: 0.3269\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.21576\n",
      "Epoch 77/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2058 - mean_absolute_error: 0.3262 - val_loss: 0.2180 - val_mean_absolute_error: 0.3289\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.21576\n",
      "Epoch 78/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2062 - mean_absolute_error: 0.3259 - val_loss: 0.2234 - val_mean_absolute_error: 0.3341\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.21576\n",
      "Epoch 79/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2059 - mean_absolute_error: 0.3262 - val_loss: 0.2176 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.21576\n",
      "Epoch 80/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2056 - mean_absolute_error: 0.3256 - val_loss: 0.2169 - val_mean_absolute_error: 0.3271\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.21576\n",
      "Epoch 81/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2060 - mean_absolute_error: 0.3261 - val_loss: 0.2206 - val_mean_absolute_error: 0.3317\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.21576\n",
      "Epoch 82/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2055 - mean_absolute_error: 0.3259 - val_loss: 0.2164 - val_mean_absolute_error: 0.3268\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21576\n",
      "Epoch 83/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2065 - mean_absolute_error: 0.3264 - val_loss: 0.2172 - val_mean_absolute_error: 0.3269\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.21576\n",
      "Epoch 84/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2054 - mean_absolute_error: 0.3255 - val_loss: 0.2163 - val_mean_absolute_error: 0.3260\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.21576\n",
      "Epoch 85/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2045 - mean_absolute_error: 0.3252 - val_loss: 0.2166 - val_mean_absolute_error: 0.3265\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.21576\n",
      "Epoch 86/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2055 - mean_absolute_error: 0.3256 - val_loss: 0.2184 - val_mean_absolute_error: 0.3284\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.21576\n",
      "Epoch 87/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2056 - mean_absolute_error: 0.3256 - val_loss: 0.2179 - val_mean_absolute_error: 0.3279\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.21576\n",
      "Epoch 88/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2048 - mean_absolute_error: 0.3252 - val_loss: 0.2228 - val_mean_absolute_error: 0.3343\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.21576\n",
      "Epoch 89/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2041 - mean_absolute_error: 0.3252 - val_loss: 0.2169 - val_mean_absolute_error: 0.3263\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.21576\n",
      "Epoch 90/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2049 - mean_absolute_error: 0.3256 - val_loss: 0.2212 - val_mean_absolute_error: 0.3321\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.21576\n",
      "Epoch 91/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2055 - mean_absolute_error: 0.3256 - val_loss: 0.2174 - val_mean_absolute_error: 0.3272\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.21576\n",
      "Epoch 92/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2043 - mean_absolute_error: 0.3250 - val_loss: 0.2164 - val_mean_absolute_error: 0.3264\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.21576\n",
      "Epoch 93/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2044 - mean_absolute_error: 0.3248 - val_loss: 0.2172 - val_mean_absolute_error: 0.3284\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.21576\n",
      "Epoch 94/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2042 - mean_absolute_error: 0.3249 - val_loss: 0.2179 - val_mean_absolute_error: 0.3280\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.21576\n",
      "Epoch 95/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2044 - mean_absolute_error: 0.3250 - val_loss: 0.2172 - val_mean_absolute_error: 0.3273\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.21576\n",
      "Epoch 96/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2038 - mean_absolute_error: 0.3244 - val_loss: 0.2173 - val_mean_absolute_error: 0.3267\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.21576\n",
      "Epoch 97/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2039 - mean_absolute_error: 0.3247 - val_loss: 0.2188 - val_mean_absolute_error: 0.3297\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.21576\n",
      "Epoch 98/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2038 - mean_absolute_error: 0.3245 - val_loss: 0.2180 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.21576\n",
      "Epoch 99/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2036 - mean_absolute_error: 0.3243 - val_loss: 0.2192 - val_mean_absolute_error: 0.3306\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.21576\n",
      "Epoch 100/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2035 - mean_absolute_error: 0.3245 - val_loss: 0.2166 - val_mean_absolute_error: 0.3263\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.21576\n",
      "Epoch 101/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2037 - mean_absolute_error: 0.3247 - val_loss: 0.2185 - val_mean_absolute_error: 0.3293\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.21576\n",
      "Epoch 102/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2038 - mean_absolute_error: 0.3247 - val_loss: 0.2184 - val_mean_absolute_error: 0.3293\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.21576\n",
      "Epoch 103/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2039 - mean_absolute_error: 0.3244 - val_loss: 0.2249 - val_mean_absolute_error: 0.3361\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.21576\n",
      "Epoch 104/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2032 - mean_absolute_error: 0.3246 - val_loss: 0.2170 - val_mean_absolute_error: 0.3266\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.21576\n",
      "Epoch 105/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2026 - mean_absolute_error: 0.3241 - val_loss: 0.2170 - val_mean_absolute_error: 0.3264\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.21576\n",
      "Epoch 106/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2032 - mean_absolute_error: 0.3243 - val_loss: 0.2153 - val_mean_absolute_error: 0.3249\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.21576 to 0.21533, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 107/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2031 - mean_absolute_error: 0.3243 - val_loss: 0.2157 - val_mean_absolute_error: 0.3258\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.21533\n",
      "Epoch 108/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2033 - mean_absolute_error: 0.3245 - val_loss: 0.2169 - val_mean_absolute_error: 0.3270\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.21533\n",
      "Epoch 109/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2029 - mean_absolute_error: 0.3241 - val_loss: 0.2179 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.21533\n",
      "Epoch 110/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2029 - mean_absolute_error: 0.3242 - val_loss: 0.2240 - val_mean_absolute_error: 0.3353\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.21533\n",
      "Epoch 111/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2029 - mean_absolute_error: 0.3240 - val_loss: 0.2197 - val_mean_absolute_error: 0.3300\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.21533\n",
      "Epoch 112/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2029 - mean_absolute_error: 0.3242 - val_loss: 0.2166 - val_mean_absolute_error: 0.3266\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.21533\n",
      "Epoch 113/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2023 - mean_absolute_error: 0.3237 - val_loss: 0.2169 - val_mean_absolute_error: 0.3272\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.21533\n",
      "Epoch 114/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2034 - mean_absolute_error: 0.3245 - val_loss: 0.2162 - val_mean_absolute_error: 0.3262\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.21533\n",
      "Epoch 115/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2020 - mean_absolute_error: 0.3236 - val_loss: 0.2218 - val_mean_absolute_error: 0.3331\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.21533\n",
      "Epoch 116/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2021 - mean_absolute_error: 0.3237 - val_loss: 0.2151 - val_mean_absolute_error: 0.3250\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.21533 to 0.21512, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models_MLP/TEST_Runtime_MLP_bestmodel.h5\n",
      "Epoch 117/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2022 - mean_absolute_error: 0.3238 - val_loss: 0.2168 - val_mean_absolute_error: 0.3261\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.21512\n",
      "Epoch 118/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2033 - mean_absolute_error: 0.3241 - val_loss: 0.2176 - val_mean_absolute_error: 0.3277\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.21512\n",
      "Epoch 119/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2018 - mean_absolute_error: 0.3234 - val_loss: 0.2242 - val_mean_absolute_error: 0.3355\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.21512\n",
      "Epoch 120/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2015 - mean_absolute_error: 0.3233 - val_loss: 0.2183 - val_mean_absolute_error: 0.3291\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.21512\n",
      "Epoch 121/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2028 - mean_absolute_error: 0.3238 - val_loss: 0.2193 - val_mean_absolute_error: 0.3289\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.21512\n",
      "Epoch 122/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2018 - mean_absolute_error: 0.3234 - val_loss: 0.2198 - val_mean_absolute_error: 0.3306\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.21512\n",
      "Epoch 123/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2021 - mean_absolute_error: 0.3237 - val_loss: 0.2176 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.21512\n",
      "Epoch 124/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2016 - mean_absolute_error: 0.3233 - val_loss: 0.2198 - val_mean_absolute_error: 0.3300\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.21512\n",
      "Epoch 125/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2014 - mean_absolute_error: 0.3235 - val_loss: 0.2167 - val_mean_absolute_error: 0.3262\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.21512\n",
      "Epoch 126/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2028 - mean_absolute_error: 0.3242 - val_loss: 0.2182 - val_mean_absolute_error: 0.3268\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.21512\n",
      "Epoch 127/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2014 - mean_absolute_error: 0.3236 - val_loss: 0.2180 - val_mean_absolute_error: 0.3285\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.21512\n",
      "Epoch 128/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2016 - mean_absolute_error: 0.3235 - val_loss: 0.2208 - val_mean_absolute_error: 0.3316\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.21512\n",
      "Epoch 129/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2016 - mean_absolute_error: 0.3230 - val_loss: 0.2165 - val_mean_absolute_error: 0.3266\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.21512\n",
      "Epoch 130/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2013 - mean_absolute_error: 0.3232 - val_loss: 0.2174 - val_mean_absolute_error: 0.3275\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.21512\n",
      "Epoch 131/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2010 - mean_absolute_error: 0.3228 - val_loss: 0.2190 - val_mean_absolute_error: 0.3293\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.21512\n",
      "Epoch 132/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2010 - mean_absolute_error: 0.3231 - val_loss: 0.2171 - val_mean_absolute_error: 0.3271\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.21512\n",
      "Epoch 133/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2017 - mean_absolute_error: 0.3237 - val_loss: 0.2186 - val_mean_absolute_error: 0.3285\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.21512\n",
      "Epoch 134/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2006 - mean_absolute_error: 0.3225 - val_loss: 0.2194 - val_mean_absolute_error: 0.3297\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.21512\n",
      "Epoch 135/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2007 - mean_absolute_error: 0.3226 - val_loss: 0.2186 - val_mean_absolute_error: 0.3293\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.21512\n",
      "Epoch 136/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2014 - mean_absolute_error: 0.3229 - val_loss: 0.2206 - val_mean_absolute_error: 0.3307\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.21512\n",
      "Epoch 137/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2008 - mean_absolute_error: 0.3227 - val_loss: 0.2188 - val_mean_absolute_error: 0.3286\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.21512\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2012 - mean_absolute_error: 0.3233 - val_loss: 0.2181 - val_mean_absolute_error: 0.3282\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.21512\n",
      "Epoch 139/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2007 - mean_absolute_error: 0.3229 - val_loss: 0.2195 - val_mean_absolute_error: 0.3290\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.21512\n",
      "Epoch 140/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2009 - mean_absolute_error: 0.3230 - val_loss: 0.2173 - val_mean_absolute_error: 0.3265\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.21512\n",
      "Epoch 141/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2010 - mean_absolute_error: 0.3229 - val_loss: 0.2160 - val_mean_absolute_error: 0.3257\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.21512\n",
      "Epoch 142/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2002 - mean_absolute_error: 0.3224 - val_loss: 0.2199 - val_mean_absolute_error: 0.3299\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.21512\n",
      "Epoch 143/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2006 - mean_absolute_error: 0.3228 - val_loss: 0.2171 - val_mean_absolute_error: 0.3278\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.21512\n",
      "Epoch 144/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2006 - mean_absolute_error: 0.3226 - val_loss: 0.2192 - val_mean_absolute_error: 0.3301\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.21512\n",
      "Epoch 145/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2001 - mean_absolute_error: 0.3223 - val_loss: 0.2162 - val_mean_absolute_error: 0.3261\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.21512\n",
      "Epoch 146/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2002 - mean_absolute_error: 0.3224 - val_loss: 0.2157 - val_mean_absolute_error: 0.3255\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.21512\n",
      "Epoch 147/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2002 - mean_absolute_error: 0.3224 - val_loss: 0.2169 - val_mean_absolute_error: 0.3262\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.21512\n",
      "Epoch 148/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2005 - mean_absolute_error: 0.3225 - val_loss: 0.2178 - val_mean_absolute_error: 0.3281\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.21512\n",
      "Epoch 149/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.1999 - mean_absolute_error: 0.3221 - val_loss: 0.2166 - val_mean_absolute_error: 0.3261\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.21512\n",
      "Epoch 150/150\n",
      "336940/336940 [==============================] - 5s 15us/step - loss: 0.2000 - mean_absolute_error: 0.3221 - val_loss: 0.2176 - val_mean_absolute_error: 0.3276\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.21512\n",
      "175200/175200 [==============================] - 4s 24us/step\n",
      "Shape of org. dataset after shift:  (8760, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  380.827530  373.449011  181.304872  1141.939459   \n",
      "2011-01-01 01:00:00  177.626129  178.244461   76.417191   584.619904   \n",
      "2011-01-01 02:00:00  227.037613   98.313347   17.179787   586.707280   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  435.200531  348.923683  533.147316  608.415188   \n",
      "2011-01-01 01:00:00  218.258728  167.308380  397.132824  417.428236   \n",
      "2011-01-01 02:00:00  259.308075  222.015762  411.844742  254.550163   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  625.676712  337.356752  499.593229  709.305538   \n",
      "2011-01-01 01:00:00  359.794854  125.779633  199.699364  433.383768   \n",
      "2011-01-01 02:00:00  230.546257  149.025307  191.309021  396.367489   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  191.569172  569.375033  456.175682  409.708357   \n",
      "2011-01-01 01:00:00   87.200584  369.907175  228.205589  275.610170   \n",
      "2011-01-01 02:00:00   91.951344  289.677204  345.998283  243.921505   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2011-01-01 00:00:00  405.261387  568.937578  207.114380  452.187860  \n",
      "2011-01-01 01:00:00  278.220135  339.260250   15.446655  286.007577  \n",
      "2011-01-01 02:00:00  322.252640  245.874129    9.441675  199.005859  \n",
      "RMSE per TS 0 : model: results_2011 : 68.24693086909723\n",
      "RMSE per TS 1 : model: results_2011 : 84.8924471879662\n",
      "RMSE per TS 2 : model: results_2011 : 79.37126698299097\n",
      "RMSE per TS 3 : model: results_2011 : 75.46212554117105\n",
      "RMSE per TS 4 : model: results_2011 : 61.857887109720984\n",
      "RMSE per TS 5 : model: results_2011 : 73.05224022600787\n",
      "RMSE per TS 6 : model: results_2011 : 57.9896776256787\n",
      "RMSE per TS 7 : model: results_2011 : 61.89454515410655\n",
      "RMSE per TS 8 : model: results_2011 : 61.461321005195394\n",
      "RMSE per TS 9 : model: results_2011 : 76.3450752653966\n",
      "RMSE per TS 10 : model: results_2011 : 71.09012043058154\n",
      "RMSE per TS 11 : model: results_2011 : 52.39587750139024\n",
      "RMSE per TS 12 : model: results_2011 : 56.686034430651965\n",
      "RMSE per TS 13 : model: results_2011 : 50.59338088401247\n",
      "RMSE per TS 14 : model: results_2011 : 52.33804126604433\n",
      "RMSE per TS 15 : model: results_2011 : 48.158130039139806\n",
      "RMSE per TS 16 : model: results_2011 : 44.585145849294825\n",
      "RMSE per TS 17 : model: results_2011 : 52.02975327993988\n",
      "RMSE per TS 18 : model: results_2011 : 80.43662432464875\n",
      "RMSE per TS 19 : model: results_2011 : 43.264775361757536\n",
      "Avg.RMSE for complex MLP model results_2011 : 62.607570016739636\n",
      "175680/175680 [==============================] - 4s 22us/step\n",
      "Shape of org. dataset after shift:  (8784, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  400.468765  353.046028  168.800173  1216.993668   \n",
      "2012-01-01 01:00:00  287.910858  245.767601   74.126272   669.690750   \n",
      "2012-01-01 02:00:00  321.687592  147.367851   40.214703   697.207737   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  441.900360  388.844604  608.239212  701.718327   \n",
      "2012-01-01 01:00:00  242.822327  224.615929  426.579460  452.788460   \n",
      "2012-01-01 02:00:00  321.737228  227.608963  422.018478  280.801546   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  844.197784  351.632832  539.742622  688.897987   \n",
      "2012-01-01 01:00:00  455.416332  193.262558  262.249962  501.024573   \n",
      "2012-01-01 02:00:00  338.057251  227.693554  179.502731  492.272932   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  263.983120  661.223415  461.110962  456.925116   \n",
      "2012-01-01 01:00:00  146.434135  502.797768  291.629616  389.413382   \n",
      "2012-01-01 02:00:00  104.217754  352.798155  549.707891  299.185503   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2012-01-01 00:00:00  392.188004  662.332325  198.700956  489.734310  \n",
      "2012-01-01 01:00:00  292.319893  344.156575  -12.690800  299.879757  \n",
      "2012-01-01 02:00:00  488.912943  273.276603    0.361769  230.813866  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE per TS 0 : model: results_2012 : 70.1774741920154\n",
      "RMSE per TS 1 : model: results_2012 : 87.31683982366056\n",
      "RMSE per TS 2 : model: results_2012 : 76.78499772570444\n",
      "RMSE per TS 3 : model: results_2012 : 75.74667501110457\n",
      "RMSE per TS 4 : model: results_2012 : 63.31250273922924\n",
      "RMSE per TS 5 : model: results_2012 : 75.89812318904372\n",
      "RMSE per TS 6 : model: results_2012 : 58.07929370266036\n",
      "RMSE per TS 7 : model: results_2012 : 64.11859579366421\n",
      "RMSE per TS 8 : model: results_2012 : 62.62450275173861\n",
      "RMSE per TS 9 : model: results_2012 : 75.23100962884303\n",
      "RMSE per TS 10 : model: results_2012 : 69.73150467629065\n",
      "RMSE per TS 11 : model: results_2012 : 50.98474740620433\n",
      "RMSE per TS 12 : model: results_2012 : 56.00039055927089\n",
      "RMSE per TS 13 : model: results_2012 : 51.07507733833642\n",
      "RMSE per TS 14 : model: results_2012 : 50.178591708094004\n",
      "RMSE per TS 15 : model: results_2012 : 48.91410580740934\n",
      "RMSE per TS 16 : model: results_2012 : 44.91928564543685\n",
      "RMSE per TS 17 : model: results_2012 : 52.29024748500837\n",
      "RMSE per TS 18 : model: results_2012 : 84.56016879682286\n",
      "RMSE per TS 19 : model: results_2012 : 42.24003940967875\n",
      "Avg.RMSE for complex MLP model results_2012 : 63.00920866951084\n"
     ]
    }
   ],
   "source": [
    "#the complex MLP processed data of multiple areas at the same time\n",
    "\n",
    "cplxMLP = ComplexMLP()\n",
    "\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "#slice input data for 20 busiest areas:\n",
    "multivar_ts_series = ts_20largest.loc['2009':'2014']\n",
    "\n",
    "#model creation:\n",
    "#returns predictions for all 20 areas for given years (validation year & test year) while model is trained on data of 2009 & 2010\n",
    "results_i = cplxMLP.create_full_pred_model(multivar_ts_series, start_train_year, last_train_set_year, \n",
    "                                           validation_set_year,test_set_year,\n",
    "                                           'TEST_complexMLP',                                        \n",
    "                                            verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultivarLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-24T21:35:10.615889Z",
     "start_time": "2019-10-24T20:52:46.685618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_validation_set_year  2011\n",
      "start_test_set_year  2012\n",
      "end_validation_set_year  None\n",
      "end_test_set_year  None\n",
      "generate data..\n",
      "st_valid_year:  2011\n",
      "end_valid_year:  2011\n",
      "st_test_year:  2012\n",
      "end_test_year:  2012\n",
      "create data of area  237\n",
      "create data of area  161\n",
      "create data of area  230\n",
      "create data of area  79\n",
      "create data of area  236\n",
      "create data of area  162\n",
      "create data of area  170\n",
      "create data of area  234\n",
      "create data of area  48\n",
      "create data of area  186\n",
      "create data of area  142\n",
      "create data of area  107\n",
      "create data of area  163\n",
      "create data of area  68\n",
      "create data of area  239\n",
      "create data of area  164\n",
      "create data of area  141\n",
      "create data of area  249\n",
      "create data of area  138\n",
      "create data of area  90\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "X_train shape of area237 before concat with other areas:  (17351, 168, 1)\n",
      "X_valid shape of area237 before concat with other areas:  (8760, 168, 1)\n",
      "X_test shape of area237 before concat with other areas:  (8784, 168, 1)\n",
      "y_train shape of area237 before concat with other areas:  (17351, 1)\n",
      "y_valid shape of area237 before concat with other areas:  (8760, 1)\n",
      "y_test shape of area237 before concat with other areas:  (8784, 1)\n",
      "final concatenated shape of X_train:  (17351, 168, 20)\n",
      "final concatenated shape of X_valid:  (8760, 168, 20)\n",
      "final concatenated shape of X_test:  (8784, 168, 20)\n",
      "final concatenated shape of y_train:  (17351, 20)\n",
      "final concatenated shape of y_valid:  (8760, 20)\n",
      "final concatenated shape of y_test:  (8784, 20)\n",
      "scaler type:  MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
      "create stacked LSTM 2 layer non-stateful model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Regular 2H-LSTM Model is created...\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 19s 1ms/step - loss: 0.0355 - mean_absolute_error: 0.1401 - val_loss: 0.0315 - val_mean_absolute_error: 0.1293\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03151, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0261 - mean_absolute_error: 0.1181 - val_loss: 0.0260 - val_mean_absolute_error: 0.1178\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03151 to 0.02595, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0221 - mean_absolute_error: 0.1091 - val_loss: 0.0222 - val_mean_absolute_error: 0.1083\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02595 to 0.02220, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 17s 981us/step - loss: 0.0199 - mean_absolute_error: 0.1030 - val_loss: 0.0207 - val_mean_absolute_error: 0.1046\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02220 to 0.02067, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 17s 966us/step - loss: 0.0185 - mean_absolute_error: 0.0989 - val_loss: 0.0194 - val_mean_absolute_error: 0.1010\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02067 to 0.01939, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0171 - mean_absolute_error: 0.0949 - val_loss: 0.0176 - val_mean_absolute_error: 0.0960\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01939 to 0.01756, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0159 - mean_absolute_error: 0.0913 - val_loss: 0.0161 - val_mean_absolute_error: 0.0914\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01756 to 0.01611, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 944us/step - loss: 0.0145 - mean_absolute_error: 0.0872 - val_loss: 0.0148 - val_mean_absolute_error: 0.0872\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01611 to 0.01478, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 17s 955us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0143 - val_mean_absolute_error: 0.0856\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01478 to 0.01433, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0128 - mean_absolute_error: 0.0813 - val_loss: 0.0133 - val_mean_absolute_error: 0.0823\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01433 to 0.01332, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 17s 966us/step - loss: 0.0123 - mean_absolute_error: 0.0795 - val_loss: 0.0125 - val_mean_absolute_error: 0.0792\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01332 to 0.01247, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 17s 966us/step - loss: 0.0118 - mean_absolute_error: 0.0776 - val_loss: 0.0125 - val_mean_absolute_error: 0.0791\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01247\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0115 - mean_absolute_error: 0.0766 - val_loss: 0.0121 - val_mean_absolute_error: 0.0781\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01247 to 0.01213, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0111 - mean_absolute_error: 0.0751 - val_loss: 0.0117 - val_mean_absolute_error: 0.0767\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01213 to 0.01171, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 941us/step - loss: 0.0111 - mean_absolute_error: 0.0749 - val_loss: 0.0115 - val_mean_absolute_error: 0.0758\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01171 to 0.01152, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 17s 954us/step - loss: 0.0108 - mean_absolute_error: 0.0740 - val_loss: 0.0114 - val_mean_absolute_error: 0.0752\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01152 to 0.01141, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0106 - mean_absolute_error: 0.0730 - val_loss: 0.0112 - val_mean_absolute_error: 0.0746\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01141 to 0.01117, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 17s 973us/step - loss: 0.0105 - mean_absolute_error: 0.0726 - val_loss: 0.0110 - val_mean_absolute_error: 0.0737\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01117 to 0.01096, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0104 - mean_absolute_error: 0.0721 - val_loss: 0.0111 - val_mean_absolute_error: 0.0742\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01096\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0103 - mean_absolute_error: 0.0717 - val_loss: 0.0111 - val_mean_absolute_error: 0.0739\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.01096\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 941us/step - loss: 0.0102 - mean_absolute_error: 0.0713 - val_loss: 0.0108 - val_mean_absolute_error: 0.0730\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01096 to 0.01076, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0101 - mean_absolute_error: 0.0708 - val_loss: 0.0107 - val_mean_absolute_error: 0.0730\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01076 to 0.01070, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0100 - mean_absolute_error: 0.0707 - val_loss: 0.0107 - val_mean_absolute_error: 0.0730\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.01070\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0099 - mean_absolute_error: 0.0703 - val_loss: 0.0105 - val_mean_absolute_error: 0.0718\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01070 to 0.01049, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 940us/step - loss: 0.0098 - mean_absolute_error: 0.0696 - val_loss: 0.0104 - val_mean_absolute_error: 0.0716\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01049 to 0.01043, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0097 - mean_absolute_error: 0.0695 - val_loss: 0.0104 - val_mean_absolute_error: 0.0713\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01043 to 0.01036, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0096 - mean_absolute_error: 0.0691 - val_loss: 0.0103 - val_mean_absolute_error: 0.0709\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01036 to 0.01027, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0096 - mean_absolute_error: 0.0689 - val_loss: 0.0102 - val_mean_absolute_error: 0.0708\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01027 to 0.01021, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0095 - mean_absolute_error: 0.0687 - val_loss: 0.0100 - val_mean_absolute_error: 0.0702\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.01021 to 0.01004, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0094 - mean_absolute_error: 0.0683 - val_loss: 0.0100 - val_mean_absolute_error: 0.0699\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01004 to 0.00999, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0094 - mean_absolute_error: 0.0683 - val_loss: 0.0102 - val_mean_absolute_error: 0.0707\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00999\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0093 - mean_absolute_error: 0.0679 - val_loss: 0.0101 - val_mean_absolute_error: 0.0704\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00999\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0093 - mean_absolute_error: 0.0675 - val_loss: 0.0100 - val_mean_absolute_error: 0.0700\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00999\n",
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0092 - mean_absolute_error: 0.0673 - val_loss: 0.0098 - val_mean_absolute_error: 0.0691\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00999 to 0.00980, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0092 - mean_absolute_error: 0.0671 - val_loss: 0.0097 - val_mean_absolute_error: 0.0692\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00980 to 0.00973, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0091 - mean_absolute_error: 0.0669 - val_loss: 0.0097 - val_mean_absolute_error: 0.0689\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00973 to 0.00972, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0091 - mean_absolute_error: 0.0668 - val_loss: 0.0098 - val_mean_absolute_error: 0.0697\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00972\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0091 - mean_absolute_error: 0.0667 - val_loss: 0.0098 - val_mean_absolute_error: 0.0691\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00972\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0090 - mean_absolute_error: 0.0665 - val_loss: 0.0096 - val_mean_absolute_error: 0.0685\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00972 to 0.00963, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0089 - mean_absolute_error: 0.0661 - val_loss: 0.0096 - val_mean_absolute_error: 0.0684\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00963 to 0.00960, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0089 - mean_absolute_error: 0.0659 - val_loss: 0.0096 - val_mean_absolute_error: 0.0684\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00960 to 0.00956, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0089 - mean_absolute_error: 0.0659 - val_loss: 0.0095 - val_mean_absolute_error: 0.0683\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00956 to 0.00953, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0089 - mean_absolute_error: 0.0659 - val_loss: 0.0095 - val_mean_absolute_error: 0.0681\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00953 to 0.00951, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0088 - mean_absolute_error: 0.0657 - val_loss: 0.0095 - val_mean_absolute_error: 0.0679\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00951 to 0.00947, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0087 - mean_absolute_error: 0.0654 - val_loss: 0.0094 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00947 to 0.00938, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0087 - mean_absolute_error: 0.0652 - val_loss: 0.0094 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00938\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0087 - mean_absolute_error: 0.0652 - val_loss: 0.0093 - val_mean_absolute_error: 0.0676\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00938 to 0.00935, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0087 - mean_absolute_error: 0.0651 - val_loss: 0.0094 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00935\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0087 - mean_absolute_error: 0.0651 - val_loss: 0.0094 - val_mean_absolute_error: 0.0674\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00935\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0086 - mean_absolute_error: 0.0648 - val_loss: 0.0093 - val_mean_absolute_error: 0.0671\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00935 to 0.00925, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0086 - mean_absolute_error: 0.0647 - val_loss: 0.0093 - val_mean_absolute_error: 0.0673\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00925\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0086 - mean_absolute_error: 0.0646 - val_loss: 0.0094 - val_mean_absolute_error: 0.0677\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00925\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0086 - mean_absolute_error: 0.0647 - val_loss: 0.0093 - val_mean_absolute_error: 0.0674\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00925\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0085 - mean_absolute_error: 0.0644 - val_loss: 0.0093 - val_mean_absolute_error: 0.0672\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00925\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0085 - mean_absolute_error: 0.0643 - val_loss: 0.0092 - val_mean_absolute_error: 0.0670\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00925 to 0.00918, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0085 - mean_absolute_error: 0.0642 - val_loss: 0.0092 - val_mean_absolute_error: 0.0668\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00918 to 0.00917, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0084 - mean_absolute_error: 0.0642 - val_loss: 0.0092 - val_mean_absolute_error: 0.0669\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00917\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0084 - mean_absolute_error: 0.0639 - val_loss: 0.0092 - val_mean_absolute_error: 0.0668\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00917\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0084 - mean_absolute_error: 0.0640 - val_loss: 0.0091 - val_mean_absolute_error: 0.0665\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00917 to 0.00909, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0083 - mean_absolute_error: 0.0639 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00909 to 0.00902, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0083 - mean_absolute_error: 0.0636 - val_loss: 0.0090 - val_mean_absolute_error: 0.0664\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00902\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0083 - mean_absolute_error: 0.0638 - val_loss: 0.0091 - val_mean_absolute_error: 0.0664\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00902\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0083 - mean_absolute_error: 0.0637 - val_loss: 0.0091 - val_mean_absolute_error: 0.0667\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00902\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0083 - mean_absolute_error: 0.0635 - val_loss: 0.0091 - val_mean_absolute_error: 0.0667\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00902\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0082 - mean_absolute_error: 0.0632 - val_loss: 0.0090 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00902 to 0.00900, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0092 - val_mean_absolute_error: 0.0669\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00900\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0082 - mean_absolute_error: 0.0634 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00900\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0081 - mean_absolute_error: 0.0631 - val_loss: 0.0089 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00900 to 0.00893, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0082 - mean_absolute_error: 0.0632 - val_loss: 0.0090 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00893\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0081 - mean_absolute_error: 0.0631 - val_loss: 0.0091 - val_mean_absolute_error: 0.0669\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00893\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0081 - mean_absolute_error: 0.0630 - val_loss: 0.0090 - val_mean_absolute_error: 0.0664\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00893\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0081 - mean_absolute_error: 0.0629 - val_loss: 0.0089 - val_mean_absolute_error: 0.0658\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00893 to 0.00889, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0080 - mean_absolute_error: 0.0626 - val_loss: 0.0089 - val_mean_absolute_error: 0.0659\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00889\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0080 - mean_absolute_error: 0.0626 - val_loss: 0.0089 - val_mean_absolute_error: 0.0660\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00889\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0080 - mean_absolute_error: 0.0627 - val_loss: 0.0088 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.00889 to 0.00884, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0080 - mean_absolute_error: 0.0624 - val_loss: 0.0089 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00884\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0080 - mean_absolute_error: 0.0624 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00884 to 0.00879, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0080 - mean_absolute_error: 0.0625 - val_loss: 0.0088 - val_mean_absolute_error: 0.0659\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00879\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.0079 - mean_absolute_error: 0.0623 - val_loss: 0.0088 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00879\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0079 - mean_absolute_error: 0.0621 - val_loss: 0.0089 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00879\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0079 - mean_absolute_error: 0.0624 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00879\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0079 - mean_absolute_error: 0.0622 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00879 to 0.00876, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0079 - mean_absolute_error: 0.0620 - val_loss: 0.0087 - val_mean_absolute_error: 0.0653\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.00876 to 0.00872, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0078 - mean_absolute_error: 0.0618 - val_loss: 0.0088 - val_mean_absolute_error: 0.0656\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00872\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.0078 - mean_absolute_error: 0.0620 - val_loss: 0.0088 - val_mean_absolute_error: 0.0656\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00872\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0078 - mean_absolute_error: 0.0618 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00872\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0078 - mean_absolute_error: 0.0618 - val_loss: 0.0087 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00872 to 0.00870, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0078 - mean_absolute_error: 0.0618 - val_loss: 0.0087 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00870 to 0.00868, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0078 - mean_absolute_error: 0.0617 - val_loss: 0.0087 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00868\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0077 - mean_absolute_error: 0.0616 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00868 to 0.00863, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0077 - mean_absolute_error: 0.0616 - val_loss: 0.0087 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00863\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0077 - mean_absolute_error: 0.0614 - val_loss: 0.0087 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00863\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0077 - mean_absolute_error: 0.0614 - val_loss: 0.0087 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00863\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0076 - mean_absolute_error: 0.0613 - val_loss: 0.0087 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00863\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0077 - mean_absolute_error: 0.0613 - val_loss: 0.0087 - val_mean_absolute_error: 0.0652\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00863\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0076 - mean_absolute_error: 0.0612 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00863 to 0.00858, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0076 - mean_absolute_error: 0.0610 - val_loss: 0.0086 - val_mean_absolute_error: 0.0649\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00858\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0076 - mean_absolute_error: 0.0609 - val_loss: 0.0086 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00858\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0076 - mean_absolute_error: 0.0609 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00858\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00858 to 0.00854, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00854\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00854\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0649\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00854\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0075 - mean_absolute_error: 0.0606 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.00854 to 0.00852, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00852\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0075 - mean_absolute_error: 0.0607 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00852\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0075 - mean_absolute_error: 0.0606 - val_loss: 0.0086 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00852\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0074 - mean_absolute_error: 0.0604 - val_loss: 0.0085 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00852 to 0.00850, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0074 - mean_absolute_error: 0.0604 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00850 to 0.00846, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0074 - mean_absolute_error: 0.0602 - val_loss: 0.0085 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00846\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0075 - mean_absolute_error: 0.0607 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00846\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0074 - mean_absolute_error: 0.0605 - val_loss: 0.0085 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.00846 to 0.00845, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0074 - mean_absolute_error: 0.0603 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00845\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0073 - mean_absolute_error: 0.0601 - val_loss: 0.0084 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00845 to 0.00843, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0073 - mean_absolute_error: 0.0601 - val_loss: 0.0085 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00843\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0073 - mean_absolute_error: 0.0599 - val_loss: 0.0084 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00843 to 0.00843, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0073 - mean_absolute_error: 0.0599 - val_loss: 0.0085 - val_mean_absolute_error: 0.0644\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00843\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0072 - mean_absolute_error: 0.0598 - val_loss: 0.0084 - val_mean_absolute_error: 0.0641\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00843 to 0.00841, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0073 - mean_absolute_error: 0.0600 - val_loss: 0.0084 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00841\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0072 - mean_absolute_error: 0.0598 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00841 to 0.00840, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0072 - mean_absolute_error: 0.0598 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00840 to 0.00834, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0084 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00834\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0076 - mean_absolute_error: 0.0612 - val_loss: 0.0085 - val_mean_absolute_error: 0.0649\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00834\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0073 - mean_absolute_error: 0.0602 - val_loss: 0.0084 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00834\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0084 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00834\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0072 - mean_absolute_error: 0.0596 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00834\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0072 - mean_absolute_error: 0.0595 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00834\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0072 - mean_absolute_error: 0.0596 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00834\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0072 - mean_absolute_error: 0.0595 - val_loss: 0.0083 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00834\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0071 - mean_absolute_error: 0.0595 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00834\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0071 - mean_absolute_error: 0.0593 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00834\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0071 - mean_absolute_error: 0.0592 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00834\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0071 - mean_absolute_error: 0.0592 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00834\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00834 to 0.00833, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0070 - mean_absolute_error: 0.0591 - val_loss: 0.0082 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.00833 to 0.00825, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0071 - mean_absolute_error: 0.0591 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00825\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0071 - mean_absolute_error: 0.0592 - val_loss: 0.0084 - val_mean_absolute_error: 0.0641\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00825\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0071 - mean_absolute_error: 0.0592 - val_loss: 0.0084 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00825\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0070 - mean_absolute_error: 0.0589 - val_loss: 0.0083 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00825\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0070 - mean_absolute_error: 0.0589 - val_loss: 0.0082 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00825 to 0.00824, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0070 - mean_absolute_error: 0.0589 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00824\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0070 - mean_absolute_error: 0.0589 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00824\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0070 - mean_absolute_error: 0.0587 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00824\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0069 - mean_absolute_error: 0.0587 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00824\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0069 - mean_absolute_error: 0.0588 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00824\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0085 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00824\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0083 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00824\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0070 - mean_absolute_error: 0.0592 - val_loss: 0.0082 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.00824 to 0.00819, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0069 - mean_absolute_error: 0.0588 - val_loss: 0.0083 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00819\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0069 - mean_absolute_error: 0.0586 - val_loss: 0.0083 - val_mean_absolute_error: 0.0634\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00819\n",
      "8760/8760 [==============================] - 43s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[156.17906  138.63138  153.4992    23.61857  141.10954  125.107864\n",
      "   63.33884   63.42494   26.59068   61.159767  30.333174  50.933926\n",
      "   85.73702   41.49755   67.801575  42.215935  78.7801   -29.348991\n",
      "   24.438911  27.04965 ]\n",
      " [131.22163   90.24167   92.138336  91.45479  105.004654  87.96163\n",
      "   52.886803  45.14247   51.17451   92.05497   73.259285  79.22443\n",
      "   52.771355  70.23857   62.768246  31.287315  76.054405  35.241627\n",
      "   47.690697  50.965927]]\n",
      "Shape of predictions: (8760, 20)\n",
      "Invert Differencing of multivariate predictions...\n",
      "Shape of org. dataset after shift:  (8760, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  670.179062  509.631378  349.499207  1191.618570   \n",
      "2011-01-01 01:00:00  461.221634  409.241669  138.138336   688.454788   \n",
      "2011-01-01 02:00:00  365.422184  238.454977  163.787956   642.976540   \n",
      "2011-01-01 03:00:00  213.008583  186.343476  144.676174   535.291033   \n",
      "2011-01-01 04:00:00   67.650276  119.747009  107.912908   433.142723   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  709.109543  566.107864  619.338840  710.424938   \n",
      "2011-01-01 01:00:00  440.004654  348.961632  505.886803  484.142471   \n",
      "2011-01-01 02:00:00  473.949722  326.712965  526.185631  320.379894   \n",
      "2011-01-01 03:00:00  272.746830  264.944431  452.318244  201.931156   \n",
      "2011-01-01 04:00:00   55.491920  189.357521  332.650436  156.803783   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  606.590679  414.159767  520.333174  774.933926   \n",
      "2011-01-01 01:00:00  408.174511  270.054970  372.259285  535.224426   \n",
      "2011-01-01 02:00:00  336.041904  263.480165  324.447275  462.632681   \n",
      "2011-01-01 03:00:00  327.248966  202.462334  219.063505  362.405584   \n",
      "2011-01-01 04:00:00  351.161026  168.657776  143.495148  298.814064   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  322.737022  611.497551  575.801575  430.215935   \n",
      "2011-01-01 01:00:00  183.771355  451.238571  356.768246  313.287315   \n",
      "2011-01-01 02:00:00  131.501698  346.762619  467.008492  287.381989   \n",
      "2011-01-01 03:00:00  133.628580  343.519331  360.872967  214.320032   \n",
      "2011-01-01 04:00:00  134.649826  198.486126  163.328979  178.260166   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2011-01-01 00:00:00  528.780098  553.651009  313.438911  493.049650  \n",
      "2011-01-01 01:00:00  417.054405  379.241627   90.690697  350.965927  \n",
      "2011-01-01 02:00:00  473.755054  274.704191   31.074787  247.475138  \n",
      "2011-01-01 03:00:00  369.102520  238.663413   -9.759856  196.960388  \n",
      "2011-01-01 04:00:00  202.689987  161.637527    3.601525  180.658726  \n",
      "RMSE per TS 0 for model: results_2011: 69.69191864131392\n",
      "RMSE per TS 1 for model: results_2011: 82.90730335788147\n",
      "RMSE per TS 2 for model: results_2011: 78.54867217592387\n",
      "RMSE per TS 3 for model: results_2011: 77.23095060754277\n",
      "RMSE per TS 4 for model: results_2011: 65.59564954738673\n",
      "RMSE per TS 5 for model: results_2011: 78.24374782338859\n",
      "RMSE per TS 6 for model: results_2011: 65.55186901828007\n",
      "RMSE per TS 7 for model: results_2011: 60.47264345618708\n",
      "RMSE per TS 8 for model: results_2011: 71.61709180393338\n",
      "RMSE per TS 9 for model: results_2011: 76.69147299688863\n",
      "RMSE per TS 10 for model: results_2011: 75.70401192455448\n",
      "RMSE per TS 11 for model: results_2011: 56.507691453203066\n",
      "RMSE per TS 12 for model: results_2011: 59.09541915447131\n",
      "RMSE per TS 13 for model: results_2011: 56.76839828508881\n",
      "RMSE per TS 14 for model: results_2011: 53.88804957471946\n",
      "RMSE per TS 15 for model: results_2011: 49.33569185587107\n",
      "RMSE per TS 16 for model: results_2011: 46.72212252718391\n",
      "RMSE per TS 17 for model: results_2011: 53.09672611252003\n",
      "RMSE per TS 18 for model: results_2011: 86.74028682522793\n",
      "RMSE per TS 19 for model: results_2011: 44.72266196070589\n",
      "Avg.RMSE for multivariate model: results_2011: 65.45661895511361\n",
      "8784/8784 [==============================] - 42s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[  58.632565      8.073216    -13.834223    -57.937252     55.603394\n",
      "    49.632732     -5.666764     -2.0329225   -56.521538     -8.797923\n",
      "   -54.18278       4.02266      14.223063     -3.3133686   -16.98063\n",
      "     4.7249074     2.6547716   -64.18387     -15.656312     -3.8114634 ]\n",
      " [  -0.67557263  -21.780762   -123.33533    -119.8192       20.621681\n",
      "    -0.722564    -54.351963    -59.79859     -93.71976     -53.797825\n",
      "   -30.204304    -36.549484    -31.663538    -45.14742     -27.185091\n",
      "   -36.15397     -23.035666    -69.91758     -97.916275    -31.827028  ]]\n",
      "Shape of predictions: (8784, 20)\n",
      "Invert Differencing of multivariate predictions...\n",
      "Shape of org. dataset after shift:  (8784, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  604.632565  485.073216  168.165777  1253.062748   \n",
      "2012-01-01 01:00:00  451.324427  346.219238  -25.335327   582.180801   \n",
      "2012-01-01 02:00:00  454.703053  220.740261    0.356201   567.941391   \n",
      "2012-01-01 03:00:00  294.469429  227.814896   25.883965   542.771008   \n",
      "2012-01-01 04:00:00  140.889471  215.709235  154.151381   457.503468   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  642.603394  504.632732  646.333236  732.967077   \n",
      "2012-01-01 01:00:00  402.621681  308.277436  457.648037  454.201408   \n",
      "2012-01-01 02:00:00  427.830912  307.275948  489.049950  243.591480   \n",
      "2012-01-01 03:00:00  342.916832  321.166851  490.455109  204.805279   \n",
      "2012-01-01 04:00:00  169.674301  223.026157  469.039935  209.234715   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  751.478462  401.202077  573.817219  726.022660   \n",
      "2012-01-01 01:00:00  392.280243  191.202175  327.795696  475.450516   \n",
      "2012-01-01 02:00:00  317.588669  216.006584  262.071632  450.843830   \n",
      "2012-01-01 03:00:00  332.354279  243.042637  309.889095  432.345562   \n",
      "2012-01-01 04:00:00  477.563375  212.886372  267.124039  456.334244   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  297.223063  670.686631  529.019369  483.724907   \n",
      "2012-01-01 01:00:00  159.336462  489.852581  341.814909  380.846031   \n",
      "2012-01-01 02:00:00  119.658470  327.107384  522.731762  281.915359   \n",
      "2012-01-01 03:00:00  138.823011  255.895802  392.804424  240.319941   \n",
      "2012-01-01 04:00:00  194.214647  225.444450  293.266922  300.566172   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2012-01-01 00:00:00  467.654772  636.816132  203.343688  508.188537  \n",
      "2012-01-01 01:00:00  364.964334  281.082420  -47.916275  296.172972  \n",
      "2012-01-01 02:00:00  473.467121  221.000481  -91.203156  211.780453  \n",
      "2012-01-01 03:00:00  403.477421  179.668678  -61.439098  204.610172  \n",
      "2012-01-01 04:00:00  275.017607  160.613697  -21.748857  175.269892  \n",
      "RMSE per TS 0 for model: results_2012: 73.33541303703697\n",
      "RMSE per TS 1 for model: results_2012: 87.18658046350703\n",
      "RMSE per TS 2 for model: results_2012: 78.01388930417524\n",
      "RMSE per TS 3 for model: results_2012: 77.68047309339205\n",
      "RMSE per TS 4 for model: results_2012: 67.73568160993565\n",
      "RMSE per TS 5 for model: results_2012: 81.08098341128583\n",
      "RMSE per TS 6 for model: results_2012: 64.94506719909997\n",
      "RMSE per TS 7 for model: results_2012: 64.83688946069026\n",
      "RMSE per TS 8 for model: results_2012: 75.51213154633037\n",
      "RMSE per TS 9 for model: results_2012: 78.71450210910542\n",
      "RMSE per TS 10 for model: results_2012: 75.47936809564554\n",
      "RMSE per TS 11 for model: results_2012: 55.422471253273834\n",
      "RMSE per TS 12 for model: results_2012: 59.489661487245975\n",
      "RMSE per TS 13 for model: results_2012: 57.55735191803381\n",
      "RMSE per TS 14 for model: results_2012: 52.91177290693211\n",
      "RMSE per TS 15 for model: results_2012: 52.2984627457765\n",
      "RMSE per TS 16 for model: results_2012: 48.0720395801905\n",
      "RMSE per TS 17 for model: results_2012: 55.656263411930816\n",
      "RMSE per TS 18 for model: results_2012: 92.27515204841106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE per TS 19 for model: results_2012: 45.46370899216406\n",
      "Avg.RMSE for multivariate model: results_2012: 67.18339318370815\n"
     ]
    }
   ],
   "source": [
    "#default feature creation with Multivar LSTM model:\n",
    "multLSTM = MultivariateLSTM()\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "\n",
    "#slice input data for 20 busiest areas:\n",
    "multivar_ts_series = ts_20largest.loc['2009':'2014']\n",
    "\n",
    "\n",
    "#create full model:\n",
    "#returns predictions for all 20 areas for given years (validation year & test year) while model is trained on data of 2009 & 2010\n",
    "results_i = multLSTM.create_full_pred_model(multivar_ts_series, start_train_year, \n",
    "                                            last_train_set_year, validation_set_year, \n",
    "                                            test_set_year, 'TEST_multivarLSTM', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:00:21.757987Z",
     "start_time": "2019-10-24T23:15:29.107594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_validation_set_year  2011\n",
      "start_test_set_year  2012\n",
      "end_validation_set_year  None\n",
      "end_test_set_year  None\n",
      "generate data..\n",
      "st_valid_year:  2011\n",
      "end_valid_year:  2011\n",
      "st_test_year:  2012\n",
      "end_test_year:  2012\n",
      "create data of area  237\n",
      "get lag encodings of area  237\n",
      "create data of area  161\n",
      "get lag encodings of area  161\n",
      "create data of area  230\n",
      "get lag encodings of area  230\n",
      "create data of area  79\n",
      "get lag encodings of area  79\n",
      "create data of area  236\n",
      "get lag encodings of area  236\n",
      "create data of area  162\n",
      "get lag encodings of area  162\n",
      "create data of area  170\n",
      "get lag encodings of area  170\n",
      "create data of area  234\n",
      "get lag encodings of area  234\n",
      "create data of area  48\n",
      "get lag encodings of area  48\n",
      "create data of area  186\n",
      "get lag encodings of area  186\n",
      "create data of area  142\n",
      "get lag encodings of area  142\n",
      "create data of area  107\n",
      "get lag encodings of area  107\n",
      "create data of area  163\n",
      "get lag encodings of area  163\n",
      "create data of area  68\n",
      "get lag encodings of area  68\n",
      "create data of area  239\n",
      "get lag encodings of area  239\n",
      "create data of area  164\n",
      "get lag encodings of area  164\n",
      "create data of area  141\n",
      "get lag encodings of area  141\n",
      "create data of area  249\n",
      "get lag encodings of area  249\n",
      "create data of area  138\n",
      "get lag encodings of area  138\n",
      "create data of area  90\n",
      "get lag encodings of area  90\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "X_train shape of area237 before concat with other areas:  (17351, 168, 1)\n",
      "X_valid shape of area237 before concat with other areas:  (8760, 168, 1)\n",
      "X_test shape of area237 before concat with other areas:  (8784, 168, 1)\n",
      "y_train shape of area237 before concat with other areas:  (17351, 1)\n",
      "y_valid shape of area237 before concat with other areas:  (8760, 1)\n",
      "y_test shape of area237 before concat with other areas:  (8784, 1)\n",
      "Addtional features of each lag are concatenated on axis = 2 \n",
      "lags shape of train set: (17351, 168, 11)\n",
      "Xtrain_shape  (17351, 168, 20)\n",
      "final concatenated shape of X_train:  (17351, 168, 31)\n",
      "final concatenated shape of X_valid:  (8760, 168, 31)\n",
      "final concatenated shape of X_test:  (8784, 168, 31)\n",
      "final concatenated shape of y_train:  (17351, 20)\n",
      "final concatenated shape of y_valid:  (8760, 20)\n",
      "final concatenated shape of y_test:  (8784, 20)\n",
      "scaler type:  MinMaxScaler(copy=True, feature_range=(-1, 1))\n",
      "create stacked LSTM 2 layer non-stateful model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Regular 2H-LSTM Model is created...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.0371 - mean_absolute_error: 0.1438 - val_loss: 0.0311 - val_mean_absolute_error: 0.1284\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03107, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0262 - mean_absolute_error: 0.1187 - val_loss: 0.0241 - val_mean_absolute_error: 0.1133\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03107 to 0.02414, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0214 - mean_absolute_error: 0.1071 - val_loss: 0.0206 - val_mean_absolute_error: 0.1040\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02414 to 0.02063, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0188 - mean_absolute_error: 0.0995 - val_loss: 0.0190 - val_mean_absolute_error: 0.0989\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02063 to 0.01902, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0173 - mean_absolute_error: 0.0949 - val_loss: 0.0175 - val_mean_absolute_error: 0.0944\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01902 to 0.01747, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0158 - mean_absolute_error: 0.0907 - val_loss: 0.0158 - val_mean_absolute_error: 0.0905\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01747 to 0.01581, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0144 - mean_absolute_error: 0.0866 - val_loss: 0.0145 - val_mean_absolute_error: 0.0859\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01581 to 0.01445, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0134 - mean_absolute_error: 0.0833 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01445 to 0.01347, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0127 - mean_absolute_error: 0.0808 - val_loss: 0.0128 - val_mean_absolute_error: 0.0802\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01347 to 0.01281, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0121 - mean_absolute_error: 0.0785 - val_loss: 0.0125 - val_mean_absolute_error: 0.0787\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01281 to 0.01245, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0117 - mean_absolute_error: 0.0772 - val_loss: 0.0119 - val_mean_absolute_error: 0.0769\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01245 to 0.01187, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0112 - mean_absolute_error: 0.0756 - val_loss: 0.0116 - val_mean_absolute_error: 0.0757\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01187 to 0.01159, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0110 - mean_absolute_error: 0.0745 - val_loss: 0.0116 - val_mean_absolute_error: 0.0756\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01159\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0108 - mean_absolute_error: 0.0739 - val_loss: 0.0112 - val_mean_absolute_error: 0.0744\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01159 to 0.01117, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0106 - mean_absolute_error: 0.0729 - val_loss: 0.0111 - val_mean_absolute_error: 0.0740\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01117 to 0.01108, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0104 - mean_absolute_error: 0.0724 - val_loss: 0.0109 - val_mean_absolute_error: 0.0738\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01108 to 0.01093, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0102 - mean_absolute_error: 0.0715 - val_loss: 0.0107 - val_mean_absolute_error: 0.0727\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01093 to 0.01069, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0102 - mean_absolute_error: 0.0713 - val_loss: 0.0106 - val_mean_absolute_error: 0.0722\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01069 to 0.01062, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0100 - mean_absolute_error: 0.0706 - val_loss: 0.0106 - val_mean_absolute_error: 0.0725\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01062 to 0.01061, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0099 - mean_absolute_error: 0.0700 - val_loss: 0.0105 - val_mean_absolute_error: 0.0716\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01061 to 0.01048, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0097 - mean_absolute_error: 0.0696 - val_loss: 0.0103 - val_mean_absolute_error: 0.0712\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01048 to 0.01035, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0096 - mean_absolute_error: 0.0692 - val_loss: 0.0103 - val_mean_absolute_error: 0.0714\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01035 to 0.01033, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.0096 - mean_absolute_error: 0.0690 - val_loss: 0.0103 - val_mean_absolute_error: 0.0711\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01033 to 0.01033, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0095 - mean_absolute_error: 0.0686 - val_loss: 0.0102 - val_mean_absolute_error: 0.0709\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01033 to 0.01019, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0095 - mean_absolute_error: 0.0685 - val_loss: 0.0100 - val_mean_absolute_error: 0.0705\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01019 to 0.01005, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0094 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0698\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01005 to 0.00998, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0093 - mean_absolute_error: 0.0678 - val_loss: 0.0100 - val_mean_absolute_error: 0.0697\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00998\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0092 - mean_absolute_error: 0.0675 - val_loss: 0.0098 - val_mean_absolute_error: 0.0691\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.00998 to 0.00983, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0092 - mean_absolute_error: 0.0672 - val_loss: 0.0098 - val_mean_absolute_error: 0.0692\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00983 to 0.00980, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0091 - mean_absolute_error: 0.0672 - val_loss: 0.0099 - val_mean_absolute_error: 0.0700\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00980\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0091 - mean_absolute_error: 0.0669 - val_loss: 0.0097 - val_mean_absolute_error: 0.0687\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00980 to 0.00969, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0091 - mean_absolute_error: 0.0668 - val_loss: 0.0096 - val_mean_absolute_error: 0.0686\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00969 to 0.00965, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0090 - mean_absolute_error: 0.0665 - val_loss: 0.0096 - val_mean_absolute_error: 0.0682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00965 to 0.00956, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0090 - mean_absolute_error: 0.0665 - val_loss: 0.0096 - val_mean_absolute_error: 0.0686\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00956\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0089 - mean_absolute_error: 0.0660 - val_loss: 0.0096 - val_mean_absolute_error: 0.0685\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00956\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0088 - mean_absolute_error: 0.0660 - val_loss: 0.0098 - val_mean_absolute_error: 0.0692\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00956\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0088 - mean_absolute_error: 0.0659 - val_loss: 0.0095 - val_mean_absolute_error: 0.0682\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00956 to 0.00953, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0088 - mean_absolute_error: 0.0655 - val_loss: 0.0096 - val_mean_absolute_error: 0.0682\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00953\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0087 - mean_absolute_error: 0.0652 - val_loss: 0.0093 - val_mean_absolute_error: 0.0675\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00953 to 0.00935, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0087 - mean_absolute_error: 0.0653 - val_loss: 0.0094 - val_mean_absolute_error: 0.0675\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00935\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0086 - mean_absolute_error: 0.0650 - val_loss: 0.0093 - val_mean_absolute_error: 0.0672\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00935 to 0.00929, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0086 - mean_absolute_error: 0.0648 - val_loss: 0.0094 - val_mean_absolute_error: 0.0677\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00929\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0085 - mean_absolute_error: 0.0647 - val_loss: 0.0095 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00929\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0085 - mean_absolute_error: 0.0645 - val_loss: 0.0094 - val_mean_absolute_error: 0.0678\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00929\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0084 - mean_absolute_error: 0.0643 - val_loss: 0.0092 - val_mean_absolute_error: 0.0667\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00929 to 0.00917, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0085 - mean_absolute_error: 0.0644 - val_loss: 0.0092 - val_mean_absolute_error: 0.0669\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00917\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0084 - mean_absolute_error: 0.0640 - val_loss: 0.0092 - val_mean_absolute_error: 0.0666\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00917 to 0.00915, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0084 - mean_absolute_error: 0.0639 - val_loss: 0.0092 - val_mean_absolute_error: 0.0668\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00915\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0083 - mean_absolute_error: 0.0639 - val_loss: 0.0091 - val_mean_absolute_error: 0.0667\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00915 to 0.00914, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0083 - mean_absolute_error: 0.0638 - val_loss: 0.0091 - val_mean_absolute_error: 0.0666\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00914 to 0.00909, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0083 - mean_absolute_error: 0.0636 - val_loss: 0.0090 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00909 to 0.00904, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.0082 - mean_absolute_error: 0.0635 - val_loss: 0.0091 - val_mean_absolute_error: 0.0665\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00904\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0083 - mean_absolute_error: 0.0636 - val_loss: 0.0091 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00904\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0090 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00904 to 0.00900, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0089 - val_mean_absolute_error: 0.0659\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00900 to 0.00892, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0089 - val_mean_absolute_error: 0.0658\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00892\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0081 - mean_absolute_error: 0.0629 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00892\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0081 - mean_absolute_error: 0.0628 - val_loss: 0.0090 - val_mean_absolute_error: 0.0661\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00892\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0080 - mean_absolute_error: 0.0628 - val_loss: 0.0090 - val_mean_absolute_error: 0.0663\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00892\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0081 - mean_absolute_error: 0.0629 - val_loss: 0.0089 - val_mean_absolute_error: 0.0662\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00892\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0080 - mean_absolute_error: 0.0626 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.00892 to 0.00880, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0080 - mean_absolute_error: 0.0626 - val_loss: 0.0088 - val_mean_absolute_error: 0.0656\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00880\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0080 - mean_absolute_error: 0.0626 - val_loss: 0.0089 - val_mean_absolute_error: 0.0657\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00880\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0079 - mean_absolute_error: 0.0621 - val_loss: 0.0088 - val_mean_absolute_error: 0.0653\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00880 to 0.00878, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0079 - mean_absolute_error: 0.0622 - val_loss: 0.0088 - val_mean_absolute_error: 0.0653\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00878\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0079 - mean_absolute_error: 0.0620 - val_loss: 0.0088 - val_mean_absolute_error: 0.0655\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00878\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0079 - mean_absolute_error: 0.0621 - val_loss: 0.0088 - val_mean_absolute_error: 0.0655\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00878\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0078 - mean_absolute_error: 0.0619 - val_loss: 0.0088 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00878 to 0.00877, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0078 - mean_absolute_error: 0.0619 - val_loss: 0.0087 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.00877 to 0.00872, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0078 - mean_absolute_error: 0.0618 - val_loss: 0.0087 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00872 to 0.00869, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0078 - mean_absolute_error: 0.0616 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00869\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0078 - mean_absolute_error: 0.0617 - val_loss: 0.0087 - val_mean_absolute_error: 0.0649\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00869 to 0.00866, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0078 - mean_absolute_error: 0.0617 - val_loss: 0.0088 - val_mean_absolute_error: 0.0654\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00866\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0077 - mean_absolute_error: 0.0615 - val_loss: 0.0086 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.00866 to 0.00860, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0077 - mean_absolute_error: 0.0614 - val_loss: 0.0087 - val_mean_absolute_error: 0.0651\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00860\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0076 - mean_absolute_error: 0.0612 - val_loss: 0.0086 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00860\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0076 - mean_absolute_error: 0.0613 - val_loss: 0.0086 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00860 to 0.00858, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0076 - mean_absolute_error: 0.0611 - val_loss: 0.0086 - val_mean_absolute_error: 0.0647\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00858\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0076 - mean_absolute_error: 0.0609 - val_loss: 0.0086 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00858\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0075 - mean_absolute_error: 0.0609 - val_loss: 0.0085 - val_mean_absolute_error: 0.0644\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00858 to 0.00854, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0648\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00854\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0075 - mean_absolute_error: 0.0607 - val_loss: 0.0085 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00854 to 0.00852, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0075 - mean_absolute_error: 0.0608 - val_loss: 0.0086 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00852\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0075 - mean_absolute_error: 0.0607 - val_loss: 0.0086 - val_mean_absolute_error: 0.0646\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00852\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0075 - mean_absolute_error: 0.0606 - val_loss: 0.0086 - val_mean_absolute_error: 0.0644\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00852\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0074 - mean_absolute_error: 0.0604 - val_loss: 0.0085 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.00852 to 0.00849, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0074 - mean_absolute_error: 0.0603 - val_loss: 0.0085 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00849\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0074 - mean_absolute_error: 0.0604 - val_loss: 0.0085 - val_mean_absolute_error: 0.0642\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00849 to 0.00847, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0074 - mean_absolute_error: 0.0604 - val_loss: 0.0085 - val_mean_absolute_error: 0.0645\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00847\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0074 - mean_absolute_error: 0.0603 - val_loss: 0.0084 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00847 to 0.00837, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0073 - mean_absolute_error: 0.0600 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00837\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0073 - mean_absolute_error: 0.0599 - val_loss: 0.0085 - val_mean_absolute_error: 0.0643\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00837\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0073 - mean_absolute_error: 0.0600 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00837\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0073 - mean_absolute_error: 0.0598 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00837 to 0.00835, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0073 - mean_absolute_error: 0.0599 - val_loss: 0.0084 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00835\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0073 - mean_absolute_error: 0.0600 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00835\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0073 - mean_absolute_error: 0.0598 - val_loss: 0.0085 - val_mean_absolute_error: 0.0644\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00835\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0072 - mean_absolute_error: 0.0597 - val_loss: 0.0084 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00835\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0072 - mean_absolute_error: 0.0596 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00835\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0072 - mean_absolute_error: 0.0596 - val_loss: 0.0084 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00835\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0072 - mean_absolute_error: 0.0595 - val_loss: 0.0084 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00835\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.00835 to 0.00831, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00831\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0071 - mean_absolute_error: 0.0593 - val_loss: 0.0085 - val_mean_absolute_error: 0.0641\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00831\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00831\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00831\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0071 - mean_absolute_error: 0.0594 - val_loss: 0.0084 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00831\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0071 - mean_absolute_error: 0.0592 - val_loss: 0.0084 - val_mean_absolute_error: 0.0640\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00831\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0071 - mean_absolute_error: 0.0590 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.00831 to 0.00829, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0070 - mean_absolute_error: 0.0590 - val_loss: 0.0082 - val_mean_absolute_error: 0.0634\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00829 to 0.00824, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0070 - mean_absolute_error: 0.0590 - val_loss: 0.0083 - val_mean_absolute_error: 0.0638\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00824\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0070 - mean_absolute_error: 0.0589 - val_loss: 0.0083 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00824\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0070 - mean_absolute_error: 0.0587 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00824\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0070 - mean_absolute_error: 0.0588 - val_loss: 0.0082 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.00824 to 0.00822, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0070 - mean_absolute_error: 0.0588 - val_loss: 0.0083 - val_mean_absolute_error: 0.0637\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00822\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0070 - mean_absolute_error: 0.0588 - val_loss: 0.0083 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00822\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0069 - mean_absolute_error: 0.0586 - val_loss: 0.0083 - val_mean_absolute_error: 0.0634\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00822\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0069 - mean_absolute_error: 0.0585 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00822\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0069 - mean_absolute_error: 0.0586 - val_loss: 0.0082 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.00822 to 0.00818, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0069 - mean_absolute_error: 0.0585 - val_loss: 0.0081 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00818 to 0.00814, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0069 - mean_absolute_error: 0.0584 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00814\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0069 - mean_absolute_error: 0.0583 - val_loss: 0.0082 - val_mean_absolute_error: 0.0634\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00814\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0068 - mean_absolute_error: 0.0583 - val_loss: 0.0083 - val_mean_absolute_error: 0.0635\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00814\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0068 - mean_absolute_error: 0.0581 - val_loss: 0.0083 - val_mean_absolute_error: 0.0636\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00814\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0068 - mean_absolute_error: 0.0582 - val_loss: 0.0081 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00814\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0068 - mean_absolute_error: 0.0581 - val_loss: 0.0082 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00814\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0068 - mean_absolute_error: 0.0582 - val_loss: 0.0082 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00814\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0068 - mean_absolute_error: 0.0580 - val_loss: 0.0081 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00814\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0067 - mean_absolute_error: 0.0578 - val_loss: 0.0084 - val_mean_absolute_error: 0.0639\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00814\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.0067 - mean_absolute_error: 0.0580 - val_loss: 0.0081 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00814 to 0.00813, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0068 - mean_absolute_error: 0.0580 - val_loss: 0.0083 - val_mean_absolute_error: 0.0633\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00813\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0067 - mean_absolute_error: 0.0579 - val_loss: 0.0082 - val_mean_absolute_error: 0.0631\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00813\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0067 - mean_absolute_error: 0.0579 - val_loss: 0.0082 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00813\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.0068 - mean_absolute_error: 0.0580 - val_loss: 0.0081 - val_mean_absolute_error: 0.0628\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.00813 to 0.00810, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0067 - mean_absolute_error: 0.0578 - val_loss: 0.0082 - val_mean_absolute_error: 0.0633\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00810\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0067 - mean_absolute_error: 0.0578 - val_loss: 0.0081 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00810\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0066 - mean_absolute_error: 0.0576 - val_loss: 0.0082 - val_mean_absolute_error: 0.0634\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00810\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0067 - mean_absolute_error: 0.0577 - val_loss: 0.0081 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00810\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0066 - mean_absolute_error: 0.0576 - val_loss: 0.0081 - val_mean_absolute_error: 0.0627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.00810 to 0.00808, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0066 - mean_absolute_error: 0.0574 - val_loss: 0.0082 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00808\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0066 - mean_absolute_error: 0.0574 - val_loss: 0.0081 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00808\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0066 - mean_absolute_error: 0.0574 - val_loss: 0.0081 - val_mean_absolute_error: 0.0628\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.00808 to 0.00807, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0066 - mean_absolute_error: 0.0573 - val_loss: 0.0082 - val_mean_absolute_error: 0.0633\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00807\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0065 - mean_absolute_error: 0.0573 - val_loss: 0.0080 - val_mean_absolute_error: 0.0627\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.00807 to 0.00804, saving model to /media/vincent/harddrive/ML-Projects_all/NY_Cab_Project/NY_Cab_Data/results/Stacked_LSTM/Hyperparam_tuning_y2011/Best_Models/TEST_Runtime_bestmodel.h5\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0065 - mean_absolute_error: 0.0572 - val_loss: 0.0081 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00804\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 920us/step - loss: 0.0065 - mean_absolute_error: 0.0572 - val_loss: 0.0081 - val_mean_absolute_error: 0.0629\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00804\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0065 - mean_absolute_error: 0.0572 - val_loss: 0.0081 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00804\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0065 - mean_absolute_error: 0.0571 - val_loss: 0.0081 - val_mean_absolute_error: 0.0630\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00804\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0065 - mean_absolute_error: 0.0571 - val_loss: 0.0081 - val_mean_absolute_error: 0.0632\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00804\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0065 - mean_absolute_error: 0.0571 - val_loss: 0.0081 - val_mean_absolute_error: 0.0627\n",
      "#Current LearningRate:  0.001\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00804\n",
      "8760/8760 [==============================] - 43s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[140.23648   135.4591     88.59923   -17.308598   98.66503   142.38945\n",
      "   51.89218    67.11075   -11.357345   91.39366    37.239082   20.004692\n",
      "   78.72653     5.461427   55.9458     45.780975   38.625946  -38.392754\n",
      "  -27.776463  -28.769764 ]\n",
      " [ 86.63855    53.371174  -23.92348   115.19798    89.634926   76.78645\n",
      "  105.0834     52.6244     42.85355    93.84395    54.891068  105.93771\n",
      "   21.401318   69.129845   84.7119     45.277653   73.98786    20.790972\n",
      "    2.6954057  31.637136 ]]\n",
      "Shape of predictions: (8760, 20)\n",
      "Invert Differencing of multivariate predictions...\n",
      "Shape of org. dataset after shift:  (8760, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  654.236481  506.459106  284.599228  1150.691402   \n",
      "2011-01-01 01:00:00  416.638550  372.371174   22.076521   712.197983   \n",
      "2011-01-01 02:00:00  394.570173  227.412893   88.019474   624.008053   \n",
      "2011-01-01 03:00:00  265.246471  192.707518  151.339111   493.934023   \n",
      "2011-01-01 04:00:00  113.954117  184.559974  170.601028   473.949272   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  666.665031  583.389450  607.892181  714.110748   \n",
      "2011-01-01 01:00:00  424.634926  337.786453  558.083397  491.624401   \n",
      "2011-01-01 02:00:00  424.583416  361.969761  568.942574  311.584495   \n",
      "2011-01-01 03:00:00  284.558762  316.510229  467.156738  199.801967   \n",
      "2011-01-01 04:00:00  112.969326  280.868092  387.092688  189.539240   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  568.642655  444.393661  527.239082  744.004692   \n",
      "2011-01-01 01:00:00  399.853550  271.843948  353.891068  561.937714   \n",
      "2011-01-01 02:00:00  386.305519  251.544378  342.748905  496.395100   \n",
      "2011-01-01 03:00:00  393.432304  193.436490  267.800772  387.141127   \n",
      "2011-01-01 04:00:00  414.532157  198.825374  217.365251  360.649532   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  315.726532  575.461427  563.945801  433.780975   \n",
      "2011-01-01 01:00:00  152.401318  450.129845  378.711899  327.277653   \n",
      "2011-01-01 02:00:00  150.004082  361.166679  476.798262  295.085150   \n",
      "2011-01-01 03:00:00  185.852295  373.774139  400.825082  220.629502   \n",
      "2011-01-01 04:00:00  200.324692  258.178338  235.243761  214.980372   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2011-01-01 00:00:00  488.625946  544.607246  261.223537  437.230236  \n",
      "2011-01-01 01:00:00  414.987862  364.790972   45.695406  331.637136  \n",
      "2011-01-01 02:00:00  465.675188  225.513643   35.624192  246.419691  \n",
      "2011-01-01 03:00:00  382.310093  208.238195    5.113045  216.210878  \n",
      "2011-01-01 04:00:00  244.175488  171.314350   12.916551  236.176842  \n",
      "RMSE per TS 0 for model: results_2011: 69.25720899995974\n",
      "RMSE per TS 1 for model: results_2011: 79.5242608015167\n",
      "RMSE per TS 2 for model: results_2011: 77.36542957839048\n",
      "RMSE per TS 3 for model: results_2011: 76.9196789355899\n",
      "RMSE per TS 4 for model: results_2011: 63.98428966521782\n",
      "RMSE per TS 5 for model: results_2011: 76.61598923960045\n",
      "RMSE per TS 6 for model: results_2011: 64.36455947012901\n",
      "RMSE per TS 7 for model: results_2011: 60.183162316930414\n",
      "RMSE per TS 8 for model: results_2011: 70.5895729103972\n",
      "RMSE per TS 9 for model: results_2011: 76.77775189181666\n",
      "RMSE per TS 10 for model: results_2011: 75.36203813208338\n",
      "RMSE per TS 11 for model: results_2011: 57.273765707890995\n",
      "RMSE per TS 12 for model: results_2011: 58.68871445778331\n",
      "RMSE per TS 13 for model: results_2011: 56.44626298054628\n",
      "RMSE per TS 14 for model: results_2011: 53.51844711551424\n",
      "RMSE per TS 15 for model: results_2011: 49.04606621468496\n",
      "RMSE per TS 16 for model: results_2011: 46.09191500653341\n",
      "RMSE per TS 17 for model: results_2011: 52.93214691532835\n",
      "RMSE per TS 18 for model: results_2011: 84.49737477072135\n",
      "RMSE per TS 19 for model: results_2011: 44.95668280242217\n",
      "Avg.RMSE for multivariate model: results_2011: 64.71976589565284\n",
      "8784/8784 [==============================] - 43s 5ms/step\n",
      "First 2 scaled predictions\n",
      "[[ 2.57981949e+01  1.28873291e+01 -3.37063217e+01  2.61311005e+02\n",
      "   8.31744671e+00  1.19880190e+01  1.08286140e+02  1.02720268e+02\n",
      "   7.15996857e+01  7.85188217e+01  1.06449745e+02  1.35053162e+02\n",
      "   2.95269470e+01  9.55934448e+01  4.47274551e+01  7.55770798e+01\n",
      "   4.25328712e+01  8.41102371e+01 -7.61366940e+00  7.66033707e+01]\n",
      " [-8.21206188e+00 -2.94459991e+01 -7.36244812e+01  9.12074356e+01\n",
      "  -1.38603020e+01 -5.07868843e+01  3.63648262e+01  3.57092133e+01\n",
      "   2.61597157e+01  1.66681557e+01 -5.48219719e+01  4.36857033e+01\n",
      "  -2.01603451e+01  2.86246624e+01 -1.05208404e-01  3.26542625e+01\n",
      "  -1.12274065e+01  2.80817280e+01 -4.68631315e+00  5.01575050e+01]]\n",
      "Shape of predictions: (8784, 20)\n",
      "Invert Differencing of multivariate predictions...\n",
      "Shape of org. dataset after shift:  (8784, 20)\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  571.798195  489.887329  148.293678  1572.311005   \n",
      "2012-01-01 01:00:00  443.787938  338.554001   24.375519   793.207436   \n",
      "2012-01-01 02:00:00  421.820877  172.923584   14.974312   614.185463   \n",
      "2012-01-01 03:00:00  265.029610  178.904373   16.668541   446.923889   \n",
      "2012-01-01 04:00:00  116.728542  193.951611  111.944725   244.527130   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  595.317447  466.988019  760.286140  837.720268   \n",
      "2012-01-01 01:00:00  368.139698  258.213116  548.364826  549.709213   \n",
      "2012-01-01 02:00:00  370.995590  240.507080  476.524776  273.770206   \n",
      "2012-01-01 03:00:00  303.006721  261.742493  404.329063  174.289505   \n",
      "2012-01-01 04:00:00  142.136917  184.690056  357.612419  142.705971   \n",
      "\n",
      "                             48         186         142         107  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  879.599686  488.518822  734.449745  857.053162   \n",
      "2012-01-01 01:00:00  512.159716  261.668156  303.178028  555.685703   \n",
      "2012-01-01 02:00:00  353.998859  208.440201  199.029907  427.288879   \n",
      "2012-01-01 03:00:00  308.420387  238.745281  243.484619  343.868835   \n",
      "2012-01-01 04:00:00  406.100388  208.966576  243.386230  356.608734   \n",
      "\n",
      "                            163          68         239         164  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  312.526947  769.593445  590.727455  554.577080   \n",
      "2012-01-01 01:00:00  170.839655  563.624662  368.894792  449.654263   \n",
      "2012-01-01 02:00:00  105.302231  364.960155  479.117783  303.222551   \n",
      "2012-01-01 03:00:00  120.711483  267.768305  334.015724  227.359146   \n",
      "2012-01-01 04:00:00  170.197136  175.831184  246.014816  272.960381   \n",
      "\n",
      "                            141         249         138          90  \n",
      "date                                                                 \n",
      "2012-01-01 00:00:00  507.532871  785.110237  211.386331  588.603371  \n",
      "2012-01-01 01:00:00  376.772593  379.081728   45.313687  378.157505  \n",
      "2012-01-01 02:00:00  426.684525  228.465473  -32.359119  250.616921  \n",
      "2012-01-01 03:00:00  351.324402  130.200462  -33.579327  201.983383  \n",
      "2012-01-01 04:00:00  227.172035   90.405769  -26.387011  140.327438  \n",
      "RMSE per TS 0 for model: results_2012: 71.09086143686295\n",
      "RMSE per TS 1 for model: results_2012: 82.94797337296897\n",
      "RMSE per TS 2 for model: results_2012: 77.56406683950975\n",
      "RMSE per TS 3 for model: results_2012: 77.72427489974561\n",
      "RMSE per TS 4 for model: results_2012: 66.04242994181176\n",
      "RMSE per TS 5 for model: results_2012: 79.0304503788963\n",
      "RMSE per TS 6 for model: results_2012: 63.89283155473734\n",
      "RMSE per TS 7 for model: results_2012: 63.64803702928355\n",
      "RMSE per TS 8 for model: results_2012: 75.22831045366244\n",
      "RMSE per TS 9 for model: results_2012: 78.8546542198766\n",
      "RMSE per TS 10 for model: results_2012: 76.88177844364755\n",
      "RMSE per TS 11 for model: results_2012: 55.22814887788593\n",
      "RMSE per TS 12 for model: results_2012: 58.35426489014036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE per TS 13 for model: results_2012: 57.18100977484582\n",
      "RMSE per TS 14 for model: results_2012: 52.20018582152766\n",
      "RMSE per TS 15 for model: results_2012: 51.607301250153476\n",
      "RMSE per TS 16 for model: results_2012: 47.051954189885755\n",
      "RMSE per TS 17 for model: results_2012: 54.754109445745364\n",
      "RMSE per TS 18 for model: results_2012: 91.45701827380273\n",
      "RMSE per TS 19 for model: results_2012: 45.83150482173603\n",
      "Avg.RMSE for multivariate model: results_2012: 66.3285582958363\n"
     ]
    }
   ],
   "source": [
    "#use additional lagged features:\n",
    "\n",
    "#additional features are created based on time series data:\n",
    "multLSTM_encod = MultivariateLSTM(use_features_per_lag_flag = True)\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "\n",
    "multivar_ts_series = ts_20largest.loc['2009':'2014']\n",
    "\n",
    "\n",
    "#create full model:\n",
    "results_i = multLSTM_encod.create_full_pred_model(multivar_ts_series, start_train_year, \n",
    "                                            last_train_set_year, validation_set_year, \n",
    "                                            test_set_year, 'TEST_MultivarLSTM', verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading of pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model instance:\n",
    "multLSTM = MultivariateLSTM()\n",
    "\n",
    "start_train_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011' \n",
    "test_set_year = '2012' \n",
    "\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('MultivarLSTM')\n",
    "multLSTM.load_model(prediction_model)\n",
    "\n",
    "#make predictions with loaded model:      \n",
    "results_tuple = multiLSTM_model_new.generate_data_get_predictions(multivar_ts_series, start_train_year, last_train_set_year,\n",
    "                                                                  validation_set_year, test_set_year, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
