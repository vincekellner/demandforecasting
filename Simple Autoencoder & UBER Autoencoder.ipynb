{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook different autoencoder architectures are implemented: Each architecture is based on an encoding-module and a prediction module. The window size refers to how many timesteps are used to make a prediction. \"Multivariate\" models process input data of multiple areas at the same time to predict demand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T08:34:09.289431Z",
     "start_time": "2020-01-30T08:34:08.021512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import save_files_collection as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:35.638236Z",
     "start_time": "2019-10-25T18:48:35.461270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:35.751534Z",
     "start_time": "2019-10-25T18:48:35.745348Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T08:34:47.682812Z",
     "start_time": "2020-01-30T08:34:47.578859Z"
    }
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "TRAIN_PATH = '/media/...'\n",
    "Store_PATH = '/media/...'\n",
    "file_name = 'ts_10largest.csv'\n",
    "ts_10largest = pd.read_csv(TRAIN_PATH + file_name, header=0, parse_dates=['date'], index_col = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:40.902272Z",
     "start_time": "2019-10-25T18:48:40.894007Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Time Series Preparation '''\n",
    "\n",
    "#Preprocessing Steps: 1) Differencing 2) Scaling 3) Supervised_Sequence\n",
    "\n",
    "#make timeseries stationary\n",
    "def differencing(dataset,shift_interval):\n",
    "    shifted = dataset.shift(shift_interval).copy(deep=True)\n",
    "    diff_series = dataset - shifted\n",
    "    diff_series.dropna(inplace=True)\n",
    "    return diff_series\n",
    "\n",
    "\n",
    "\n",
    "def invert_differencing(history, predictions, year_to_access):\n",
    "    history_copy = history.copy()\n",
    "    history_shift = history_copy.shift(1)\n",
    "    history_shift.dropna(inplace=True)\n",
    "    preds_all = predictions + history_shift.loc[year_to_access]\n",
    "    \n",
    "    return preds_all\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:41.679211Z",
     "start_time": "2019-10-25T18:48:41.661659Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#it is recommended to scale input data in the range of the activation function used by the network. Since we use \"tanh\"..\n",
    "# scale datasets based on fit of train_set; input set has to be a reshaped numpy_array!\n",
    "def data_scaler(train_data, valid_data, test_data, scale_range, standardizing_flag=False):\n",
    "        \n",
    "    if standardizing_flag == False:  \n",
    "        print('MinMax-Scaling used...')\n",
    "        #use MinMax-Scaling based on scale_range given...\n",
    "        scaler = MinMaxScaler(feature_range=scale_range) #feature_range=(-1, 1)\n",
    "        scaler = scaler.fit(train_data)\n",
    "        #scale train_set:\n",
    "        train_scaled = scaler.transform(train_data)\n",
    "        train_scaled_df = pd.DataFrame(train_scaled)\n",
    "        #scale valididation data based on scaler fitted on training data:\n",
    "        valid_scaled = scaler.transform(valid_data)\n",
    "        valid_scaled_df = pd.DataFrame(valid_scaled)\n",
    "        #scale test_set based on scaler fitted on training data:\n",
    "        test_scaled = scaler.transform(test_data)\n",
    "        test_scaled_df = pd.DataFrame(test_scaled)\n",
    "            \n",
    "    else:  \n",
    "        print('Standardizing used...')\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler = scaler.fit(train_data)\n",
    "        #scale train_set:\n",
    "        train_scaled = scaler.transform(train_data)\n",
    "        train_scaled_df = pd.DataFrame(train_scaled)\n",
    "        #scale valididation data based on scaler fitted on training data:\n",
    "        valid_scaled = scaler.transform(valid_data)\n",
    "        valid_scaled_df = pd.DataFrame(valid_scaled)\n",
    "        #scale test_set based on scaler fitted on training data:\n",
    "        test_scaled = scaler.transform(test_data)\n",
    "        test_scaled_df = pd.DataFrame(test_scaled)\n",
    "        \n",
    "        \n",
    "    #return data as df, since \"supervised\" function requires df or series as input\n",
    "    return scaler, train_scaled_df, valid_scaled_df, test_scaled_df\n",
    "\n",
    "\n",
    "#function to invert scaling:\n",
    "def invert_data_scaler(scaler, predictions_data, standardizing_flag, scale_range, n_preds):\n",
    "    \n",
    "    if standardizing_flag == False: \n",
    "        #create new scaler: -> necessary since old scaler might be fitted on lagged values -> expects more dimensions as the predictions have! \n",
    "        new_scaler = MinMaxScaler(feature_range=scale_range)\n",
    "        #copy attributes of old scaler to new one: but only for the first columns for which we have real predictions\n",
    "        new_scaler.min_, new_scaler.scale_ = scaler.min_[:n_preds], scaler.scale_[:n_preds]\n",
    "\n",
    "        preds = new_scaler.inverse_transform(predictions_data)\n",
    "        \n",
    "    else:\n",
    "        new_scaler = StandardScaler()\n",
    "        #copy attributes of old scaler to new one: but only for the first columns for which we have real predictions\n",
    "        new_scaler.mean_, new_scaler.scale_ = scaler.mean_[:n_preds], scaler.scale_[:n_preds]\n",
    "\n",
    "        preds = new_scaler.inverse_transform(predictions_data)\n",
    "\n",
    "    \n",
    "    return preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:43.120232Z",
     "start_time": "2019-10-25T18:48:43.112647Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_supervised_data_single_ts(single_ts, n_timesteps):\n",
    "    #copy dfs to ensure no changes are made on original dataset:\n",
    "    sequence_copy = single_ts.copy(deep=True)\n",
    "    #create dfs to easily append new columns and access columns:\n",
    "    sequence_copy = pd.DataFrame(sequence_copy)\n",
    "    sequence_df = pd.DataFrame(sequence_copy)\n",
    "    \n",
    "    #Note: range starts reversed to make sure the lags are in correct order\n",
    "    for i in range(n_timesteps,0,-1): \n",
    "        sequence_df['lag_{}'.format(i)] = sequence_copy.iloc[:,0].shift(i)\n",
    "    \n",
    "       \n",
    "    #drop rows with NaNs -> if no lagged features are available, we drop the whole row\n",
    "    sequence_df.dropna(inplace=True) \n",
    "\n",
    "    return sequence_df  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:43.809741Z",
     "start_time": "2019-10-25T18:48:43.800777Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_supervised_UBER_data_single_ts(single_ts, n_timesteps_T, n_timesteps_F):\n",
    "    #copy dfs to ensure no changes are made on original dataset:\n",
    "    sequence_copy = single_ts.copy(deep=True)\n",
    "    #create dfs to easily append new columns and access columns:\n",
    "    sequence_copy = pd.DataFrame(sequence_copy)\n",
    "    sequence_df = pd.DataFrame(sequence_copy)\n",
    "    \n",
    "    #Note: range starts reversed to make sure the lags are in correct order\n",
    "    for i in range(n_timesteps_T,0,-1): \n",
    "        sequence_df['lag_T{}'.format(i)] = sequence_copy.iloc[:,0].shift(i)\n",
    "    for i in range(n_timesteps_F,0,-1): \n",
    "        sequence_df['prev_lag_F{}'.format(i)] = sequence_copy.iloc[:,0].shift(i)\n",
    "    #Note: for future values range starts \"normal\":\n",
    "    for i in range(1,n_timesteps_F+1): \n",
    "        sequence_df['fut_{}'.format(i)] = sequence_copy.iloc[:,0].shift(-i)\n",
    "\n",
    "       \n",
    "    #drop rows with NaNs -> if no lagged features are available, we drop the whole row\n",
    "    sequence_df.dropna(inplace=True) \n",
    "\n",
    "    return sequence_df  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data for models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:52.293782Z",
     "start_time": "2019-10-25T18:48:52.242219Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_UBER_data_autoencoder(ts_series, multivariate_flag, last_train_set_year, \n",
    "                                   validation_set_year, test_set_year, n_timesteps_T, n_timesteps_F, n_preds, \n",
    "                                   scale_range, standardizing_flag):         \n",
    "    #prepare data:\n",
    "    \n",
    "    # 1) apply differencing\n",
    "    ts_diff = differencing(ts_series,1)\n",
    "\n",
    "    #reassign df:\n",
    "    ts_series = ts_diff\n",
    "    \n",
    "    # 2) get supervised data:\n",
    "    if multivariate_flag == False:\n",
    "        #change type of ts_series:\n",
    "        ts_series = pd.DataFrame(ts_series) #-> this way we can access the \"column\" parameter\n",
    "    \n",
    "    #prepare dict to store results of each area:\n",
    "    area_labels = list(ts_series.columns)\n",
    "    area_supervised_dict = {}\n",
    "    scaler_list = []\n",
    "    \n",
    "    #get data for each area:\n",
    "    for i in range(len(area_labels)):\n",
    "        print('data for area{} is prepared...'.format(area_labels[i]))\n",
    "        #create key:\n",
    "        area_supervised_dict['area{}'.format(area_labels[i])] = []\n",
    "        #get supervised data:\n",
    "        supervised_data_df = create_supervised_UBER_data_single_ts(ts_series.iloc[:,i], n_timesteps_T, n_timesteps_F)\n",
    "        \n",
    "        \n",
    "        #get train/test split:      \n",
    "        ts_train_all, ts_test = supervised_data_df.loc[:validation_set_year], supervised_data_df.loc[test_set_year]             \n",
    "        #get train/validation Split:\n",
    "        ts_train, ts_valid = ts_train_all.loc[:last_train_set_year], ts_train_all.loc[validation_set_year]\n",
    "                \n",
    "           \n",
    "        #scale data:\n",
    "        #create numpy arrays for scaler:\n",
    "        ts_train_array = ts_train.values\n",
    "        ts_valid_array = ts_valid.values\n",
    "        ts_test_array = ts_test.values\n",
    "        \n",
    "        print('Data is scaled...')\n",
    "        scaler, train_scaled, valid_scaled, test_scaled = data_scaler(ts_train_array, ts_valid_array, ts_test_array, scale_range, standardizing_flag)\n",
    "        \n",
    "        #store scaler:\n",
    "        scaler_list.append(scaler)\n",
    "        \n",
    "        #restore index:\n",
    "        train_scaled.index = ts_train.index\n",
    "        valid_scaled.index = ts_valid.index\n",
    "        test_scaled.index = ts_test.index\n",
    "\n",
    "        \n",
    "        #slice lags_T, lags_F, future_F values:\n",
    "        #slice lags_T which are actually X:\n",
    "        X_train = train_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        X_valid = valid_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        X_test = test_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        #get y values:\n",
    "        y_train = train_scaled.iloc[:,0].values\n",
    "        y_valid = valid_scaled.iloc[:,0].values\n",
    "        y_test = test_scaled.iloc[:,0].values  \n",
    "        #slice lags_F:\n",
    "        lags_F_train = train_scaled.iloc[:,-(n_timesteps_F + n_timesteps_F):-(n_timesteps_F)].values\n",
    "        lags_F_valid = valid_scaled.iloc[:,-(n_timesteps_F + n_timesteps_F):-(n_timesteps_F)].values\n",
    "        lags_F_test = test_scaled.iloc[:,-(n_timesteps_F + n_timesteps_F):-(n_timesteps_F)].values\n",
    "        #slice future_F:\n",
    "        fut_F_train = train_scaled.iloc[:,-(n_timesteps_F):].values\n",
    "        fut_F_valid = valid_scaled.iloc[:,-(n_timesteps_F):].values\n",
    "        fut_F_test = test_scaled.iloc[:,-(n_timesteps_F):].values\n",
    "        \n",
    "        \n",
    "        #reshape X, y and lags_F and fut_F data to easily append other areas later on:\n",
    "        list_to_reshape_X_lags_F_fut_F = [X_train, X_valid, X_test, lags_F_train, lags_F_valid, lags_F_test, fut_F_train, fut_F_valid, fut_F_test] \n",
    "        list_to_reshape_y = [y_train, y_valid, y_test]\n",
    "        reshaped_list_X_lags_F_fut_F = []\n",
    "        reshaped_list_y = []\n",
    "        #reshape X:\n",
    "        for array in list_to_reshape_X_lags_F_fut_F:\n",
    "            #shape: (#n_samples,n_lags,n_areas)\n",
    "            array = array.reshape((array.shape[0],array.shape[1],1))\n",
    "            reshaped_list_X_lags_F_fut_F.append(array)\n",
    "            \n",
    "        #reshape y:\n",
    "        for array in list_to_reshape_y:\n",
    "            #shape: (#n_samples,n_areas)\n",
    "            array = array.reshape((array.shape[0],1))\n",
    "            reshaped_list_y.append(array)\n",
    "            \n",
    "           \n",
    "        #append results to dict:\n",
    "        items_to_append = [reshaped_list_X_lags_F_fut_F[0], reshaped_list_y[0], reshaped_list_X_lags_F_fut_F[3], reshaped_list_X_lags_F_fut_F[6],\n",
    "                          reshaped_list_X_lags_F_fut_F[1], reshaped_list_y[1], reshaped_list_X_lags_F_fut_F[4], reshaped_list_X_lags_F_fut_F[7],\n",
    "                          reshaped_list_X_lags_F_fut_F[2], reshaped_list_y[2], reshaped_list_X_lags_F_fut_F[5], reshaped_list_X_lags_F_fut_F[8]]\n",
    "        \n",
    "        for u in range(len(items_to_append)):\n",
    "            area_supervised_dict['area{}'.format(area_labels[i])].append(items_to_append[u])\n",
    "    \n",
    "        \n",
    "        #print shapes:\n",
    "        print('shape of X_train single area: ', reshaped_list_X_lags_F_fut_F[0].shape)\n",
    "        print('shape of X_valid single area: ', reshaped_list_X_lags_F_fut_F[1].shape)\n",
    "        print('shape of X_test single area: ', reshaped_list_X_lags_F_fut_F[2].shape)\n",
    "        print('shape of y_train single area: ', reshaped_list_y[0].shape)\n",
    "        print('shape of y_valid single area: ', reshaped_list_y[1].shape)\n",
    "        print('shape of y_test single area: ', reshaped_list_y[2].shape)\n",
    "        print('shape of prev_lags_F_train single area: ', reshaped_list_X_lags_F_fut_F[3].shape)\n",
    "        print('shape of fut_F_values_train single area: ', reshaped_list_X_lags_F_fut_F[6].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #concat results of all areas:\n",
    "    key_list = list(area_supervised_dict.keys())\n",
    "    \n",
    "    if multivariate_flag == True:\n",
    "        #create training set, valid & test set containing inputs of all selected areas: -> append all areas into one big np.array!\n",
    "\n",
    "        #fill arrays with entries of first area:\n",
    "        X_train, y_train = area_supervised_dict[key_list[0]][0], area_supervised_dict[key_list[0]][1]\n",
    "        X_valid, y_valid = area_supervised_dict[key_list[0]][4], area_supervised_dict[key_list[0]][5]\n",
    "        X_test, y_test = area_supervised_dict[key_list[0]][8], area_supervised_dict[key_list[0]][9]\n",
    "        \n",
    "        lags_F_train = area_supervised_dict[key_list[0]][2]\n",
    "        lags_F_valid = area_supervised_dict[key_list[0]][6]\n",
    "        lags_F_test = area_supervised_dict[key_list[0]][10]\n",
    "\n",
    "        fut_F_train = area_supervised_dict[key_list[0]][3]\n",
    "        fut_F_valid = area_supervised_dict[key_list[0]][7]\n",
    "        fut_F_test = area_supervised_dict[key_list[0]][11]\n",
    " \n",
    "        \n",
    "        #concat remaining areas:\n",
    "        for i in range(1,len(key_list)):\n",
    "            X_train = np.concatenate((X_train, area_supervised_dict[key_list[i]][0]),axis=2)\n",
    "            X_valid = np.concatenate((X_valid, area_supervised_dict[key_list[i]][4]),axis=2)\n",
    "            X_test = np.concatenate((X_test, area_supervised_dict[key_list[i]][8]),axis=2)\n",
    "            y_train = np.concatenate((y_train, area_supervised_dict[key_list[i]][1]),axis=1)\n",
    "            y_valid = np.concatenate((y_valid, area_supervised_dict[key_list[i]][5]),axis=1)\n",
    "            y_test = np.concatenate((y_test, area_supervised_dict[key_list[i]][9]),axis=1)\n",
    "            \n",
    "            lags_F_train = np.concatenate((lags_F_train, area_supervised_dict[key_list[i]][2]),axis=2)\n",
    "            lags_F_valid = np.concatenate((lags_F_valid, area_supervised_dict[key_list[i]][6]),axis=2)\n",
    "            lags_F_test = np.concatenate((lags_F_test, area_supervised_dict[key_list[i]][10]),axis=2)\n",
    "\n",
    "            fut_F_train = np.concatenate((fut_F_train, area_supervised_dict[key_list[i]][3]),axis=2)\n",
    "            fut_F_valid = np.concatenate((fut_F_valid, area_supervised_dict[key_list[i]][7]),axis=2)\n",
    "            fut_F_test = np.concatenate((fut_F_test, area_supervised_dict[key_list[i]][11]),axis=2)\n",
    "     \n",
    "    \n",
    "    else:\n",
    "        X_train, y_train = area_supervised_dict[key_list[0]][0], area_supervised_dict[key_list[0]][1]\n",
    "        X_valid, y_valid = area_supervised_dict[key_list[0]][4], area_supervised_dict[key_list[0]][5]\n",
    "        X_test, y_test = area_supervised_dict[key_list[0]][8], area_supervised_dict[key_list[0]][9]\n",
    "        \n",
    "        lags_F_train = area_supervised_dict[key_list[0]][2]\n",
    "        lags_F_valid = area_supervised_dict[key_list[0]][6]\n",
    "        lags_F_test = area_supervised_dict[key_list[0]][10]\n",
    "\n",
    "        fut_F_train = area_supervised_dict[key_list[0]][3]\n",
    "        fut_F_valid = area_supervised_dict[key_list[0]][7]\n",
    "        fut_F_test = area_supervised_dict[key_list[0]][11]\n",
    "        \n",
    "    \n",
    "    print('final shape of X_train: ', X_train.shape)\n",
    "    \n",
    "    #return area_supervised_dict\n",
    "    \n",
    "    \n",
    "    #Note: We only need to return lags_F & fut_F_values for train and valid set since autoencoder is only \"trained\" on these sets..\n",
    "    return (X_train, y_train, X_valid, y_valid, X_test, y_test, lags_F_train, lags_F_valid, fut_F_train, \n",
    "            fut_F_valid, scaler_list)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:52.984356Z",
     "start_time": "2019-10-25T18:48:52.957858Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_autoencoder(ts_series, multivariate_flag, last_train_set_year, validation_set_year, test_set_year, \n",
    "                              n_timesteps, n_preds, scale_range, standardizing_flag):\n",
    "    #prepare data:\n",
    "    \n",
    "    # 1) apply differencing\n",
    "    ts_diff = differencing(ts_series,1)\n",
    "\n",
    "    #reassign df:\n",
    "    ts_series = ts_diff\n",
    "    \n",
    "        # 2) get supervised data:\n",
    "    if multivariate_flag == False:\n",
    "        #change type of ts_series:\n",
    "        ts_series = pd.DataFrame(ts_series) #-> this way we can access the \"column\" parameter\n",
    "    \n",
    "    #prepare dict to store results of each area:\n",
    "    area_labels = list(ts_series.columns)\n",
    "    area_supervised_dict = {}\n",
    "    scaler_list = []\n",
    "    \n",
    "    #get data for each area:\n",
    "    for i in range(len(area_labels)):\n",
    "        print('data for area{} is prepared...'.format(area_labels[i]))\n",
    "        #create key:\n",
    "        area_supervised_dict['area{}'.format(area_labels[i])] = []\n",
    "        #get supervised data:\n",
    "        supervised_data_df = create_supervised_data_single_ts(ts_series.iloc[:,i], n_timesteps)\n",
    "        \n",
    "        \n",
    "        #get train/test split:      \n",
    "        ts_train_all, ts_test = supervised_data_df.loc[:validation_set_year], supervised_data_df.loc[test_set_year]             \n",
    "        #get train/validation Split:\n",
    "        ts_train, ts_valid = ts_train_all.loc[:last_train_set_year], ts_train_all.loc[validation_set_year]\n",
    "\n",
    "           \n",
    "        #scale data:\n",
    "        #create numpy arrays for scaler:\n",
    "        ts_train_array = ts_train.values\n",
    "        ts_valid_array = ts_valid.values\n",
    "        ts_test_array = ts_test.values\n",
    "        \n",
    "        print('Data is scaled...')\n",
    "        scaler, train_scaled, valid_scaled, test_scaled = data_scaler(ts_train_array, ts_valid_array, ts_test_array, scale_range, standardizing_flag)\n",
    "        \n",
    "        #store scaler:\n",
    "        scaler_list.append(scaler)\n",
    "        \n",
    "        #restore index:\n",
    "        train_scaled.index = ts_train.index\n",
    "        valid_scaled.index = ts_valid.index\n",
    "        test_scaled.index = ts_test.index\n",
    "\n",
    "        \n",
    "        #get X, y pairs:\n",
    "        X_train = train_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        X_valid = valid_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        X_test = test_scaled.iloc[:,1:(n_timesteps_T+1)].values\n",
    "        #get y values:\n",
    "        y_train = train_scaled.iloc[:,0].values\n",
    "        y_valid = valid_scaled.iloc[:,0].values\n",
    "        y_test = test_scaled.iloc[:,0].values  \n",
    "      \n",
    "        \n",
    "        #reshape X, y to easily append other areas later on:\n",
    "        list_to_reshape_X = [X_train, X_valid, X_test] \n",
    "        list_to_reshape_y = [y_train, y_valid, y_test]\n",
    "        reshaped_list_X = []\n",
    "        reshaped_list_y = []\n",
    "        #reshape X:\n",
    "        for array in list_to_reshape_X:\n",
    "            #shape: (#n_samples,n_lags,n_areas)\n",
    "            array = array.reshape((array.shape[0],array.shape[1],1))\n",
    "            reshaped_list_X.append(array)\n",
    "            \n",
    "        #reshape y:\n",
    "        for array in list_to_reshape_y:\n",
    "            #shape: (#n_samples,n_areas)\n",
    "            array = array.reshape((array.shape[0],1))\n",
    "            reshaped_list_y.append(array)\n",
    "            \n",
    "           \n",
    "        #append results to dict:\n",
    "        items_to_append = [reshaped_list_X[0], reshaped_list_y[0], \n",
    "                          reshaped_list_X[1], reshaped_list_y[1], \n",
    "                          reshaped_list_X[2], reshaped_list_y[2]]\n",
    "        \n",
    "        for u in range(len(items_to_append)):\n",
    "            area_supervised_dict['area{}'.format(area_labels[i])].append(items_to_append[u])\n",
    "    \n",
    "        \n",
    "        #print shapes:\n",
    "        print('shape of X_train single area: ', reshaped_list_X[0].shape)\n",
    "        print('shape of X_valid single area: ', reshaped_list_X[1].shape)\n",
    "        print('shape of X_test single area: ', reshaped_list_X[2].shape)\n",
    "        print('shape of y_train single area: ', reshaped_list_y[0].shape)\n",
    "        print('shape of y_valid single area: ', reshaped_list_y[1].shape)\n",
    "        print('shape of y_test single area: ', reshaped_list_y[2].shape)    \n",
    "    \n",
    "    \n",
    "    #concat results of all areas:\n",
    "    key_list = list(area_supervised_dict.keys())\n",
    "    \n",
    "    if multivariate_flag == True:\n",
    "        #create training set, valid & test set containing inputs of all selected areas: -> append all areas into one big np.array!\n",
    "\n",
    "        #fill arrays with entries of first area:\n",
    "        X_train, y_train = area_supervised_dict[key_list[0]][0], area_supervised_dict[key_list[0]][1]\n",
    "        X_valid, y_valid = area_supervised_dict[key_list[0]][2], area_supervised_dict[key_list[0]][3]\n",
    "        X_test, y_test = area_supervised_dict[key_list[0]][4], area_supervised_dict[key_list[0]][5]\n",
    "                \n",
    "        #concat remaining areas:\n",
    "        for i in range(1,len(key_list)):\n",
    "            X_train = np.concatenate((X_train, area_supervised_dict[key_list[i]][0]),axis=2)\n",
    "            X_valid = np.concatenate((X_valid, area_supervised_dict[key_list[i]][2]),axis=2)\n",
    "            X_test = np.concatenate((X_test, area_supervised_dict[key_list[i]][4]),axis=2)\n",
    "            y_train = np.concatenate((y_train, area_supervised_dict[key_list[i]][1]),axis=1)\n",
    "            y_valid = np.concatenate((y_valid, area_supervised_dict[key_list[i]][3]),axis=1)\n",
    "            y_test = np.concatenate((y_test, area_supervised_dict[key_list[i]][5]),axis=1)\n",
    "                 \n",
    "    \n",
    "    else:\n",
    "        X_train, y_train = area_supervised_dict[key_list[0]][0], area_supervised_dict[key_list[0]][1]\n",
    "        X_valid, y_valid = area_supervised_dict[key_list[0]][2], area_supervised_dict[key_list[0]][3]\n",
    "        X_test, y_test = area_supervised_dict[key_list[0]][4], area_supervised_dict[key_list[0]][5]\n",
    "        \n",
    "    print('final shape of X_train: ', X_train.shape)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, scaler_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:16:34.303171Z",
     "start_time": "2020-01-21T18:16:34.300205Z"
    }
   },
   "source": [
    "## simpler autoencoder with either 1 hidden layer or 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:48:57.845010Z",
     "start_time": "2019-10-25T18:48:57.835919Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_autoencoder_1H(X_train, X_valid, n_timesteps, n_preds, n_hidden1_units, dropout_rate, n_batch_size_training, n_epochs):\n",
    "    #create encoder with Keras Functional API:\n",
    "    # define encoder\n",
    "    input1 = Input(shape=(X_train.shape[1], X_train.shape[2]))  \n",
    "    encoder = LSTM(n_hidden1_units, activation='tanh', dropout=dropout_rate)(input1) \n",
    "    #encoder layer returns a static meaningful vector -> to receive a sequence again, we use RepeatVector which returns the meaningful vector \"n\" times\n",
    "    # define reconstruct decoder\n",
    "    encoder2 = RepeatVector(n_timesteps)(encoder) #needed to keep the 3D shape (X_train.shape[1])\n",
    "    decoder = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(encoder2) #previously 32units\n",
    "    decoder = TimeDistributed(Dense(n_preds))(decoder)\n",
    "\n",
    "    autoencoder_model_ts = Model(inputs=input1, outputs=decoder)\n",
    "    autoencoder_model_ts.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    history = autoencoder_model_ts.fit(X_train, X_train, batch_size=n_batch_size_training, epochs=n_epochs, validation_data=(X_valid, X_valid), verbose=1, shuffle=True)\n",
    "    return history, autoencoder_model_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:00.854820Z",
     "start_time": "2019-10-25T18:49:00.842016Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_stacked_autoencoder_2H(X_train, X_valid, n_timesteps, n_preds, n_hidden1_units, n_hidden2_units, dropout_rate, n_batch_size_training, n_epochs):\n",
    "    #create encoder with Keras Functional API:\n",
    "    # define encoder\n",
    "    input1 = Input(shape=(X_train.shape[1], X_train.shape[2]))  \n",
    "    encoder1 = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(input1)\n",
    "    encoder2 = LSTM(n_hidden2_units, activation='tanh', dropout=dropout_rate)(encoder1)\n",
    "    #encoder2 layer returns a static meaningful vector -> to receive a sequence again, we use RepeatVector..\n",
    "    #define reconstruct decoder\n",
    "    encoder3 = RepeatVector(n_timesteps)(encoder2) #RepeatVector actually belongs to the encoding-module -> name = \"encoder3\"\n",
    "    decoder1 = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(encoder3)\n",
    "    decoder2 = TimeDistributed(Dense(n_preds))(decoder1)\n",
    "\n",
    "    autoencoder_model_ts = Model(inputs=input1, outputs=decoder2)\n",
    "    autoencoder_model_ts.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    history = autoencoder_model_ts.fit(X_train, X_train, batch_size=n_batch_size_training, epochs=n_epochs, validation_data=(X_valid, X_valid), verbose=1, shuffle=True)\n",
    "    return history, autoencoder_model_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:03.632380Z",
     "start_time": "2019-10-25T18:49:03.620689Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_prediction_model_2H(X_train_encoded, y_train, X_valid_encoded, y_valid, n_preds, n_hidden1_units, n_hidden2_units, dropout_rate, n_batch_size_training, n_epochs, shuffle_flag, early_stopping_flag):     \n",
    "    \n",
    "    #create callback list for different callbacks:\n",
    "    callback_list = []\n",
    "    \n",
    "    if early_stopping_flag == True:\n",
    "        print('Early Stopping applied')\n",
    "        #create Callback for EarlyStopping:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "        #append callback to list:\n",
    "        callback_list.append(early_stop)\n",
    "\n",
    "    \n",
    "    # define prediction model:\n",
    "    input2 = Input(shape=(X_train_encoded.shape[1], X_train_encoded.shape[2]))\n",
    "    lstm_2 = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(input2)\n",
    "    lstm_2 = LSTM(n_hidden2_units, activation='tanh', return_sequences=False, dropout=dropout_rate)(lstm_2)\n",
    "    #dense2 = Dense(50)(lstm_2)\n",
    "    out2 = Dense(n_preds)(lstm_2)\n",
    "\n",
    "    predict_model = Model(input2, out2)\n",
    "    predict_model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    history = predict_model.fit(X_train_encoded, y_train, epochs=n_epochs, batch_size=n_batch_size_training, validation_data=(X_valid_encoded, y_valid), verbose=1, shuffle=shuffle_flag, callbacks = callback_list)\n",
    "    return history, predict_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T18:17:01.015574Z",
     "start_time": "2020-01-21T18:17:01.012765Z"
    }
   },
   "source": [
    "## UBER autoencoder with either 1 hidden layer or 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:08.168637Z",
     "start_time": "2019-10-25T18:49:08.162176Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Note: There are different ways how the autoencoder can be constructed:\n",
    "    1) LSTM LAyers of Encoding Scheme don't use return_sequences -> this way a static embedding is returned after all timesteps are processed, -> a RepeatVector-Layer is needed which creates multiple copies of the static embedding\n",
    "    2) LSTM LAyers of Encoding Scheme use return_sequences -> The hidden state of each time step is returned -> we don't need RepeatVector-Layer\n",
    "    \n",
    "    --> here: repeatVector used in this Version\n",
    "'''\n",
    "def create_UBER_autoencoder_1H_with_RepeatVect(X_train_T, X_train_F_prev, X_train_F_fut, X_valid_T, X_valid_F_prev, X_valid_F_fut, n_timesteps_F, n_preds, n_hidden1_units, n_hidden2_units, dropout_rate, n_batch_size_training, n_epochs):\n",
    "    #create encoder with Keras Functional API:\n",
    "    # define encoder\n",
    "    #define input of regular sliding window sequence (previous 't' days)\n",
    "    input1 = Input(shape=(X_train_T.shape[1], X_train_T.shape[2]))\n",
    "    #create LSTM layers:\n",
    "    encoder1 = LSTM(n_hidden1_units, activation='tanh', return_sequences=False, dropout=dropout_rate)(input1)\n",
    "    #encoder1 layer returns a static meaningful vector -> to receive a sequence again, we use RepeatVector..\n",
    "    #define reconstruct decoder\n",
    "    encoder2 = RepeatVector(n_timesteps_F)(encoder1) #RepeatVector actually belongs to the encoding-module -> name = \"encoder3\"\n",
    "    #define input of additional sliding window sequence (previous 'F' days of target) -> defined in UBER Paper.. \n",
    "    input2 = Input(shape=(X_train_F_prev.shape[1], X_train_F_prev.shape[2]))\n",
    "    #create concat layer to add additional sequence input on axis=2:\n",
    "    concat1 = Concatenate(axis=2)([encoder2,input2])\n",
    "    #Decoding Module:\n",
    "    decoder1 = LSTM(n_hidden2_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(concat1) #decoder1 returns sequence of hidden states\n",
    "    decoder2 = TimeDistributed(Dense(n_preds))(decoder1) #Note: \"TimeDistributed\" applies Dense-Layer on each sequence returned by decoder1 \n",
    "\n",
    "    autoencoder_model_ts = Model(inputs=[input1,input2], outputs=decoder2)\n",
    "    autoencoder_model_ts.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    history = autoencoder_model_ts.fit([X_train_T, X_train_F_prev], X_train_F_fut, batch_size=n_batch_size_training, epochs=n_epochs, validation_data=([X_valid_T, X_valid_F_prev], X_valid_F_fut), verbose=1, shuffle=True)\n",
    "    \n",
    "    return history, autoencoder_model_ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:08.841193Z",
     "start_time": "2019-10-25T18:49:08.834502Z"
    }
   },
   "outputs": [],
   "source": [
    "''' --> here: repeatVector used in this Version '''\n",
    "def create_stacked_UBER_autoencoder_2H(X_train_T, X_train_F_prev, X_train_F_fut, X_valid_T, X_valid_F_prev, X_valid_F_fut, n_timesteps_F, n_preds, n_hidden1_units, n_hidden2_units, dropout_rate, n_batch_size_training, n_epochs):\n",
    "    #create encoder with Keras Functional API:\n",
    "    # define encoder\n",
    "    #define input of regular sliding window sequence (previous 't' days)\n",
    "    input1 = Input(shape=(X_train_T.shape[1], X_train_T.shape[2]))\n",
    "    #create LSTM layers:\n",
    "    encoder1 = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(input1)\n",
    "    encoder2 = LSTM(n_hidden2_units, activation='tanh', dropout=dropout_rate)(encoder1)\n",
    "    #encoder2 layer returns a static meaningful vector -> to receive a sequence again, we use RepeatVector..\n",
    "    #define reconstruct decoder\n",
    "    encoder3 = RepeatVector(n_timesteps_F)(encoder2) #RepeatVector actually belongs to the encoding-module -> name = \"encoder3\"\n",
    "    #define input of additional sliding window sequence (previous 'F' days of target) -> defined in UBER Paper.. \n",
    "    input2 = Input(shape=(X_train_F_prev.shape[1], X_train_F_prev.shape[2]))\n",
    "    #create concat layer to add additional sequence input on axis=2:\n",
    "    concat1 = Concatenate(axis=2)([encoder3,input2])\n",
    "    #Decoding Module:\n",
    "    decoder1 = LSTM(n_hidden1_units, activation='tanh', return_sequences=True, dropout=dropout_rate)(concat1)\n",
    "    decoder2 = TimeDistributed(Dense(n_preds))(decoder1)\n",
    "\n",
    "    autoencoder_model_ts = Model(inputs=[input1,input2], outputs=decoder2)\n",
    "    autoencoder_model_ts.compile(optimizer='adam', loss='mse')\n",
    "    # fit model\n",
    "    history = autoencoder_model_ts.fit([X_train_T, X_train_F_prev], X_train_F_fut, batch_size=n_batch_size_training, epochs=n_epochs, validation_data=([X_valid_T, X_valid_F_prev], X_valid_F_fut), verbose=1, shuffle=True)\n",
    "    \n",
    "    return history, autoencoder_model_ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:10.712003Z",
     "start_time": "2019-10-25T18:49:10.700535Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_UBER_prediction_model_3H(X_train_encoded, y_train, X_valid_encoded, y_valid, n_preds, n_hidden1_units, n_hidden2_units, n_hidden3_units, dropout_rate, n_batch_size_training, n_epochs, shuffle_flag):     \n",
    "    # define prediction model:\n",
    "    #we only need 2D shape for Dense-Layers..\n",
    "    input_data = Input(shape=(X_train_encoded.shape[1], ))\n",
    "    dense1 = Dense(n_hidden1_units, activation='tanh')(input_data)\n",
    "    dropout1 = Dropout(dropout_rate)(dense1)\n",
    "    dense2 = Dense(n_hidden2_units, activation='tanh')(dropout1)\n",
    "    dropout2 = Dropout(dropout_rate)(dense2)\n",
    "    dense3 = Dense(n_hidden3_units, activation='tanh')(dropout2)\n",
    "    dropout3 = Dropout(dropout_rate)(dense3)\n",
    "    out2 = Dense(n_preds)(dropout3)\n",
    "    \n",
    "    #compile model:\n",
    "    predict_model = Model(input_data, out2)\n",
    "    predict_model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    history = predict_model.fit(X_train_encoded, y_train, epochs=n_epochs, batch_size=n_batch_size_training, validation_data=(X_valid_encoded, y_valid), verbose=1, shuffle=shuffle_flag)\n",
    "    \n",
    "    return history, predict_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:13.299398Z",
     "start_time": "2019-10-25T18:49:13.277810Z"
    }
   },
   "outputs": [],
   "source": [
    "#get prediction for scaled + differenced input_data (assuming scaled input) !!!!!!:\n",
    "\n",
    "#assuming encoding & reshaping for input_data was already done \n",
    "\n",
    "def get_rescaled_decoded_predictions(model, multivariate_flag, X_test_encoded, year_to_access, scaler_list, standardizing_flag, scale_range, n_preds, original_complete_dataset, model_name):       \n",
    "    \n",
    "    # 1) get predictions:\n",
    "    print('get predictions of model...')\n",
    "    yhat = model.predict(X_test_encoded, verbose = 1)\n",
    "    #print(yhat.shape)\n",
    "    #print(yhat)\n",
    "    \n",
    "    print('yhat shape: ', yhat.shape)\n",
    "    \n",
    "    # 2) rescale predictions (use scaler of each area):\n",
    "    if multivariate_flag == False:\n",
    "        yhat_rescaled_all = invert_data_scaler(scaler_list[0], yhat, standardizing_flag, scale_range, n_preds)\n",
    "    \n",
    "    else:\n",
    "        yhat_rescaled_all_list = []\n",
    "        for i in range(len(scaler_list)):\n",
    "            #slice predictions for each area and rescale predictions: (Note: slicing columns of numpy array returns a list! -> reshaping necessary afterwards)\n",
    "            yhat_area_i = yhat[:,i]\n",
    "            #reshape unscaled predictions for scaler:\n",
    "            yhat_area_i = yhat_area_i.reshape(len(yhat_area_i),1)\n",
    "            #apply scaler:\n",
    "            yhat_rescaled_area_i = invert_data_scaler(scaler_list[i], yhat_area_i, standardizing_flag, scale_range, 1) #Note: for multivariate case n_preds is set to \"1\" since we only take first column of scaler for each area\n",
    "            yhat_rescaled_all_list.append(yhat_rescaled_area_i)\n",
    "    \n",
    "        #restore numpy_array based on yhat_rescaled_all_list: (this way we have all rescaled predictions for each area in one big numpy array)\n",
    "        yhat_rescaled_all = yhat_rescaled_all_list[0]\n",
    "        for i in range(1,len(yhat_rescaled_all_list)):\n",
    "            yhat_rescaled_all = np.concatenate((yhat_rescaled_all,yhat_rescaled_all_list[i]),axis=1)\n",
    "        \n",
    "    \n",
    "    print('First 2 scaled predictions')\n",
    "    print(yhat_rescaled_all[0:2])\n",
    "    print('Shape of predictions:', yhat_rescaled_all.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 3) compare predictions with actuals / invert differencing:    \n",
    "    if multivariate_flag == True:\n",
    "        print('Invert Differencing of multivariate predictions...')    \n",
    "        #invert differencing:  (adding value of previous timestep)\n",
    "        predictions_all = invert_differencing(original_complete_dataset, yhat_rescaled_all, year_to_access)\n",
    "        \n",
    "        print('predictions preview:')\n",
    "        print(predictions_all.head())\n",
    "        \n",
    "        # 4) get rmse for each timeseries\n",
    "        rmse_per_ts = []\n",
    "        for u in range(n_preds):\n",
    "            rmse_single_ts = np.sqrt(mean_squared_error(original_complete_dataset.loc[year_to_access].iloc[:,u], predictions_all.iloc[:,u]))\n",
    "            rmse_per_ts.append(rmse_single_ts)\n",
    "            print('RMSE per TS {} for model: {}: {}'.format(u, model_name, rmse_per_ts[u]))\n",
    "        \n",
    "        #get average of all rmses\n",
    "        total_rmse = np.mean(rmse_per_ts)\n",
    "        print('Avg.RMSE for multivariate model: {}: {}'.format(model_name, total_rmse))\n",
    "        \n",
    "    else:\n",
    "        #invert differencing:  (adding value of previous timestep)\n",
    "        print('Invert Differencing of predictions...')        \n",
    "        predictions_all = invert_differencing(original_complete_dataset, yhat_rescaled_all[:,0], year_to_access)\n",
    " \n",
    "        print('predictions preview:')\n",
    "        print(predictions_all.head())\n",
    "        \n",
    "        # 4) get rmse:\n",
    "        rmse = np.sqrt(mean_squared_error(original_complete_dataset[year_to_access], predictions_all))\n",
    "        print('RMSE for model: {}: {}'.format(model_name, rmse))\n",
    "    \n",
    "    \n",
    "    #return RMSE results:\n",
    "    rmse_results = []\n",
    "    if multivariate_flag == True:\n",
    "        rmse_results.append(total_rmse)\n",
    "        rmse_results.append(rmse_per_ts)\n",
    "   \n",
    "    else:\n",
    "        rmse_results.append(rmse)\n",
    "        \n",
    "    \n",
    "    return predictions_all, rmse_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions to store predictions on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe predictions and history into df: (assuming models are trained with MSE: training and validation set)\n",
    "def preds_into_df(preds_df, original_complete_dataset, multivariate_flag, year_to_store):    \n",
    "    #add actuals to single df:\n",
    "    if multivariate_flag == False:\n",
    "        print('Add Actuals to df for single area...')\n",
    "        #create dataframe to add column easily:\n",
    "        preds_df = pd.DataFrame(preds_df)\n",
    "        preds_df['actuals'] = original_complete_dataset[year_to_store] #store actual values to calculate RMSE quickly\n",
    "\n",
    "    print('creation of preds_df done')\n",
    "        \n",
    "    return preds_df\n",
    "\n",
    "def trainhistory_into_df(encoder_training_history, prediction_model_training_history, hist_col_labels):\n",
    "    #create df for traning_history:\n",
    "    hist_encoder_df = pd.DataFrame(encoder_training_history.history['loss'],columns=[hist_col_labels[0]])\n",
    "    hist_encoder_df[hist_col_labels[1]] = encoder_training_history.history['val_loss']\n",
    "    \n",
    "    hist_prediction_df = pd.DataFrame(prediction_model_training_history.history['loss'],columns=[hist_col_labels[0]])\n",
    "    hist_prediction_df[hist_col_labels[1]] = prediction_model_training_history.history['val_loss']\n",
    "    \n",
    "    print('creation of history_dfs done')\n",
    "    \n",
    "    return  hist_encoder_df, hist_prediction_df\n",
    "\n",
    "\n",
    "#store predictions and training_history on disk:\n",
    "def store_preds_and_trainhistory_on_disk(preds_df, hist_encoder_df, hist_prediction_df, preds_df_filename, hist_encoder_df_filename, hist_prediction_df_filename, Store_PATH):\n",
    "    #get path where to store df:\n",
    "    preds_df_final_path = os.path.join(Store_PATH, preds_df_filename)\n",
    "    #store df:\n",
    "    preds_df.to_csv(preds_df_final_path, header=True)\n",
    "    \n",
    "    #store history:\n",
    "    hist_encoder_df_final_path = os.path.join(Store_PATH, hist_encoder_df_filename)\n",
    "    hist_encoder_df.to_csv(hist_encoder_df_final_path, header=True)\n",
    "    \n",
    "    hist_prediction_df_final_path = os.path.join(Store_PATH, hist_prediction_df_filename)\n",
    "    hist_prediction_df.to_csv(hist_prediction_df_final_path, header=True)\n",
    "    \n",
    "    print('Save df on disk done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models to JSON -> check that weights_file name uses \".h5\" format & model_file_name \".json\"\n",
    "def save_models_to_json(model_file_name, model_weights_file_name, Store_PATH, model):\n",
    "    \n",
    "    #create paths for model architecture & weights:\n",
    "    model_final_path = Store_PATH + model_file_name    \n",
    "    weights_final_path = Store_PATH + model_weights_file_name\n",
    "\n",
    "    #store model & weights:\n",
    "    model_as_json = model.to_json()\n",
    "    with open(model_final_path, \"w\") as json_file:\n",
    "        json_file.write(model_as_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(weights_final_path)\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store history + prediction_results in dicts diretly on disk with the help of other functions:\n",
    "def store_results_of_dicts_on_disk(dict_to_access, validation_set_year, test_set_year, Df_Store_PATH, Model_Save_PATH, RMSE_Store_PATH, RMSE_df_name):\n",
    "    \n",
    "    #create & store dfs of Results:\n",
    "    for key in dict_to_access:\n",
    "        print('Store results of key: ', key)\n",
    "                \n",
    "        #store test_results:\n",
    "        if 'multivariate' not in key:\n",
    "            multivariate_flag = False\n",
    "            \n",
    "            #get prediction_df: \n",
    "            preds_df = preds_into_df(dict_to_access[key][0][2], dict_to_access[key][0][4], multivariate_flag, test_set_year)\n",
    "            \n",
    "            #create history dfs:\n",
    "            hist_col_labels = ['loss (mse)','mae','val_loss (mse)','val_mae']\n",
    "            hist_encoder_df, hist_prediction_df = trainhistory_into_df(dict_to_access[key][0][8], dict_to_access[key][0][0], hist_col_labels)\n",
    "\n",
    "            #create validation_df:    \n",
    "            valid_preds_df = dict_to_access[key][0][5] \n",
    "            valid_preds_df = pd.DataFrame(valid_preds_df)\n",
    "            valid_preds_df['actuals'] = dict_to_access[key][0][4].loc[validation_set_year] #store actual values of original dataset                        \n",
    "        \n",
    "\n",
    "        else:\n",
    "            multivariate_flag = True\n",
    "            #get prediction_df: \n",
    "            preds_df = preds_into_df(dict_to_access[key][0][2], dict_to_access[key][0][4], multivariate_flag, test_set_year)    \n",
    "            \n",
    "            #create history dfs:\n",
    "            hist_col_labels = ['loss (mse)','mae','val_loss (mse)','val_mae']\n",
    "            hist_encoder_df, hist_prediction_df = trainhistory_into_df(dict_to_access[key][0][8], dict_to_access[key][0][0], hist_col_labels)\n",
    "                  \n",
    "            #create validation_df for multivariate case:  \n",
    "            valid_preds_df = dict_to_access[key][0][5]           \n",
    "\n",
    "   \n",
    "        #create filenames:\n",
    "        results_file = 'results_' + dict_to_access[key][0][3] + '.csv'\n",
    "        history_encoder_file = 'history_autoencoder_' + dict_to_access[key][0][3] + '.csv'\n",
    "        history_predict_file = 'history_predict_' + dict_to_access[key][0][3] + '.csv'\n",
    "        valid_results_file = 'validation_results_' + dict_to_access[key][0][3] + '.csv'\n",
    "              \n",
    "        #store test results: \n",
    "        store_preds_and_trainhistory_on_disk(preds_df, hist_encoder_df, hist_prediction_df, results_file, history_encoder_file, history_predict_file, Df_Store_PATH)\n",
    "        #store validation results:\n",
    "        #get path where to store df:\n",
    "        valid_df_final_path = os.path.join(Df_Store_PATH, valid_results_file)\n",
    "        #store df:\n",
    "        valid_preds_df.to_csv(valid_df_final_path, header=True)\n",
    "  \n",
    "\n",
    "        #store model architecture (architecture + weights):\n",
    "        autoencoder_model_architecture_file = 'autoencoder_model_' + dict_to_access[key][0][3] + '.json'\n",
    "        autoencoder_model_weights_file = 'autoencoder_model_' + dict_to_access[key][0][3] + '_weights.h5'\n",
    "        predict_model_architecture_file = 'predict_model_' + dict_to_access[key][0][3] + '.json'\n",
    "        predict_model_weights_file = 'predict_model_' + dict_to_access[key][0][3] + '_weights.h5'\n",
    "        \n",
    "        #call function to save models:   \n",
    "        #save autoencoder model:\n",
    "        save_models_to_json(autoencoder_model_architecture_file, autoencoder_model_weights_file, Model_Save_PATH, dict_to_access[key][0][9])\n",
    "        #save predict model:\n",
    "        save_models_to_json(predict_model_architecture_file, predict_model_weights_file, Model_Save_PATH, dict_to_access[key][0][1])\n",
    "\n",
    "        \n",
    "        #store RMSE results of models:\n",
    "        #create dict to store RMSE results:\n",
    "        dict_test_rmse = {}\n",
    "        dict_valid_rmse = {}\n",
    "        \n",
    "        #add values to dict:\n",
    "        #Note: for multivar_models only avg. RMSE is stored!!\n",
    "        if 'multivar' in key:\n",
    "            dict_valid_rmse[key] = []\n",
    "            dict_test_rmse[key] = []\n",
    "            #append avg. RMSE results of multivar model:\n",
    "            dict_valid_rmse[key].append(dict_to_access[key][0][7][0])\n",
    "            dict_test_rmse[key].append(dict_to_access[key][0][6][0])\n",
    "        \n",
    "        else:\n",
    "            dict_test_rmse[key] = []\n",
    "            dict_valid_rmse[key] = []\n",
    "            dict_valid_rmse[key].append(dict_to_access[key][0][7])\n",
    "            dict_test_rmse[key].append(dict_to_access[key][0][6])\n",
    "        \n",
    "        \n",
    "    #create Df from rmse_dicts:\n",
    "    rmse_valid_df = pd.DataFrame.from_dict(dict_valid_rmse, orient='index')\n",
    "    rmse_test_df = pd.DataFrame.from_dict(dict_test_rmse, orient='index')\n",
    "\n",
    "    #store results in one df:\n",
    "    all_rmse_df = pd.concat([rmse_valid_df,rmse_test_df], axis=1, names=['RMSE_Validation','RMSE_Test'])\n",
    "    all_rmse_df.columns = ['RMSE_Validation', 'RMSE_Test']\n",
    "\n",
    "    #store df on disk:\n",
    "    #Note: RMSE results are  stored in same path as prediction results\n",
    "    #set name of df:\n",
    "    RMSE_df_name_final = 'RMSE_Results_all_models_' + RMSE_df_name + '.csv'\n",
    "    #store df:\n",
    "    all_rmse_df.to_csv(RMSE_Store_PATH + RMSE_df_name_final, header=True)\n",
    "\n",
    "    print('All dfs & models of dict stored!')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get full autoencoder models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T18:49:20.896547Z",
     "start_time": "2019-10-25T18:49:20.870920Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Function to train selected autoencoder \"automatically\"... '''\n",
    "\n",
    "\n",
    "def get_full_autoencoder_pred_model(ts_series, multivariate_flag, last_train_set_year, \n",
    "                                    validation_set_year, test_set_year, model_name, n_timesteps_T, \n",
    "                                    n_timesteps_F, n_preds,\n",
    "                                    scale_range, standardizing_flag, stacked_encoder_flag, \n",
    "                                    n_hidden1_units_autoencod, n_hidden2_units_autoencod, \n",
    "                                    n_hidden1_units_pred, n_hidden2_units_pred, n_hidden3_units_pred, \n",
    "                                    n_batch_size_autoencoder, n_batch_size_pred_model, dropout_rate, \n",
    "                                    n_epochs, shuffle_flag, early_stopping_flag, uber_flag):       \n",
    "    \n",
    "    # 1) get data for model:\n",
    "    if uber_flag == True:\n",
    "        print('#Generate data for UBER Model...')\n",
    "        #call function to genereate data for UBER Models:\n",
    "        X_train_T, y_train_T, X_valid_T, y_valid_T, X_test_T, y_test_T, X_train_F_prev, X_valid_F_prev, X_train_F_fut, X_valid_F_fut, scaler_list = generate_UBER_data_autoencoder(ts_series, multivariate_flag, last_train_set_year, validation_set_year, test_set_year, n_timesteps_T, n_timesteps_F, n_preds, scale_range, standardizing_flag)\n",
    "        \n",
    "    else:\n",
    "        print('#Generate data for regular Model...')\n",
    "        #call regular function to get data for model:\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test, scaler_list = generate_data_autoencoder(ts_series, multivariate_flag, last_train_set_year, validation_set_year, test_set_year, n_timesteps_T, n_preds, scale_range, standardizing_flag)                                                 \n",
    "  \n",
    "  \n",
    "    # 2) create autoencoder model:   \n",
    "    if uber_flag == True:\n",
    "        print('create UBER Autoencoder...')\n",
    "        #get autoencoder:\n",
    "        history_autoencoder, autoencoder_model = create_UBER_autoencoder_1H_with_RepeatVect(X_train_T, X_train_F_prev, X_train_F_fut, X_valid_T, X_valid_F_prev, X_valid_F_fut, n_timesteps_F, n_preds, n_hidden1_units_autoencod, n_hidden2_units_autoencod, dropout_rate, n_batch_size_autoencoder, n_epochs)\n",
    "\n",
    "        #get the standalone encoder:\n",
    "        #since the approach in \"UBER-Paper\" uses a prediciton net, based on dense layers, we need an encoder which outputs 2D outputs.. -> we take take last LSTM layer as output of encoder instead of RepeatVector..\n",
    "        print('create standalone UBER encoder...')\n",
    "        standalone_encoder = Model(inputs=autoencoder_model.layers[0].input, outputs=autoencoder_model.layers[1].output)\n",
    "        #--> eventuell braucht man hier \"inputs=autoencoder_model.inputs[0] oder so, da wir fr standalone encoder nur x_train als input verwenden und nicht noch x_Train_F_prev..\n",
    "    \n",
    "    if stacked_encoder_flag == True and uber_flag == False:\n",
    "        print('create stacked autoencoder 2 layer:')  \n",
    "        #get autoencoder:\n",
    "        history_autoencoder, autoencoder_model = create_stacked_autoencoder_2H(X_train, X_valid, n_timesteps_T, n_preds, n_hidden1_units_autoencod, n_hidden2_units_autoencod, dropout_rate, n_batch_size_autoencoder, n_epochs)\n",
    "                 \n",
    "        #get the standalone encoder:\n",
    "        print('create standalone encoder:')  \n",
    "        standalone_encoder = Model(inputs=autoencoder_model.inputs, outputs=autoencoder_model.layers[3].output)\n",
    "        \n",
    "\n",
    "    if stacked_encoder_flag == False and uber_flag == False:\n",
    "        print('create autoencoder single layer:') \n",
    "        #get autoencoder:\n",
    "        history_autoencoder, autoencoder_model = create_autoencoder_1H(X_train, X_valid, n_timesteps_T, n_preds, n_hidden1_units_autoencod, dropout_rate, n_batch_size_autoencoder, n_epochs)\n",
    "        \n",
    "        #get the standalone encoder:\n",
    "        print('create standalone encoder:')  \n",
    "        standalone_encoder = Model(inputs=autoencoder_model.inputs, outputs=autoencoder_model.layers[2].output)\n",
    "    \n",
    "    \n",
    "    #prepare data for prediction model:\n",
    "    #encode input:\n",
    "    if uber_flag == True:\n",
    "        print('encode data with UBER standalone encoder:')  \n",
    "        X_train_encoded = standalone_encoder.predict(X_train_T)\n",
    "        X_valid_encoded = standalone_encoder.predict(X_valid_T)\n",
    "        X_test_encoded = standalone_encoder.predict(X_test_T)\n",
    "\n",
    "        print('X_train_encoded shape: ', X_train_encoded.shape)\n",
    "        print('X_valid_encoded shape: ', X_valid_encoded.shape)\n",
    "        print('X_test_encoded shape: ', X_test_encoded.shape)\n",
    "    \n",
    "    else:\n",
    "        print('encode data with standalone encoder:')  \n",
    "        X_train_encoded = standalone_encoder.predict(X_train)\n",
    "        X_valid_encoded = standalone_encoder.predict(X_valid)\n",
    "        X_test_encoded = standalone_encoder.predict(X_test)\n",
    "\n",
    "        print('X_train_encoded shape: ', X_train_encoded.shape)\n",
    "        print('X_valid_encoded shape: ', X_valid_encoded.shape)\n",
    "        print('X_test_encoded shape: ', X_test_encoded.shape)\n",
    "\n",
    "    \n",
    "    #create prediction model:\n",
    "    if uber_flag == True:\n",
    "        print('create UBER prediction_model:')   \n",
    "        history_predict_model , prediction_model = create_UBER_prediction_model_3H(X_train_encoded, y_train_T, X_valid_encoded, y_valid_T, n_preds, n_hidden1_units_pred, n_hidden2_units_pred, n_hidden3_units_pred, dropout_rate, n_batch_size_pred_model, n_epochs, shuffle_flag)     \n",
    "\n",
    "    else:\n",
    "        print('create prediction_model:')                                           \n",
    "        history_predict_model , prediction_model = create_prediction_model_2H(X_train_encoded, y_train, X_valid_encoded, y_valid, n_preds, n_hidden1_units_pred, n_hidden2_units_pred, dropout_rate, n_batch_size_pred_model, n_epochs, shuffle_flag, early_stopping_flag)\n",
    "    \n",
    "    #get prediction results:\n",
    "    #results for valid_set:       \n",
    "    validation_results, rmse_valid = get_rescaled_decoded_predictions(prediction_model, multivariate_flag, X_valid_encoded, validation_set_year, scaler_list, standardizing_flag, scale_range, n_preds, ts_series, 'validation_set')\n",
    "\n",
    "    #results for test_set: \n",
    "    predictions_results, rmse_test = get_rescaled_decoded_predictions(prediction_model, multivariate_flag, X_test_encoded, test_set_year, scaler_list, standardizing_flag, scale_range, n_preds, ts_series, model_name)\n",
    "\n",
    "\n",
    "    return history_predict_model, prediction_model, predictions_results, model_name, ts_series, validation_results, rmse_test, rmse_valid, history_autoencoder, autoencoder_model\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START Training of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## START Experiment encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_area237__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928,)\n",
      "complete test_set shape:  (8952,)\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Train_data shape:  (17519, 1)\n",
      "valid_data shape:  (8928, 1)\n",
      "Test_data shape:  (8952, 1)\n",
      "X_train shape for autoencoder:  (17351, 168, 1)\n",
      "X_valid shape for autoencoder:  (8760, 168, 1)\n",
      "X_test shape for autoencoder:  (8784, 168, 1)\n",
      "y_train shape before model creation:  (17351, 1)\n",
      "y_valid shape before model creation:  (8760, 1)\n",
      "y_test shape before model creation:  (8784, 1)\n",
      "create stacked autoencoder 2 layer:\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 24s 1ms/step - loss: 0.0435 - val_loss: 0.0505\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0501\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0426 - val_loss: 0.0504\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0421 - val_loss: 0.0500\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0418 - val_loss: 0.0495\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0419 - val_loss: 0.0504\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0420 - val_loss: 0.0499\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0416 - val_loss: 0.0476\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0421 - val_loss: 0.0503\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0419 - val_loss: 0.0496\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0416 - val_loss: 0.0494\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0414 - val_loss: 0.0493\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0414 - val_loss: 0.0468\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0404 - val_loss: 0.0482\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0408 - val_loss: 0.0494\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0412 - val_loss: 0.0478\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0391 - val_loss: 0.0474\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0404 - val_loss: 0.0496\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0412 - val_loss: 0.0480\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0379 - val_loss: 0.0492\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0383 - val_loss: 0.0405\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0337 - val_loss: 0.0374\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0317 - val_loss: 0.0353\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0295 - val_loss: 0.0343\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0284 - val_loss: 0.0378\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0284 - val_loss: 0.0324\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0282 - val_loss: 0.0333\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0278 - val_loss: 0.0403\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0283 - val_loss: 0.0319\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0278 - val_loss: 0.0312\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0276 - val_loss: 0.0305\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0271 - val_loss: 0.0311\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0270 - val_loss: 0.0326\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0271 - val_loss: 0.0304\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0270 - val_loss: 0.0292\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0265 - val_loss: 0.0335\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0260 - val_loss: 0.0294\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0251 - val_loss: 0.0292\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0249 - val_loss: 0.0277\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0246 - val_loss: 0.0277\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0248 - val_loss: 0.0266\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0241 - val_loss: 0.0270\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0242 - val_loss: 0.0263\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0244 - val_loss: 0.0308\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0241 - val_loss: 0.0267\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0268 - val_loss: 0.0360\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0261 - val_loss: 0.0254\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0301 - val_loss: 0.0299\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0244 - val_loss: 0.0280\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0240 - val_loss: 0.0254\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0226 - val_loss: 0.0277\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0222 - val_loss: 0.0253\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0215 - val_loss: 0.0217\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0220 - val_loss: 0.0319\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0223 - val_loss: 0.0229\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0211 - val_loss: 0.0248\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0221 - val_loss: 0.0227\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0208 - val_loss: 0.0237\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0211 - val_loss: 0.0228\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0204 - val_loss: 0.0238\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0204 - val_loss: 0.0220\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0234 - val_loss: 0.0242\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0203 - val_loss: 0.0221\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0196 - val_loss: 0.0247\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0198 - val_loss: 0.0232\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0206 - val_loss: 0.0241\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0197 - val_loss: 0.0208\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0194 - val_loss: 0.0216\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0193 - val_loss: 0.0200\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0197 - val_loss: 0.0196\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0204 - val_loss: 0.0232\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0205 - val_loss: 0.0202\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0188 - val_loss: 0.0221\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0189 - val_loss: 0.0213\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0188 - val_loss: 0.0246\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0193 - val_loss: 0.0239\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0192 - val_loss: 0.0283\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0192 - val_loss: 0.0197\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0182 - val_loss: 0.0229\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0192 - val_loss: 0.0198\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0199 - val_loss: 0.0189\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0187 - val_loss: 0.0273\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0199 - val_loss: 0.0200\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0178 - val_loss: 0.0194\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0169 - val_loss: 0.0187\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0208 - val_loss: 0.0216\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0177 - val_loss: 0.0188\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0190 - val_loss: 0.0230\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0182 - val_loss: 0.0199\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0176 - val_loss: 0.0183\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0183 - val_loss: 0.0188\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0174 - val_loss: 0.0204\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0195 - val_loss: 0.0227\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0170 - val_loss: 0.0187\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0163 - val_loss: 0.0180\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0180 - val_loss: 0.0180\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0171 - val_loss: 0.0197\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0201 - val_loss: 0.0216\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0166 - val_loss: 0.0179\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0180 - val_loss: 0.0192\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0169 - val_loss: 0.0177\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0160 - val_loss: 0.0180\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0157 - val_loss: 0.0175\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0182 - val_loss: 0.0212\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0192 - val_loss: 0.0243\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0199 - val_loss: 0.0270\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0199 - val_loss: 0.0200\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0173 - val_loss: 0.0188\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0198 - val_loss: 0.0236\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0240 - val_loss: 0.0223\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0234 - val_loss: 0.0218\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0218 - val_loss: 0.0372\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0248 - val_loss: 0.0221\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0210 - val_loss: 0.0297\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0267 - val_loss: 0.0292\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0226 - val_loss: 0.0238\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0213 - val_loss: 0.0428\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0332 - val_loss: 0.0517\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0420 - val_loss: 0.0471\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0346 - val_loss: 0.0361\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0308 - val_loss: 0.0377\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0281 - val_loss: 0.0332\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0269 - val_loss: 0.0293\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0277 - val_loss: 0.0364\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0294 - val_loss: 0.0305\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0245 - val_loss: 0.0284\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0236 - val_loss: 0.0362\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0225 - val_loss: 0.0290\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0217 - val_loss: 0.0262\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0241 - val_loss: 0.0272\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0229 - val_loss: 0.0234\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0303 - val_loss: 0.0366\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0273 - val_loss: 0.0285\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0240 - val_loss: 0.0272\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0228 - val_loss: 0.0236\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0220 - val_loss: 0.0236\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0214 - val_loss: 0.0245\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0208 - val_loss: 0.0251\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 128)\n",
      "X_valid_encoded shape:  (8760, 168, 128)\n",
      "X_test_encoded shape:  (8784, 168, 128)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.0316 - mean_absolute_error: 0.1291 - val_loss: 0.0320 - val_mean_absolute_error: 0.1297\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 17s 976us/step - loss: 0.0259 - mean_absolute_error: 0.1173 - val_loss: 0.0261 - val_mean_absolute_error: 0.1189\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0224 - mean_absolute_error: 0.1095 - val_loss: 0.0246 - val_mean_absolute_error: 0.1136\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0204 - mean_absolute_error: 0.1046 - val_loss: 0.0207 - val_mean_absolute_error: 0.1044\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0177 - mean_absolute_error: 0.0974 - val_loss: 0.0226 - val_mean_absolute_error: 0.1015\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0167 - mean_absolute_error: 0.0936 - val_loss: 0.0151 - val_mean_absolute_error: 0.0893\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.0155 - mean_absolute_error: 0.0903 - val_loss: 0.0156 - val_mean_absolute_error: 0.0927\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0152 - mean_absolute_error: 0.0892 - val_loss: 0.0144 - val_mean_absolute_error: 0.0879\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0147 - mean_absolute_error: 0.0876 - val_loss: 0.0124 - val_mean_absolute_error: 0.0825\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0133 - mean_absolute_error: 0.0832 - val_loss: 0.0121 - val_mean_absolute_error: 0.0799\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0127 - mean_absolute_error: 0.0814 - val_loss: 0.0117 - val_mean_absolute_error: 0.0782\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0127 - mean_absolute_error: 0.0813 - val_loss: 0.0119 - val_mean_absolute_error: 0.0799\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 16s 942us/step - loss: 0.0122 - mean_absolute_error: 0.0799 - val_loss: 0.0112 - val_mean_absolute_error: 0.0778\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0119 - mean_absolute_error: 0.0791 - val_loss: 0.0113 - val_mean_absolute_error: 0.0789\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.0119 - mean_absolute_error: 0.0788 - val_loss: 0.0114 - val_mean_absolute_error: 0.0802\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0124 - mean_absolute_error: 0.0802 - val_loss: 0.0112 - val_mean_absolute_error: 0.0781\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0116 - mean_absolute_error: 0.0778 - val_loss: 0.0101 - val_mean_absolute_error: 0.0739\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0115 - mean_absolute_error: 0.0777 - val_loss: 0.0104 - val_mean_absolute_error: 0.0757\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0114 - mean_absolute_error: 0.0771 - val_loss: 0.0105 - val_mean_absolute_error: 0.0751\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0115 - mean_absolute_error: 0.0771 - val_loss: 0.0101 - val_mean_absolute_error: 0.0742\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0112 - mean_absolute_error: 0.0762 - val_loss: 0.0095 - val_mean_absolute_error: 0.0717\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0121 - mean_absolute_error: 0.0787 - val_loss: 0.0115 - val_mean_absolute_error: 0.0773\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.0115 - mean_absolute_error: 0.0772 - val_loss: 0.0119 - val_mean_absolute_error: 0.0807\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0113 - mean_absolute_error: 0.0770 - val_loss: 0.0106 - val_mean_absolute_error: 0.0759\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0109 - mean_absolute_error: 0.0752 - val_loss: 0.0103 - val_mean_absolute_error: 0.0748\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0107 - mean_absolute_error: 0.0747 - val_loss: 0.0097 - val_mean_absolute_error: 0.0730\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.0121 - mean_absolute_error: 0.0789 - val_loss: 0.0122 - val_mean_absolute_error: 0.0804\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0114 - mean_absolute_error: 0.0771 - val_loss: 0.0098 - val_mean_absolute_error: 0.0736\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 15s 889us/step - loss: 0.0105 - mean_absolute_error: 0.0737 - val_loss: 0.0095 - val_mean_absolute_error: 0.0706\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0103 - mean_absolute_error: 0.0727 - val_loss: 0.0093 - val_mean_absolute_error: 0.0701\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0105 - mean_absolute_error: 0.0743 - val_loss: 0.0094 - val_mean_absolute_error: 0.0713\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.0107 - mean_absolute_error: 0.0741 - val_loss: 0.0090 - val_mean_absolute_error: 0.0695\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0102 - mean_absolute_error: 0.0726 - val_loss: 0.0092 - val_mean_absolute_error: 0.0707\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0104 - mean_absolute_error: 0.0731 - val_loss: 0.0091 - val_mean_absolute_error: 0.0707\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.0101 - mean_absolute_error: 0.0724 - val_loss: 0.0088 - val_mean_absolute_error: 0.0687\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0101 - mean_absolute_error: 0.0720 - val_loss: 0.0092 - val_mean_absolute_error: 0.0705\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0100 - mean_absolute_error: 0.0722 - val_loss: 0.0090 - val_mean_absolute_error: 0.0687\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0120 - mean_absolute_error: 0.0769 - val_loss: 0.0095 - val_mean_absolute_error: 0.0718\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0108 - mean_absolute_error: 0.0743 - val_loss: 0.0092 - val_mean_absolute_error: 0.0699\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0107 - mean_absolute_error: 0.0737 - val_loss: 0.0092 - val_mean_absolute_error: 0.0699\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0102 - mean_absolute_error: 0.0723 - val_loss: 0.0092 - val_mean_absolute_error: 0.0701\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 893us/step - loss: 0.0103 - mean_absolute_error: 0.0726 - val_loss: 0.0090 - val_mean_absolute_error: 0.0696\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0103 - mean_absolute_error: 0.0727 - val_loss: 0.0090 - val_mean_absolute_error: 0.0694\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0101 - mean_absolute_error: 0.0717 - val_loss: 0.0087 - val_mean_absolute_error: 0.0695\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0095 - mean_absolute_error: 0.0701 - val_loss: 0.0085 - val_mean_absolute_error: 0.0679\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0094 - mean_absolute_error: 0.0694 - val_loss: 0.0085 - val_mean_absolute_error: 0.0673\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0095 - mean_absolute_error: 0.0701 - val_loss: 0.0086 - val_mean_absolute_error: 0.0682\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0102 - mean_absolute_error: 0.0719 - val_loss: 0.0089 - val_mean_absolute_error: 0.0690\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0096 - mean_absolute_error: 0.0707 - val_loss: 0.0090 - val_mean_absolute_error: 0.0688\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.0099 - mean_absolute_error: 0.0708 - val_loss: 0.0094 - val_mean_absolute_error: 0.0703\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0102 - mean_absolute_error: 0.0720 - val_loss: 0.0088 - val_mean_absolute_error: 0.0683\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 893us/step - loss: 0.0097 - mean_absolute_error: 0.0704 - val_loss: 0.0084 - val_mean_absolute_error: 0.0670\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0094 - mean_absolute_error: 0.0693 - val_loss: 0.0084 - val_mean_absolute_error: 0.0664\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.0095 - mean_absolute_error: 0.0692 - val_loss: 0.0082 - val_mean_absolute_error: 0.0658\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0092 - mean_absolute_error: 0.0685 - val_loss: 0.0088 - val_mean_absolute_error: 0.0692\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0092 - mean_absolute_error: 0.0681 - val_loss: 0.0087 - val_mean_absolute_error: 0.0675\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0093 - mean_absolute_error: 0.0689 - val_loss: 0.0088 - val_mean_absolute_error: 0.0687\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 893us/step - loss: 0.0094 - mean_absolute_error: 0.0689 - val_loss: 0.0093 - val_mean_absolute_error: 0.0699\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0092 - mean_absolute_error: 0.0684 - val_loss: 0.0083 - val_mean_absolute_error: 0.0666\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0093 - mean_absolute_error: 0.0688 - val_loss: 0.0085 - val_mean_absolute_error: 0.0673\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0092 - mean_absolute_error: 0.0684 - val_loss: 0.0084 - val_mean_absolute_error: 0.0668\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0093 - mean_absolute_error: 0.0688 - val_loss: 0.0086 - val_mean_absolute_error: 0.0685\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0090 - mean_absolute_error: 0.0676 - val_loss: 0.0084 - val_mean_absolute_error: 0.0666\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.0094 - mean_absolute_error: 0.0687 - val_loss: 0.0091 - val_mean_absolute_error: 0.0691\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0094 - mean_absolute_error: 0.0689 - val_loss: 0.0083 - val_mean_absolute_error: 0.0664\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0091 - mean_absolute_error: 0.0678 - val_loss: 0.0084 - val_mean_absolute_error: 0.0665\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 15s 888us/step - loss: 0.0090 - mean_absolute_error: 0.0677 - val_loss: 0.0085 - val_mean_absolute_error: 0.0681\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.0093 - mean_absolute_error: 0.0680 - val_loss: 0.0090 - val_mean_absolute_error: 0.0676\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0099 - mean_absolute_error: 0.0706 - val_loss: 0.0082 - val_mean_absolute_error: 0.0661\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0088 - mean_absolute_error: 0.0668 - val_loss: 0.0082 - val_mean_absolute_error: 0.0649\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0099 - mean_absolute_error: 0.0699 - val_loss: 0.0082 - val_mean_absolute_error: 0.0657\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0089 - mean_absolute_error: 0.0671 - val_loss: 0.0082 - val_mean_absolute_error: 0.0653\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0088 - mean_absolute_error: 0.0665 - val_loss: 0.0079 - val_mean_absolute_error: 0.0643\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0088 - mean_absolute_error: 0.0670 - val_loss: 0.0080 - val_mean_absolute_error: 0.0648\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0087 - mean_absolute_error: 0.0663 - val_loss: 0.0081 - val_mean_absolute_error: 0.0655\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0089 - mean_absolute_error: 0.0665 - val_loss: 0.0084 - val_mean_absolute_error: 0.0659\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0087 - mean_absolute_error: 0.0662 - val_loss: 0.0081 - val_mean_absolute_error: 0.0651\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0087 - mean_absolute_error: 0.0662 - val_loss: 0.0081 - val_mean_absolute_error: 0.0653\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0091 - mean_absolute_error: 0.0672 - val_loss: 0.0087 - val_mean_absolute_error: 0.0671\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0092 - mean_absolute_error: 0.0683 - val_loss: 0.0088 - val_mean_absolute_error: 0.0715\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0092 - mean_absolute_error: 0.0676 - val_loss: 0.0120 - val_mean_absolute_error: 0.0752\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0093 - mean_absolute_error: 0.0681 - val_loss: 0.0082 - val_mean_absolute_error: 0.0658\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.0091 - mean_absolute_error: 0.0670 - val_loss: 0.0084 - val_mean_absolute_error: 0.0665\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0089 - mean_absolute_error: 0.0670 - val_loss: 0.0081 - val_mean_absolute_error: 0.0655\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0088 - mean_absolute_error: 0.0666 - val_loss: 0.0081 - val_mean_absolute_error: 0.0657\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0088 - mean_absolute_error: 0.0662 - val_loss: 0.0080 - val_mean_absolute_error: 0.0648\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0097 - mean_absolute_error: 0.0691 - val_loss: 0.0091 - val_mean_absolute_error: 0.0689\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0090 - mean_absolute_error: 0.0671 - val_loss: 0.0080 - val_mean_absolute_error: 0.0647\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0087 - mean_absolute_error: 0.0661 - val_loss: 0.0082 - val_mean_absolute_error: 0.0654\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0087 - mean_absolute_error: 0.0659 - val_loss: 0.0079 - val_mean_absolute_error: 0.0643\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0085 - mean_absolute_error: 0.0652 - val_loss: 0.0080 - val_mean_absolute_error: 0.0644\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0085 - mean_absolute_error: 0.0650 - val_loss: 0.0081 - val_mean_absolute_error: 0.0649\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0093 - mean_absolute_error: 0.0675 - val_loss: 0.0090 - val_mean_absolute_error: 0.0665\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.0107 - mean_absolute_error: 0.0731 - val_loss: 0.0095 - val_mean_absolute_error: 0.0711\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.0096 - mean_absolute_error: 0.0699 - val_loss: 0.0086 - val_mean_absolute_error: 0.0674\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0093 - mean_absolute_error: 0.0685 - val_loss: 0.0087 - val_mean_absolute_error: 0.0672\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0088 - mean_absolute_error: 0.0668 - val_loss: 0.0083 - val_mean_absolute_error: 0.0662\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.0087 - mean_absolute_error: 0.0665 - val_loss: 0.0082 - val_mean_absolute_error: 0.0659\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0087 - mean_absolute_error: 0.0659 - val_loss: 0.0086 - val_mean_absolute_error: 0.0671\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0088 - mean_absolute_error: 0.0668 - val_loss: 0.0083 - val_mean_absolute_error: 0.0660\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0086 - mean_absolute_error: 0.0661 - val_loss: 0.0081 - val_mean_absolute_error: 0.0651\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0086 - mean_absolute_error: 0.0658 - val_loss: 0.0081 - val_mean_absolute_error: 0.0652\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.0085 - mean_absolute_error: 0.0653 - val_loss: 0.0079 - val_mean_absolute_error: 0.0640\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 15s 883us/step - loss: 0.0086 - mean_absolute_error: 0.0659 - val_loss: 0.0083 - val_mean_absolute_error: 0.0659\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0085 - mean_absolute_error: 0.0655 - val_loss: 0.0078 - val_mean_absolute_error: 0.0635\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0084 - mean_absolute_error: 0.0647 - val_loss: 0.0079 - val_mean_absolute_error: 0.0635\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.0084 - mean_absolute_error: 0.0645 - val_loss: 0.0081 - val_mean_absolute_error: 0.0653\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0084 - mean_absolute_error: 0.0649 - val_loss: 0.0078 - val_mean_absolute_error: 0.0634\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0084 - mean_absolute_error: 0.0646 - val_loss: 0.0077 - val_mean_absolute_error: 0.0630\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 15s 882us/step - loss: 0.0085 - mean_absolute_error: 0.0652 - val_loss: 0.0081 - val_mean_absolute_error: 0.0652\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.0089 - mean_absolute_error: 0.0663 - val_loss: 0.0078 - val_mean_absolute_error: 0.0638\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0084 - mean_absolute_error: 0.0648 - val_loss: 0.0077 - val_mean_absolute_error: 0.0627\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0085 - mean_absolute_error: 0.0652 - val_loss: 0.0079 - val_mean_absolute_error: 0.0641\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0086 - mean_absolute_error: 0.0655 - val_loss: 0.0084 - val_mean_absolute_error: 0.0654\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0086 - mean_absolute_error: 0.0655 - val_loss: 0.0078 - val_mean_absolute_error: 0.0637\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0085 - mean_absolute_error: 0.0652 - val_loss: 0.0091 - val_mean_absolute_error: 0.0685\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0084 - mean_absolute_error: 0.0650 - val_loss: 0.0080 - val_mean_absolute_error: 0.0638\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.0083 - mean_absolute_error: 0.0643 - val_loss: 0.0078 - val_mean_absolute_error: 0.0634\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.0081 - mean_absolute_error: 0.0639 - val_loss: 0.0078 - val_mean_absolute_error: 0.0632\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0087 - mean_absolute_error: 0.0654 - val_loss: 0.0080 - val_mean_absolute_error: 0.0645\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0084 - mean_absolute_error: 0.0645 - val_loss: 0.0079 - val_mean_absolute_error: 0.0637\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0083 - mean_absolute_error: 0.0643 - val_loss: 0.0080 - val_mean_absolute_error: 0.0643\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.0082 - mean_absolute_error: 0.0641 - val_loss: 0.0079 - val_mean_absolute_error: 0.0636\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0082 - mean_absolute_error: 0.0636 - val_loss: 0.0077 - val_mean_absolute_error: 0.0634\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.0083 - mean_absolute_error: 0.0634 - val_loss: 0.0079 - val_mean_absolute_error: 0.0636\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.0082 - mean_absolute_error: 0.0639 - val_loss: 0.0078 - val_mean_absolute_error: 0.0632\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 15s 886us/step - loss: 0.0082 - mean_absolute_error: 0.0640 - val_loss: 0.0079 - val_mean_absolute_error: 0.0639\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0085 - mean_absolute_error: 0.0646 - val_loss: 0.0078 - val_mean_absolute_error: 0.0629\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0082 - mean_absolute_error: 0.0637 - val_loss: 0.0075 - val_mean_absolute_error: 0.0627\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0083 - mean_absolute_error: 0.0638 - val_loss: 0.0081 - val_mean_absolute_error: 0.0648\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0085 - mean_absolute_error: 0.0646 - val_loss: 0.0082 - val_mean_absolute_error: 0.0648\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.0097 - mean_absolute_error: 0.0683 - val_loss: 0.0079 - val_mean_absolute_error: 0.0641\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.0085 - mean_absolute_error: 0.0653 - val_loss: 0.0080 - val_mean_absolute_error: 0.0643\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0083 - mean_absolute_error: 0.0643 - val_loss: 0.0079 - val_mean_absolute_error: 0.0633\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.0084 - mean_absolute_error: 0.0645 - val_loss: 0.0079 - val_mean_absolute_error: 0.0631\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0082 - mean_absolute_error: 0.0639 - val_loss: 0.0076 - val_mean_absolute_error: 0.0623\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0082 - mean_absolute_error: 0.0641 - val_loss: 0.0078 - val_mean_absolute_error: 0.0630\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.0082 - mean_absolute_error: 0.0637 - val_loss: 0.0079 - val_mean_absolute_error: 0.0634\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.0082 - mean_absolute_error: 0.0638 - val_loss: 0.0078 - val_mean_absolute_error: 0.0632\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.0081 - mean_absolute_error: 0.0638 - val_loss: 0.0079 - val_mean_absolute_error: 0.0639\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0080 - mean_absolute_error: 0.0633 - val_loss: 0.0077 - val_mean_absolute_error: 0.0627\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0083 - mean_absolute_error: 0.0641 - val_loss: 0.0085 - val_mean_absolute_error: 0.0663\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.0084 - mean_absolute_error: 0.0648 - val_loss: 0.0078 - val_mean_absolute_error: 0.0635\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.0082 - mean_absolute_error: 0.0636 - val_loss: 0.0076 - val_mean_absolute_error: 0.0623\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0080 - mean_absolute_error: 0.0631 - val_loss: 0.0075 - val_mean_absolute_error: 0.0620\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.0080 - mean_absolute_error: 0.0629 - val_loss: 0.0075 - val_mean_absolute_error: 0.0620\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.0081 - mean_absolute_error: 0.0635 - val_loss: 0.0075 - val_mean_absolute_error: 0.0617\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.0082 - mean_absolute_error: 0.0633 - val_loss: 0.0076 - val_mean_absolute_error: 0.0623\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.0080 - mean_absolute_error: 0.0627 - val_loss: 0.0078 - val_mean_absolute_error: 0.0632\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 15s 878us/step - loss: 0.0081 - mean_absolute_error: 0.0634 - val_loss: 0.0079 - val_mean_absolute_error: 0.0642\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 41s 5ms/step\n",
      "yhat shape:  (8760, 1)\n",
      "yhat_rescaled shape:  (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2011-01-01 00:00:00    407.292915\n",
      "2011-01-01 01:00:00    227.711182\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE validation_set: 80.61140662741218\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 41s 5ms/step\n",
      "yhat shape:  (8784, 1)\n",
      "yhat_rescaled shape:  (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2012-01-01 00:00:00    432.633003\n",
      "2012-01-01 01:00:00    337.138489\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_area237__y2012: 81.68107727956472\n",
      "######## START Experiment encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_area237__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928,)\n",
      "complete test_set shape:  (8952,)\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Train_data shape:  (17519, 1)\n",
      "valid_data shape:  (8928, 1)\n",
      "Test_data shape:  (8952, 1)\n",
      "X_train shape for autoencoder:  (17351, 168, 1)\n",
      "X_valid shape for autoencoder:  (8760, 168, 1)\n",
      "X_test shape for autoencoder:  (8784, 168, 1)\n",
      "y_train shape before model creation:  (17351, 1)\n",
      "y_valid shape before model creation:  (8760, 1)\n",
      "y_test shape before model creation:  (8784, 1)\n",
      "create stacked autoencoder 2 layer:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 24s 1ms/step - loss: 1.0088 - val_loss: 1.1934\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0004 - val_loss: 1.1809\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9899 - val_loss: 1.1731\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9824 - val_loss: 1.1657\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9840 - val_loss: 1.1967\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9982 - val_loss: 1.1775\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9798 - val_loss: 1.1789\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9863 - val_loss: 1.1628\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0151 - val_loss: 1.2031\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0070 - val_loss: 1.1986\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0067 - val_loss: 1.2009\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0068 - val_loss: 1.2008\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0066 - val_loss: 1.2003\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0059 - val_loss: 1.1980\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0021 - val_loss: 1.1872\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9930 - val_loss: 1.1933\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9973 - val_loss: 1.1822\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9844 - val_loss: 1.1744\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9870 - val_loss: 1.1828\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9840 - val_loss: 1.1636\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9907 - val_loss: 1.1789\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9837 - val_loss: 1.1659\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9869 - val_loss: 1.1802\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9777 - val_loss: 1.1640\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9895 - val_loss: 1.1869\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9900 - val_loss: 1.1738\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9779 - val_loss: 1.1613\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9887 - val_loss: 1.1829\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9876 - val_loss: 1.1722\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9768 - val_loss: 1.2037\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9910 - val_loss: 1.1877\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9908 - val_loss: 1.1738\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9832 - val_loss: 1.1684\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9756 - val_loss: 1.1615\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9793 - val_loss: 1.1744\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9798 - val_loss: 1.1612\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9686 - val_loss: 1.1474\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9632 - val_loss: 1.1555\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9535 - val_loss: 1.4565\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9856 - val_loss: 1.1421\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9773 - val_loss: 1.1649\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9648 - val_loss: 1.1347\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9571 - val_loss: 1.1642\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0082 - val_loss: 1.2014\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0062 - val_loss: 1.1975\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0004 - val_loss: 1.1837\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9918 - val_loss: 1.2014\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0130 - val_loss: 1.2019\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0072 - val_loss: 1.2011\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0070 - val_loss: 1.2011\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0069 - val_loss: 1.2011\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0069 - val_loss: 1.2010\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0069 - val_loss: 1.2010\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0069 - val_loss: 1.2010\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0068 - val_loss: 1.2010\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0068 - val_loss: 1.2009\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0068 - val_loss: 1.2009\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0067 - val_loss: 1.2008\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0066 - val_loss: 1.2005\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0062 - val_loss: 1.1992\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0043 - val_loss: 1.2011\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0067 - val_loss: 1.1998\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0050 - val_loss: 1.1969\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0010 - val_loss: 1.1859\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0029 - val_loss: 1.1915\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0002 - val_loss: 1.2025\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0074 - val_loss: 1.2005\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0061 - val_loss: 1.1993\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0047 - val_loss: 1.1962\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0003 - val_loss: 1.1843\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9945 - val_loss: 1.1918\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9937 - val_loss: 1.1777\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9768 - val_loss: 1.1474\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9938 - val_loss: 1.1865\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9874 - val_loss: 1.1674\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9707 - val_loss: 1.1448\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9552 - val_loss: 1.1388\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9498 - val_loss: 1.1210\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9778 - val_loss: 1.1982\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0053 - val_loss: 1.2028\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0082 - val_loss: 1.2009\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0068 - val_loss: 1.2005\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0063 - val_loss: 1.1997\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0052 - val_loss: 1.1970\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0036 - val_loss: 1.1902\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9897 - val_loss: 1.1594\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9803 - val_loss: 1.1614\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9662 - val_loss: 1.0511\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9899 - val_loss: 1.1934\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9949 - val_loss: 1.1747\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9817 - val_loss: 1.1614\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9709 - val_loss: 1.1215\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9805 - val_loss: 1.1554\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9641 - val_loss: 1.1379\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9472 - val_loss: 1.0961\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9377 - val_loss: 1.0747\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9321 - val_loss: 1.1036\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 1.0070 - val_loss: 1.1974\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9981 - val_loss: 1.1772\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9776 - val_loss: 1.1465\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9526 - val_loss: 1.0928\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9677 - val_loss: 1.1586\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9612 - val_loss: 1.1476\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.9321 - val_loss: 1.0505\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.8276 - val_loss: 0.8949\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.7753 - val_loss: 0.8334\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.7147 - val_loss: 0.8701\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.7372 - val_loss: 0.8189\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.7153 - val_loss: 0.8513\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6688 - val_loss: 0.7992\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.7131 - val_loss: 0.7694\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6546 - val_loss: 0.7963\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6741 - val_loss: 0.7711\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6403 - val_loss: 0.7249\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6303 - val_loss: 0.7923\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6543 - val_loss: 0.7064\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6456 - val_loss: 0.7315\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6128 - val_loss: 0.6783\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5956 - val_loss: 0.6947\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6092 - val_loss: 0.6911\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6069 - val_loss: 0.7262\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5889 - val_loss: 0.6307\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5847 - val_loss: 0.6617\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5644 - val_loss: 0.6301\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5718 - val_loss: 0.6128\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5680 - val_loss: 0.5831\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5522 - val_loss: 0.6753\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5681 - val_loss: 0.6078\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5427 - val_loss: 0.6012\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5256 - val_loss: 0.6108\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5255 - val_loss: 0.5463\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5199 - val_loss: 0.5831\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5803 - val_loss: 0.9766\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6976 - val_loss: 0.6779\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5280 - val_loss: 0.5794\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4914 - val_loss: 0.5406\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5087 - val_loss: 0.5354\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4987 - val_loss: 0.5391\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5028 - val_loss: 0.5239\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4928 - val_loss: 0.5341\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4978 - val_loss: 0.6004\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4803 - val_loss: 0.5213\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4827 - val_loss: 0.5101\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4942 - val_loss: 0.5047\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4719 - val_loss: 0.6011\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.5094 - val_loss: 0.5300\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4686 - val_loss: 0.4945\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4751 - val_loss: 0.4983\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4603 - val_loss: 0.4923\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.4628 - val_loss: 0.4902\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 128)\n",
      "X_valid_encoded shape:  (8760, 168, 128)\n",
      "X_test_encoded shape:  (8784, 168, 128)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 17s 996us/step - loss: 0.6218 - mean_absolute_error: 0.5765 - val_loss: 0.6338 - val_mean_absolute_error: 0.5883\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.5296 - mean_absolute_error: 0.5349 - val_loss: 0.5775 - val_mean_absolute_error: 0.5616\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.5053 - mean_absolute_error: 0.5173 - val_loss: 0.6292 - val_mean_absolute_error: 0.5720\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.4837 - mean_absolute_error: 0.5069 - val_loss: 0.4765 - val_mean_absolute_error: 0.5051\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.4276 - mean_absolute_error: 0.4790 - val_loss: 0.3883 - val_mean_absolute_error: 0.4652\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.4112 - mean_absolute_error: 0.4695 - val_loss: 0.3748 - val_mean_absolute_error: 0.4669\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.4371 - mean_absolute_error: 0.4766 - val_loss: 0.5741 - val_mean_absolute_error: 0.5472\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.4657 - mean_absolute_error: 0.4987 - val_loss: 0.4665 - val_mean_absolute_error: 0.4999\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.4142 - mean_absolute_error: 0.4668 - val_loss: 0.4271 - val_mean_absolute_error: 0.4741\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 15s 883us/step - loss: 0.3890 - mean_absolute_error: 0.4573 - val_loss: 0.4010 - val_mean_absolute_error: 0.4686\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.3854 - mean_absolute_error: 0.4543 - val_loss: 0.3690 - val_mean_absolute_error: 0.4496\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.3674 - mean_absolute_error: 0.4450 - val_loss: 0.3956 - val_mean_absolute_error: 0.4557\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3938 - mean_absolute_error: 0.4524 - val_loss: 0.4588 - val_mean_absolute_error: 0.4869\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 15s 889us/step - loss: 0.4258 - mean_absolute_error: 0.4719 - val_loss: 0.4365 - val_mean_absolute_error: 0.4737\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.4048 - mean_absolute_error: 0.4580 - val_loss: 0.4293 - val_mean_absolute_error: 0.4705\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3940 - mean_absolute_error: 0.4524 - val_loss: 0.4121 - val_mean_absolute_error: 0.4672\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 893us/step - loss: 0.3783 - mean_absolute_error: 0.4432 - val_loss: 0.3572 - val_mean_absolute_error: 0.4385\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3794 - mean_absolute_error: 0.4425 - val_loss: 0.4281 - val_mean_absolute_error: 0.4760\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.3560 - mean_absolute_error: 0.4316 - val_loss: 0.3253 - val_mean_absolute_error: 0.4194\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 15s 888us/step - loss: 0.3486 - mean_absolute_error: 0.4280 - val_loss: 0.3220 - val_mean_absolute_error: 0.4222\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.3596 - mean_absolute_error: 0.4335 - val_loss: 0.3388 - val_mean_absolute_error: 0.4305\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3619 - mean_absolute_error: 0.4346 - val_loss: 0.3467 - val_mean_absolute_error: 0.4297\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3874 - mean_absolute_error: 0.4476 - val_loss: 0.3370 - val_mean_absolute_error: 0.4371\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.3273 - mean_absolute_error: 0.4190 - val_loss: 0.3305 - val_mean_absolute_error: 0.4193\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.3212 - mean_absolute_error: 0.4116 - val_loss: 0.2964 - val_mean_absolute_error: 0.4069\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.3071 - mean_absolute_error: 0.4051 - val_loss: 0.3199 - val_mean_absolute_error: 0.4162\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2972 - mean_absolute_error: 0.3972 - val_loss: 0.2902 - val_mean_absolute_error: 0.4039\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3002 - mean_absolute_error: 0.3999 - val_loss: 0.2809 - val_mean_absolute_error: 0.3953\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.2901 - mean_absolute_error: 0.3935 - val_loss: 0.2754 - val_mean_absolute_error: 0.3899\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.2883 - mean_absolute_error: 0.3917 - val_loss: 0.2774 - val_mean_absolute_error: 0.3931\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2865 - mean_absolute_error: 0.3900 - val_loss: 0.2810 - val_mean_absolute_error: 0.3943\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.2859 - mean_absolute_error: 0.3897 - val_loss: 0.2861 - val_mean_absolute_error: 0.3928\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.3200 - mean_absolute_error: 0.4099 - val_loss: 0.2945 - val_mean_absolute_error: 0.4045\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3198 - mean_absolute_error: 0.4090 - val_loss: 0.3304 - val_mean_absolute_error: 0.4220\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2940 - mean_absolute_error: 0.3961 - val_loss: 0.2775 - val_mean_absolute_error: 0.3921\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2836 - mean_absolute_error: 0.3882 - val_loss: 0.2799 - val_mean_absolute_error: 0.3914\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.2867 - mean_absolute_error: 0.3897 - val_loss: 0.2714 - val_mean_absolute_error: 0.3875\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2936 - mean_absolute_error: 0.3948 - val_loss: 0.2833 - val_mean_absolute_error: 0.3911\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.2847 - mean_absolute_error: 0.3881 - val_loss: 0.2810 - val_mean_absolute_error: 0.3911\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2807 - mean_absolute_error: 0.3845 - val_loss: 0.2790 - val_mean_absolute_error: 0.3909\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.2841 - mean_absolute_error: 0.3874 - val_loss: 0.2769 - val_mean_absolute_error: 0.3909\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.2854 - mean_absolute_error: 0.3895 - val_loss: 0.2779 - val_mean_absolute_error: 0.3896\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2787 - mean_absolute_error: 0.3844 - val_loss: 0.2743 - val_mean_absolute_error: 0.3863\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2742 - mean_absolute_error: 0.3814 - val_loss: 0.2747 - val_mean_absolute_error: 0.3875\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2739 - mean_absolute_error: 0.3807 - val_loss: 0.2662 - val_mean_absolute_error: 0.3819\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.2739 - mean_absolute_error: 0.3810 - val_loss: 0.2644 - val_mean_absolute_error: 0.3843\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2715 - mean_absolute_error: 0.3793 - val_loss: 0.2658 - val_mean_absolute_error: 0.3810\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2729 - mean_absolute_error: 0.3795 - val_loss: 0.2666 - val_mean_absolute_error: 0.3822\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 917us/step - loss: 0.2714 - mean_absolute_error: 0.3787 - val_loss: 0.2620 - val_mean_absolute_error: 0.3772\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2740 - mean_absolute_error: 0.3804 - val_loss: 0.2668 - val_mean_absolute_error: 0.3857\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2776 - mean_absolute_error: 0.3817 - val_loss: 0.2900 - val_mean_absolute_error: 0.3974\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.2805 - mean_absolute_error: 0.3857 - val_loss: 0.2725 - val_mean_absolute_error: 0.3848\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2776 - mean_absolute_error: 0.3849 - val_loss: 0.2818 - val_mean_absolute_error: 0.3949\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2789 - mean_absolute_error: 0.3851 - val_loss: 0.2564 - val_mean_absolute_error: 0.3759\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2705 - mean_absolute_error: 0.3783 - val_loss: 0.2579 - val_mean_absolute_error: 0.3753\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2586 - mean_absolute_error: 0.3706 - val_loss: 0.2542 - val_mean_absolute_error: 0.3735\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.2619 - mean_absolute_error: 0.3720 - val_loss: 0.2610 - val_mean_absolute_error: 0.3783\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2612 - mean_absolute_error: 0.3714 - val_loss: 0.2514 - val_mean_absolute_error: 0.3711\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.2628 - mean_absolute_error: 0.3742 - val_loss: 0.2576 - val_mean_absolute_error: 0.3776\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2618 - mean_absolute_error: 0.3727 - val_loss: 0.2683 - val_mean_absolute_error: 0.3820\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2696 - mean_absolute_error: 0.3756 - val_loss: 0.2561 - val_mean_absolute_error: 0.3739\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2606 - mean_absolute_error: 0.3709 - val_loss: 0.2514 - val_mean_absolute_error: 0.3683\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2630 - mean_absolute_error: 0.3719 - val_loss: 0.2598 - val_mean_absolute_error: 0.3737\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2541 - mean_absolute_error: 0.3679 - val_loss: 0.2545 - val_mean_absolute_error: 0.3735\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.2545 - mean_absolute_error: 0.3666 - val_loss: 0.2444 - val_mean_absolute_error: 0.3641\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.2643 - mean_absolute_error: 0.3723 - val_loss: 0.2540 - val_mean_absolute_error: 0.3721\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.2920 - mean_absolute_error: 0.3901 - val_loss: 0.2660 - val_mean_absolute_error: 0.3803\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 15s 883us/step - loss: 0.2645 - mean_absolute_error: 0.3733 - val_loss: 0.2556 - val_mean_absolute_error: 0.3712\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2632 - mean_absolute_error: 0.3726 - val_loss: 0.2580 - val_mean_absolute_error: 0.3748\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.2597 - mean_absolute_error: 0.3705 - val_loss: 0.2498 - val_mean_absolute_error: 0.3686\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2659 - mean_absolute_error: 0.3747 - val_loss: 0.2456 - val_mean_absolute_error: 0.3665\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2531 - mean_absolute_error: 0.3656 - val_loss: 0.2499 - val_mean_absolute_error: 0.3679\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.2543 - mean_absolute_error: 0.3664 - val_loss: 0.2425 - val_mean_absolute_error: 0.3599\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.2591 - mean_absolute_error: 0.3691 - val_loss: 0.2389 - val_mean_absolute_error: 0.3619\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.2676 - mean_absolute_error: 0.3712 - val_loss: 0.2830 - val_mean_absolute_error: 0.3864\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2652 - mean_absolute_error: 0.3723 - val_loss: 0.2599 - val_mean_absolute_error: 0.3714\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.2604 - mean_absolute_error: 0.3702 - val_loss: 0.2700 - val_mean_absolute_error: 0.3771\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.2684 - mean_absolute_error: 0.3749 - val_loss: 0.2570 - val_mean_absolute_error: 0.3773\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2609 - mean_absolute_error: 0.3715 - val_loss: 0.2457 - val_mean_absolute_error: 0.3669\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2548 - mean_absolute_error: 0.3663 - val_loss: 0.2472 - val_mean_absolute_error: 0.3653\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.2535 - mean_absolute_error: 0.3649 - val_loss: 0.2586 - val_mean_absolute_error: 0.3711\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2599 - mean_absolute_error: 0.3678 - val_loss: 0.2466 - val_mean_absolute_error: 0.3629\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.2550 - mean_absolute_error: 0.3650 - val_loss: 0.2524 - val_mean_absolute_error: 0.3678\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2513 - mean_absolute_error: 0.3634 - val_loss: 0.2389 - val_mean_absolute_error: 0.3621\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2583 - mean_absolute_error: 0.3678 - val_loss: 0.2426 - val_mean_absolute_error: 0.3639\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2555 - mean_absolute_error: 0.3667 - val_loss: 0.2467 - val_mean_absolute_error: 0.3643\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2527 - mean_absolute_error: 0.3643 - val_loss: 0.2412 - val_mean_absolute_error: 0.3592\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2500 - mean_absolute_error: 0.3617 - val_loss: 0.2495 - val_mean_absolute_error: 0.3648\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2560 - mean_absolute_error: 0.3662 - val_loss: 0.2362 - val_mean_absolute_error: 0.3567\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.2495 - mean_absolute_error: 0.3623 - val_loss: 0.2508 - val_mean_absolute_error: 0.3659\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.2486 - mean_absolute_error: 0.3610 - val_loss: 0.2346 - val_mean_absolute_error: 0.3557\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2432 - mean_absolute_error: 0.3579 - val_loss: 0.2404 - val_mean_absolute_error: 0.3605\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.2529 - mean_absolute_error: 0.3620 - val_loss: 0.2401 - val_mean_absolute_error: 0.3580\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2462 - mean_absolute_error: 0.3587 - val_loss: 0.2436 - val_mean_absolute_error: 0.3594\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.2475 - mean_absolute_error: 0.3594 - val_loss: 0.2345 - val_mean_absolute_error: 0.3542\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2449 - mean_absolute_error: 0.3577 - val_loss: 0.2345 - val_mean_absolute_error: 0.3535\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2407 - mean_absolute_error: 0.3539 - val_loss: 0.2355 - val_mean_absolute_error: 0.3511\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2425 - mean_absolute_error: 0.3562 - val_loss: 0.2425 - val_mean_absolute_error: 0.3619\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2457 - mean_absolute_error: 0.3591 - val_loss: 0.2316 - val_mean_absolute_error: 0.3510\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2444 - mean_absolute_error: 0.3565 - val_loss: 0.2365 - val_mean_absolute_error: 0.3586\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2393 - mean_absolute_error: 0.3544 - val_loss: 0.2357 - val_mean_absolute_error: 0.3539\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 15s 889us/step - loss: 0.2383 - mean_absolute_error: 0.3528 - val_loss: 0.2311 - val_mean_absolute_error: 0.3529\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.2486 - mean_absolute_error: 0.3586 - val_loss: 0.2469 - val_mean_absolute_error: 0.3596\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.2408 - mean_absolute_error: 0.3539 - val_loss: 0.2424 - val_mean_absolute_error: 0.3594\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2385 - mean_absolute_error: 0.3534 - val_loss: 0.2388 - val_mean_absolute_error: 0.3581\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2398 - mean_absolute_error: 0.3551 - val_loss: 0.2353 - val_mean_absolute_error: 0.3572\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2415 - mean_absolute_error: 0.3556 - val_loss: 0.2345 - val_mean_absolute_error: 0.3544\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2501 - mean_absolute_error: 0.3601 - val_loss: 0.2441 - val_mean_absolute_error: 0.3648\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.2449 - mean_absolute_error: 0.3585 - val_loss: 0.2315 - val_mean_absolute_error: 0.3538\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.2398 - mean_absolute_error: 0.3536 - val_loss: 0.2327 - val_mean_absolute_error: 0.3547\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2442 - mean_absolute_error: 0.3569 - val_loss: 0.2613 - val_mean_absolute_error: 0.3690\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.2427 - mean_absolute_error: 0.3570 - val_loss: 0.2339 - val_mean_absolute_error: 0.3518\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2389 - mean_absolute_error: 0.3531 - val_loss: 0.2352 - val_mean_absolute_error: 0.3513\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2355 - mean_absolute_error: 0.3503 - val_loss: 0.2320 - val_mean_absolute_error: 0.3536\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2369 - mean_absolute_error: 0.3510 - val_loss: 0.2300 - val_mean_absolute_error: 0.3501\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2531 - mean_absolute_error: 0.3585 - val_loss: 0.2355 - val_mean_absolute_error: 0.3564\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2394 - mean_absolute_error: 0.3535 - val_loss: 0.2337 - val_mean_absolute_error: 0.3530\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 15s 888us/step - loss: 0.2435 - mean_absolute_error: 0.3555 - val_loss: 0.2488 - val_mean_absolute_error: 0.3632\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 918us/step - loss: 0.2450 - mean_absolute_error: 0.3572 - val_loss: 0.2337 - val_mean_absolute_error: 0.3523\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.2390 - mean_absolute_error: 0.3535 - val_loss: 0.2250 - val_mean_absolute_error: 0.3471\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.2381 - mean_absolute_error: 0.3529 - val_loss: 0.2256 - val_mean_absolute_error: 0.3460\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.2388 - mean_absolute_error: 0.3544 - val_loss: 0.2384 - val_mean_absolute_error: 0.3560\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.2381 - mean_absolute_error: 0.3525 - val_loss: 0.2256 - val_mean_absolute_error: 0.3482\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 15s 884us/step - loss: 0.2363 - mean_absolute_error: 0.3524 - val_loss: 0.2376 - val_mean_absolute_error: 0.3576\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2404 - mean_absolute_error: 0.3544 - val_loss: 0.2304 - val_mean_absolute_error: 0.3513\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.2346 - mean_absolute_error: 0.3504 - val_loss: 0.2310 - val_mean_absolute_error: 0.3512\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2319 - mean_absolute_error: 0.3489 - val_loss: 0.2262 - val_mean_absolute_error: 0.3481\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.2353 - mean_absolute_error: 0.3496 - val_loss: 0.2272 - val_mean_absolute_error: 0.3490\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.2352 - mean_absolute_error: 0.3504 - val_loss: 0.2307 - val_mean_absolute_error: 0.3488\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.2397 - mean_absolute_error: 0.3525 - val_loss: 0.2283 - val_mean_absolute_error: 0.3459\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.2331 - mean_absolute_error: 0.3483 - val_loss: 0.2280 - val_mean_absolute_error: 0.3471\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.2364 - mean_absolute_error: 0.3512 - val_loss: 0.2298 - val_mean_absolute_error: 0.3492\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.2446 - mean_absolute_error: 0.3538 - val_loss: 0.2341 - val_mean_absolute_error: 0.3521\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2370 - mean_absolute_error: 0.3511 - val_loss: 0.2319 - val_mean_absolute_error: 0.3523\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.2298 - mean_absolute_error: 0.3466 - val_loss: 0.2218 - val_mean_absolute_error: 0.3437\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.2320 - mean_absolute_error: 0.3473 - val_loss: 0.2254 - val_mean_absolute_error: 0.3442\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.2343 - mean_absolute_error: 0.3507 - val_loss: 0.2338 - val_mean_absolute_error: 0.3595\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.2339 - mean_absolute_error: 0.3497 - val_loss: 0.2331 - val_mean_absolute_error: 0.3514\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.2324 - mean_absolute_error: 0.3475 - val_loss: 0.2240 - val_mean_absolute_error: 0.3452\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.2312 - mean_absolute_error: 0.3476 - val_loss: 0.2227 - val_mean_absolute_error: 0.3458\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 15s 891us/step - loss: 0.2309 - mean_absolute_error: 0.3468 - val_loss: 0.2250 - val_mean_absolute_error: 0.3469\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2310 - mean_absolute_error: 0.3470 - val_loss: 0.2237 - val_mean_absolute_error: 0.3431\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2292 - mean_absolute_error: 0.3451 - val_loss: 0.2271 - val_mean_absolute_error: 0.3469\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.2488 - mean_absolute_error: 0.3570 - val_loss: 0.2337 - val_mean_absolute_error: 0.3501\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.2342 - mean_absolute_error: 0.3490 - val_loss: 0.2273 - val_mean_absolute_error: 0.3456\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.2281 - mean_absolute_error: 0.3445 - val_loss: 0.2234 - val_mean_absolute_error: 0.3425\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.2329 - mean_absolute_error: 0.3474 - val_loss: 0.2255 - val_mean_absolute_error: 0.3448\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.2374 - mean_absolute_error: 0.3506 - val_loss: 0.2334 - val_mean_absolute_error: 0.3531\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.2288 - mean_absolute_error: 0.3448 - val_loss: 0.2262 - val_mean_absolute_error: 0.3464\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.2328 - mean_absolute_error: 0.3480 - val_loss: 0.2222 - val_mean_absolute_error: 0.3442\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 42s 5ms/step\n",
      "yhat shape:  (8760, 1)\n",
      "yhat_rescaled shape:  (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2011-01-01 00:00:00    386.870972\n",
      "2011-01-01 01:00:00    194.383331\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE validation_set: 87.45553226681737\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 41s 5ms/step\n",
      "yhat shape:  (8784, 1)\n",
      "yhat_rescaled shape:  (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2012-01-01 00:00:00    409.536163\n",
      "2012-01-01 01:00:00    320.104263\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_area237__y2012: 88.84515132333232\n",
      "######## START Experiment multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928, 10)\n",
      "complete test_set shape:  (8952, 10)\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Train_data shape:  (17519, 10)\n",
      "valid_data shape:  (8928, 10)\n",
      "Test_data shape:  (8952, 10)\n",
      "X_train shape for autoencoder:  (17351, 168, 10)\n",
      "X_valid shape for autoencoder:  (8760, 168, 10)\n",
      "X_test shape for autoencoder:  (8784, 168, 10)\n",
      "y_train shape before model creation:  (17351, 10)\n",
      "y_valid shape before model creation:  (8760, 10)\n",
      "y_test shape before model creation:  (8784, 10)\n",
      "create stacked autoencoder 2 layer:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 25s 1ms/step - loss: 0.0397 - val_loss: 0.0428\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0377 - val_loss: 0.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0374 - val_loss: 0.0424\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0381 - val_loss: 0.0427\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0371 - val_loss: 0.0420\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0368 - val_loss: 0.0422\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0360 - val_loss: 0.0415\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0335 - val_loss: 0.0372\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0317 - val_loss: 0.0358\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0296 - val_loss: 0.0357\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0268 - val_loss: 0.0300\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0261 - val_loss: 0.0279\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0253 - val_loss: 0.0272\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0252 - val_loss: 0.0270\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0246 - val_loss: 0.0258\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0240 - val_loss: 0.0283\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0242 - val_loss: 0.0282\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0237 - val_loss: 0.0249\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0231 - val_loss: 0.0279\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0226 - val_loss: 0.0261\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0225 - val_loss: 0.0264\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0219 - val_loss: 0.0230\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0214 - val_loss: 0.0250\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0216 - val_loss: 0.0250\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0212 - val_loss: 0.0236\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0210 - val_loss: 0.0219\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0210 - val_loss: 0.0222\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0208 - val_loss: 0.0249\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0207 - val_loss: 0.0228\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0206 - val_loss: 0.0221\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0207 - val_loss: 0.0218\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0204 - val_loss: 0.0257\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0207 - val_loss: 0.0215\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0198 - val_loss: 0.0213\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0202 - val_loss: 0.0208\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0197 - val_loss: 0.0220\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0196 - val_loss: 0.0206\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0200 - val_loss: 0.0228\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0192 - val_loss: 0.0218\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0194 - val_loss: 0.0208\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0201 - val_loss: 0.0204\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0202\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0232\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0187 - val_loss: 0.0197\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0192 - val_loss: 0.0222\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0220\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0191 - val_loss: 0.0199\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0189 - val_loss: 0.0227\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0184 - val_loss: 0.0195\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0187 - val_loss: 0.0217\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0189 - val_loss: 0.0210\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0187 - val_loss: 0.0222\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0181 - val_loss: 0.0194\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0183 - val_loss: 0.0204\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0209\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0197 - val_loss: 0.0218\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0175 - val_loss: 0.0190\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0173 - val_loss: 0.0232\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0176 - val_loss: 0.0193\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0171 - val_loss: 0.0189\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0205 - val_loss: 0.0199\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0180 - val_loss: 0.0204\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0175 - val_loss: 0.0191\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0181 - val_loss: 0.0191\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0172 - val_loss: 0.0197\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0185 - val_loss: 0.0228\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0195 - val_loss: 0.0194\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0179 - val_loss: 0.0189\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0171 - val_loss: 0.0187\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0168 - val_loss: 0.0187\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0210 - val_loss: 0.0226\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0192\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0172 - val_loss: 0.0197\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0178 - val_loss: 0.0201\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0174 - val_loss: 0.0187\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0167 - val_loss: 0.0184\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0166 - val_loss: 0.0186\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0204 - val_loss: 0.0222\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0188 - val_loss: 0.0194\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0174 - val_loss: 0.0192\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0228 - val_loss: 0.0219\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0180 - val_loss: 0.0190\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0171 - val_loss: 0.0189\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0168 - val_loss: 0.0188\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0220 - val_loss: 0.0293\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0196 - val_loss: 0.0207\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0165 - val_loss: 0.0182\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0165 - val_loss: 0.0184\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0164 - val_loss: 0.0184\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0164 - val_loss: 0.0181\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0163 - val_loss: 0.0182\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0163 - val_loss: 0.0180\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0161 - val_loss: 0.0179\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0246 - val_loss: 0.0265\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0211 - val_loss: 0.0206\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0175 - val_loss: 0.0221\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0188 - val_loss: 0.0189\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0169 - val_loss: 0.0184\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0166 - val_loss: 0.0182\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0163 - val_loss: 0.0178\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0162 - val_loss: 0.0176\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0159 - val_loss: 0.0172\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0163 - val_loss: 0.0213\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0249 - val_loss: 0.0218\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0176 - val_loss: 0.0188\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0166 - val_loss: 0.0179\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0161 - val_loss: 0.0175\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0159 - val_loss: 0.0176\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0231 - val_loss: 0.0207\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0173 - val_loss: 0.0182\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0160 - val_loss: 0.0171\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0195 - val_loss: 0.0266\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0179 - val_loss: 0.0158\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0140 - val_loss: 0.0153\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0181 - val_loss: 0.0152\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0136 - val_loss: 0.0144\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0133 - val_loss: 0.0139\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0177 - val_loss: 0.0269\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0181 - val_loss: 0.0234\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0199 - val_loss: 0.0186\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0252 - val_loss: 0.0273\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0215 - val_loss: 0.0200\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0178 - val_loss: 0.0249\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0218 - val_loss: 0.0198\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0198 - val_loss: 0.0218\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0187 - val_loss: 0.0251\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0268 - val_loss: 0.0336\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0267 - val_loss: 0.0282\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.0258 - val_loss: 0.0279\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0254 - val_loss: 0.0262\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0248 - val_loss: 0.0245\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0224 - val_loss: 0.0236\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0263 - val_loss: 0.0263\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 128)\n",
      "X_valid_encoded shape:  (8760, 168, 128)\n",
      "X_test_encoded shape:  (8784, 168, 128)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.0311 - mean_absolute_error: 0.1298 - val_loss: 0.0264 - val_mean_absolute_error: 0.1177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0240 - mean_absolute_error: 0.1127 - val_loss: 0.0244 - val_mean_absolute_error: 0.1127\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 939us/step - loss: 0.0219 - mean_absolute_error: 0.1077 - val_loss: 0.0217 - val_mean_absolute_error: 0.1074\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 16s 940us/step - loss: 0.0200 - mean_absolute_error: 0.1025 - val_loss: 0.0191 - val_mean_absolute_error: 0.1000\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0179 - mean_absolute_error: 0.0968 - val_loss: 0.0165 - val_mean_absolute_error: 0.0926\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0165 - mean_absolute_error: 0.0928 - val_loss: 0.0166 - val_mean_absolute_error: 0.0928\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0156 - mean_absolute_error: 0.0896 - val_loss: 0.0157 - val_mean_absolute_error: 0.0897\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0150 - mean_absolute_error: 0.0879 - val_loss: 0.0150 - val_mean_absolute_error: 0.0872\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0143 - mean_absolute_error: 0.0858 - val_loss: 0.0150 - val_mean_absolute_error: 0.0872\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0144 - mean_absolute_error: 0.0858 - val_loss: 0.0149 - val_mean_absolute_error: 0.0872\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0143 - mean_absolute_error: 0.0854 - val_loss: 0.0143 - val_mean_absolute_error: 0.0847\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0139 - mean_absolute_error: 0.0841 - val_loss: 0.0137 - val_mean_absolute_error: 0.0825\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0136 - mean_absolute_error: 0.0832 - val_loss: 0.0136 - val_mean_absolute_error: 0.0824\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0134 - mean_absolute_error: 0.0825 - val_loss: 0.0137 - val_mean_absolute_error: 0.0830\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0132 - mean_absolute_error: 0.0814 - val_loss: 0.0131 - val_mean_absolute_error: 0.0805\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0131 - mean_absolute_error: 0.0810 - val_loss: 0.0125 - val_mean_absolute_error: 0.0789\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0128 - mean_absolute_error: 0.0801 - val_loss: 0.0128 - val_mean_absolute_error: 0.0800\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0127 - mean_absolute_error: 0.0797 - val_loss: 0.0128 - val_mean_absolute_error: 0.0797\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0126 - mean_absolute_error: 0.0794 - val_loss: 0.0125 - val_mean_absolute_error: 0.0787\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0125 - mean_absolute_error: 0.0787 - val_loss: 0.0129 - val_mean_absolute_error: 0.0807\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0125 - mean_absolute_error: 0.0788 - val_loss: 0.0120 - val_mean_absolute_error: 0.0767\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0124 - mean_absolute_error: 0.0784 - val_loss: 0.0123 - val_mean_absolute_error: 0.0781\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 937us/step - loss: 0.0122 - mean_absolute_error: 0.0775 - val_loss: 0.0122 - val_mean_absolute_error: 0.0775\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0123 - mean_absolute_error: 0.0781 - val_loss: 0.0118 - val_mean_absolute_error: 0.0765\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0121 - mean_absolute_error: 0.0774 - val_loss: 0.0117 - val_mean_absolute_error: 0.0756\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0131 - mean_absolute_error: 0.0798 - val_loss: 0.0139 - val_mean_absolute_error: 0.0826\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0140 - mean_absolute_error: 0.0832 - val_loss: 0.0129 - val_mean_absolute_error: 0.0794\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0130 - mean_absolute_error: 0.0799 - val_loss: 0.0139 - val_mean_absolute_error: 0.0825\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0128 - mean_absolute_error: 0.0800 - val_loss: 0.0128 - val_mean_absolute_error: 0.0793\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0123 - mean_absolute_error: 0.0779 - val_loss: 0.0119 - val_mean_absolute_error: 0.0765\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 937us/step - loss: 0.0118 - mean_absolute_error: 0.0763 - val_loss: 0.0119 - val_mean_absolute_error: 0.0759\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0119 - mean_absolute_error: 0.0764 - val_loss: 0.0119 - val_mean_absolute_error: 0.0764\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0122 - mean_absolute_error: 0.0776 - val_loss: 0.0118 - val_mean_absolute_error: 0.0760\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0118 - mean_absolute_error: 0.0764 - val_loss: 0.0116 - val_mean_absolute_error: 0.0747\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0115 - mean_absolute_error: 0.0752 - val_loss: 0.0114 - val_mean_absolute_error: 0.0745\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0115 - mean_absolute_error: 0.0750 - val_loss: 0.0113 - val_mean_absolute_error: 0.0741\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0116 - mean_absolute_error: 0.0753 - val_loss: 0.0116 - val_mean_absolute_error: 0.0755\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0113 - mean_absolute_error: 0.0745 - val_loss: 0.0114 - val_mean_absolute_error: 0.0743\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0115 - mean_absolute_error: 0.0747 - val_loss: 0.0115 - val_mean_absolute_error: 0.0746\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0112 - mean_absolute_error: 0.0738 - val_loss: 0.0112 - val_mean_absolute_error: 0.0737\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0113 - mean_absolute_error: 0.0743 - val_loss: 0.0112 - val_mean_absolute_error: 0.0736\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0110 - mean_absolute_error: 0.0732 - val_loss: 0.0109 - val_mean_absolute_error: 0.0725\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0110 - mean_absolute_error: 0.0731 - val_loss: 0.0110 - val_mean_absolute_error: 0.0729\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0112 - mean_absolute_error: 0.0736 - val_loss: 0.0113 - val_mean_absolute_error: 0.0739\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 943us/step - loss: 0.0113 - mean_absolute_error: 0.0739 - val_loss: 0.0112 - val_mean_absolute_error: 0.0736\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0111 - mean_absolute_error: 0.0733 - val_loss: 0.0109 - val_mean_absolute_error: 0.0727\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0110 - mean_absolute_error: 0.0727 - val_loss: 0.0118 - val_mean_absolute_error: 0.0752\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0112 - mean_absolute_error: 0.0735 - val_loss: 0.0110 - val_mean_absolute_error: 0.0731\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0110 - mean_absolute_error: 0.0730 - val_loss: 0.0112 - val_mean_absolute_error: 0.0737\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0123 - mean_absolute_error: 0.0773 - val_loss: 0.0121 - val_mean_absolute_error: 0.0767\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 942us/step - loss: 0.0116 - mean_absolute_error: 0.0751 - val_loss: 0.0112 - val_mean_absolute_error: 0.0741\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0112 - mean_absolute_error: 0.0738 - val_loss: 0.0119 - val_mean_absolute_error: 0.0749\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0110 - mean_absolute_error: 0.0730 - val_loss: 0.0110 - val_mean_absolute_error: 0.0729\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0108 - mean_absolute_error: 0.0723 - val_loss: 0.0111 - val_mean_absolute_error: 0.0726\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0108 - mean_absolute_error: 0.0721 - val_loss: 0.0108 - val_mean_absolute_error: 0.0721\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0106 - mean_absolute_error: 0.0717 - val_loss: 0.0107 - val_mean_absolute_error: 0.0718\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0106 - mean_absolute_error: 0.0716 - val_loss: 0.0110 - val_mean_absolute_error: 0.0732\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0113 - mean_absolute_error: 0.0736 - val_loss: 0.0114 - val_mean_absolute_error: 0.0748\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 939us/step - loss: 0.0113 - mean_absolute_error: 0.0737 - val_loss: 0.0110 - val_mean_absolute_error: 0.0732\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0108 - mean_absolute_error: 0.0722 - val_loss: 0.0108 - val_mean_absolute_error: 0.0720\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0106 - mean_absolute_error: 0.0716 - val_loss: 0.0107 - val_mean_absolute_error: 0.0720\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0109 - mean_absolute_error: 0.0722 - val_loss: 0.0109 - val_mean_absolute_error: 0.0720\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0111 - mean_absolute_error: 0.0728 - val_loss: 0.0107 - val_mean_absolute_error: 0.0719\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0106 - mean_absolute_error: 0.0715 - val_loss: 0.0106 - val_mean_absolute_error: 0.0715\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0106 - mean_absolute_error: 0.0713 - val_loss: 0.0107 - val_mean_absolute_error: 0.0714\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0105 - mean_absolute_error: 0.0710 - val_loss: 0.0105 - val_mean_absolute_error: 0.0709\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0106 - mean_absolute_error: 0.0713 - val_loss: 0.0108 - val_mean_absolute_error: 0.0721\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0105 - mean_absolute_error: 0.0710 - val_loss: 0.0105 - val_mean_absolute_error: 0.0710\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0104 - mean_absolute_error: 0.0708 - val_loss: 0.0106 - val_mean_absolute_error: 0.0715\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0104 - mean_absolute_error: 0.0706 - val_loss: 0.0104 - val_mean_absolute_error: 0.0706\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0103 - mean_absolute_error: 0.0704 - val_loss: 0.0104 - val_mean_absolute_error: 0.0706\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0103 - mean_absolute_error: 0.0702 - val_loss: 0.0104 - val_mean_absolute_error: 0.0705\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0103 - mean_absolute_error: 0.0704 - val_loss: 0.0106 - val_mean_absolute_error: 0.0716\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 937us/step - loss: 0.0103 - mean_absolute_error: 0.0705 - val_loss: 0.0103 - val_mean_absolute_error: 0.0710\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0106 - mean_absolute_error: 0.0715 - val_loss: 0.0107 - val_mean_absolute_error: 0.0719\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0104 - mean_absolute_error: 0.0708 - val_loss: 0.0105 - val_mean_absolute_error: 0.0710\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0104 - mean_absolute_error: 0.0704 - val_loss: 0.0103 - val_mean_absolute_error: 0.0705\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0102 - mean_absolute_error: 0.0699 - val_loss: 0.0105 - val_mean_absolute_error: 0.0710\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0104 - mean_absolute_error: 0.0706 - val_loss: 0.0107 - val_mean_absolute_error: 0.0723\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0103 - mean_absolute_error: 0.0704 - val_loss: 0.0103 - val_mean_absolute_error: 0.0705\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0102 - mean_absolute_error: 0.0697 - val_loss: 0.0103 - val_mean_absolute_error: 0.0710\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0102 - mean_absolute_error: 0.0698 - val_loss: 0.0105 - val_mean_absolute_error: 0.0707\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0102 - mean_absolute_error: 0.0699 - val_loss: 0.0103 - val_mean_absolute_error: 0.0701\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0103 - mean_absolute_error: 0.0704 - val_loss: 0.0106 - val_mean_absolute_error: 0.0714\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 16s 927us/step - loss: 0.0103 - mean_absolute_error: 0.0702 - val_loss: 0.0103 - val_mean_absolute_error: 0.0704\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0101 - mean_absolute_error: 0.0696 - val_loss: 0.0103 - val_mean_absolute_error: 0.0702\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0101 - mean_absolute_error: 0.0697 - val_loss: 0.0104 - val_mean_absolute_error: 0.0708\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0106 - mean_absolute_error: 0.0712 - val_loss: 0.0103 - val_mean_absolute_error: 0.0704\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0102 - mean_absolute_error: 0.0700 - val_loss: 0.0101 - val_mean_absolute_error: 0.0694\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0101 - mean_absolute_error: 0.0693 - val_loss: 0.0106 - val_mean_absolute_error: 0.0712\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0103 - mean_absolute_error: 0.0701 - val_loss: 0.0104 - val_mean_absolute_error: 0.0713\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0101 - mean_absolute_error: 0.0697 - val_loss: 0.0103 - val_mean_absolute_error: 0.0702\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0100 - mean_absolute_error: 0.0693 - val_loss: 0.0103 - val_mean_absolute_error: 0.0706\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0101 - mean_absolute_error: 0.0693 - val_loss: 0.0102 - val_mean_absolute_error: 0.0702\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0102 - mean_absolute_error: 0.0697 - val_loss: 0.0105 - val_mean_absolute_error: 0.0708\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0101 - mean_absolute_error: 0.0693 - val_loss: 0.0101 - val_mean_absolute_error: 0.0693\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0100 - mean_absolute_error: 0.0692 - val_loss: 0.0106 - val_mean_absolute_error: 0.0712\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0100 - mean_absolute_error: 0.0691 - val_loss: 0.0101 - val_mean_absolute_error: 0.0695\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0100 - mean_absolute_error: 0.0689 - val_loss: 0.0104 - val_mean_absolute_error: 0.0707\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0100 - mean_absolute_error: 0.0689 - val_loss: 0.0101 - val_mean_absolute_error: 0.0697\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0101 - mean_absolute_error: 0.0694 - val_loss: 0.0103 - val_mean_absolute_error: 0.0701\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0100 - mean_absolute_error: 0.0690 - val_loss: 0.0102 - val_mean_absolute_error: 0.0696\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 926us/step - loss: 0.0100 - mean_absolute_error: 0.0691 - val_loss: 0.0104 - val_mean_absolute_error: 0.0705\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0103 - mean_absolute_error: 0.0700 - val_loss: 0.0102 - val_mean_absolute_error: 0.0697\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0100 - mean_absolute_error: 0.0689 - val_loss: 0.0102 - val_mean_absolute_error: 0.0698\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0099 - mean_absolute_error: 0.0689 - val_loss: 0.0107 - val_mean_absolute_error: 0.0712\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0105 - mean_absolute_error: 0.0708 - val_loss: 0.0103 - val_mean_absolute_error: 0.0702\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 937us/step - loss: 0.0101 - mean_absolute_error: 0.0692 - val_loss: 0.0102 - val_mean_absolute_error: 0.0699\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0100 - mean_absolute_error: 0.0690 - val_loss: 0.0104 - val_mean_absolute_error: 0.0701\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0099 - mean_absolute_error: 0.0689 - val_loss: 0.0102 - val_mean_absolute_error: 0.0696\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0099 - mean_absolute_error: 0.0686 - val_loss: 0.0101 - val_mean_absolute_error: 0.0698\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0098 - mean_absolute_error: 0.0684 - val_loss: 0.0100 - val_mean_absolute_error: 0.0688\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0099 - mean_absolute_error: 0.0685 - val_loss: 0.0105 - val_mean_absolute_error: 0.0707\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0102 - mean_absolute_error: 0.0696 - val_loss: 0.0105 - val_mean_absolute_error: 0.0708\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0110 - mean_absolute_error: 0.0721 - val_loss: 0.0110 - val_mean_absolute_error: 0.0734\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 937us/step - loss: 0.0103 - mean_absolute_error: 0.0704 - val_loss: 0.0102 - val_mean_absolute_error: 0.0699\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0101 - mean_absolute_error: 0.0695 - val_loss: 0.0101 - val_mean_absolute_error: 0.0693\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0100 - mean_absolute_error: 0.0689 - val_loss: 0.0121 - val_mean_absolute_error: 0.0755\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0101 - mean_absolute_error: 0.0694 - val_loss: 0.0102 - val_mean_absolute_error: 0.0705\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 924us/step - loss: 0.0099 - mean_absolute_error: 0.0686 - val_loss: 0.0101 - val_mean_absolute_error: 0.0693\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0098 - mean_absolute_error: 0.0685 - val_loss: 0.0100 - val_mean_absolute_error: 0.0691\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 940us/step - loss: 0.0098 - mean_absolute_error: 0.0684 - val_loss: 0.0100 - val_mean_absolute_error: 0.0696\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0097 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0690\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 16s 935us/step - loss: 0.0098 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0691\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0098 - mean_absolute_error: 0.0682 - val_loss: 0.0109 - val_mean_absolute_error: 0.0721\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 931us/step - loss: 0.0105 - mean_absolute_error: 0.0711 - val_loss: 0.0111 - val_mean_absolute_error: 0.0741\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0102 - mean_absolute_error: 0.0701 - val_loss: 0.0100 - val_mean_absolute_error: 0.0694\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 922us/step - loss: 0.0099 - mean_absolute_error: 0.0688 - val_loss: 0.0100 - val_mean_absolute_error: 0.0690\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0098 - mean_absolute_error: 0.0681 - val_loss: 0.0099 - val_mean_absolute_error: 0.0687\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0098 - mean_absolute_error: 0.0683 - val_loss: 0.0100 - val_mean_absolute_error: 0.0689\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 16s 928us/step - loss: 0.0098 - mean_absolute_error: 0.0684 - val_loss: 0.0100 - val_mean_absolute_error: 0.0693\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0097 - mean_absolute_error: 0.0678 - val_loss: 0.0099 - val_mean_absolute_error: 0.0687\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 921us/step - loss: 0.0098 - mean_absolute_error: 0.0682 - val_loss: 0.0100 - val_mean_absolute_error: 0.0691\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0097 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0690\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 932us/step - loss: 0.0097 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0691\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0097 - mean_absolute_error: 0.0678 - val_loss: 0.0102 - val_mean_absolute_error: 0.0695\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0098 - mean_absolute_error: 0.0684 - val_loss: 0.0099 - val_mean_absolute_error: 0.0688\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 16s 925us/step - loss: 0.0097 - mean_absolute_error: 0.0679 - val_loss: 0.0099 - val_mean_absolute_error: 0.0686\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 16s 940us/step - loss: 0.0096 - mean_absolute_error: 0.0677 - val_loss: 0.0099 - val_mean_absolute_error: 0.0683\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0097 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0690\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 16s 933us/step - loss: 0.0097 - mean_absolute_error: 0.0680 - val_loss: 0.0099 - val_mean_absolute_error: 0.0688\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0097 - mean_absolute_error: 0.0677 - val_loss: 0.0100 - val_mean_absolute_error: 0.0693\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 16s 919us/step - loss: 0.0096 - mean_absolute_error: 0.0677 - val_loss: 0.0099 - val_mean_absolute_error: 0.0689\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 923us/step - loss: 0.0104 - mean_absolute_error: 0.0697 - val_loss: 0.0114 - val_mean_absolute_error: 0.0731\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 934us/step - loss: 0.0103 - mean_absolute_error: 0.0699 - val_loss: 0.0103 - val_mean_absolute_error: 0.0698\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 941us/step - loss: 0.0099 - mean_absolute_error: 0.0685 - val_loss: 0.0103 - val_mean_absolute_error: 0.0697\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 929us/step - loss: 0.0099 - mean_absolute_error: 0.0684 - val_loss: 0.0102 - val_mean_absolute_error: 0.0694\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 936us/step - loss: 0.0098 - mean_absolute_error: 0.0683 - val_loss: 0.0100 - val_mean_absolute_error: 0.0690\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 938us/step - loss: 0.0098 - mean_absolute_error: 0.0682 - val_loss: 0.0100 - val_mean_absolute_error: 0.0688\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 16s 930us/step - loss: 0.0098 - mean_absolute_error: 0.0681 - val_loss: 0.0099 - val_mean_absolute_error: 0.0684\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 42s 5ms/step\n",
      "yhat shape:  (8760, 10)\n",
      "yhat_rescaled shape:  (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "First 2 predictions org. scale:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  402.637733  213.893265  -19.009796  1038.791580   \n",
      "2011-01-01 01:00:00  248.320404  201.704857 -130.427063   443.570969   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  488.837402  308.553299  431.110252  521.260880   \n",
      "2011-01-01 01:00:00  282.777832  160.994926  349.848732  331.760773   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  444.871399  181.154922  \n",
      "2011-01-01 01:00:00  236.242210   37.323456  \n",
      "RMSE per TS 0 for model: validation_set: 77.89221429499995\n",
      "RMSE per TS 1 for model: validation_set: 97.5883525244283\n",
      "RMSE per TS 2 for model: validation_set: 91.6546326925446\n",
      "RMSE per TS 3 for model: validation_set: 92.7481226290711\n",
      "RMSE per TS 4 for model: validation_set: 75.28457789950693\n",
      "RMSE per TS 5 for model: validation_set: 88.23859126450957\n",
      "RMSE per TS 6 for model: validation_set: 71.76556312830608\n",
      "RMSE per TS 7 for model: validation_set: 69.42904153782652\n",
      "RMSE per TS 8 for model: validation_set: 77.9011359590919\n",
      "RMSE per TS 9 for model: validation_set: 87.0296728962404\n",
      "Avg.RMSE for multivariate model: validation_set: 82.95319048265253\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 41s 5ms/step\n",
      "yhat shape:  (8784, 10)\n",
      "yhat_rescaled shape:  (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "First 2 predictions org. scale:\n",
      "                            237         161        230          79  \\\n",
      "date                                                                 \n",
      "2012-01-01 00:00:00  417.712677  288.044952 -54.225891  1177.56340   \n",
      "2012-01-01 01:00:00  365.257210  239.030792 -88.928040   543.30899   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  499.798904  298.387283  514.332336  592.257935   \n",
      "2012-01-01 01:00:00  328.080353  199.693604  404.371185  401.063461   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  664.693008  242.035309  \n",
      "2012-01-01 01:00:00  361.976158  103.100189  \n",
      "RMSE per TS 0 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 79.9671439233428\n",
      "RMSE per TS 1 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 100.1101912959503\n",
      "RMSE per TS 2 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 90.92020771376004\n",
      "RMSE per TS 3 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 93.79590007987815\n",
      "RMSE per TS 4 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 76.80602265153124\n",
      "RMSE per TS 5 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 91.41493600992774\n",
      "RMSE per TS 6 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 70.52063962934453\n",
      "RMSE per TS 7 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 74.14232585077845\n",
      "RMSE per TS 8 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 83.73337819110515\n",
      "RMSE per TS 9 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 89.10699451552652\n",
      "Avg.RMSE for multivariate model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012: 85.05177398611448\n",
      "######## START Experiment multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928, 10)\n",
      "complete test_set shape:  (8952, 10)\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Train_data shape:  (17519, 10)\n",
      "valid_data shape:  (8928, 10)\n",
      "Test_data shape:  (8952, 10)\n",
      "X_train shape for autoencoder:  (17351, 168, 10)\n",
      "X_valid shape for autoencoder:  (8760, 168, 10)\n",
      "X_test shape for autoencoder:  (8784, 168, 10)\n",
      "y_train shape before model creation:  (17351, 10)\n",
      "y_valid shape before model creation:  (8760, 10)\n",
      "y_test shape before model creation:  (8784, 10)\n",
      "create stacked autoencoder 2 layer:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 26s 1ms/step - loss: 1.0058 - val_loss: 1.1437\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9714 - val_loss: 1.1108\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9199 - val_loss: 1.0947\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8389 - val_loss: 0.8608\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8058 - val_loss: 0.9892\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7584 - val_loss: 0.7645\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7095 - val_loss: 0.8480\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6861 - val_loss: 0.7500\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6705 - val_loss: 0.7791\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6515 - val_loss: 0.6817\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6480 - val_loss: 0.7710\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6328 - val_loss: 0.6633\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6215 - val_loss: 0.7282\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6224 - val_loss: 0.6993\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6025 - val_loss: 0.6328\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5968 - val_loss: 0.7061\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5974 - val_loss: 0.6282\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5919 - val_loss: 0.7281\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5802 - val_loss: 0.7364\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5787 - val_loss: 0.6206\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5689 - val_loss: 0.6049\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5698 - val_loss: 0.5964\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5571 - val_loss: 0.6280\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5711 - val_loss: 0.7603\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5531 - val_loss: 0.5764\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5570 - val_loss: 0.6683\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5500 - val_loss: 0.6380\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5443 - val_loss: 0.5631\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5410 - val_loss: 0.5666\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5445 - val_loss: 0.5666\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5437 - val_loss: 0.5555\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.4997 - val_loss: 0.5476\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5982 - val_loss: 0.6195\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5619 - val_loss: 0.6993\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5123 - val_loss: 0.5462\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5634 - val_loss: 0.6295\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5128 - val_loss: 0.6944\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5199 - val_loss: 0.5542\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5165 - val_loss: 0.5549\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5074 - val_loss: 0.5336\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5134 - val_loss: 0.5500\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5591 - val_loss: 0.7216\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5994 - val_loss: 0.6012\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7406 - val_loss: 0.6618\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6685 - val_loss: 1.0081\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7706 - val_loss: 0.6987\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6675 - val_loss: 0.6822\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6054 - val_loss: 0.6104\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 22s 1ms/step - loss: 0.6169 - val_loss: 0.6559\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5754 - val_loss: 0.8126\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6310 - val_loss: 0.6160\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6007 - val_loss: 0.9276\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7148 - val_loss: 0.6600\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.5882 - val_loss: 0.6706\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7106 - val_loss: 1.1556\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8953 - val_loss: 0.8756\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7590 - val_loss: 0.8207\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6908 - val_loss: 0.7370\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6415 - val_loss: 0.6773\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6138 - val_loss: 0.6578\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6800 - val_loss: 1.3819\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0655 - val_loss: 1.1583\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0102 - val_loss: 1.1524\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0076 - val_loss: 1.1508\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0068 - val_loss: 1.1498\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0062 - val_loss: 1.1493\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0057 - val_loss: 1.1489\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0054 - val_loss: 1.1489\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0051 - val_loss: 1.1482\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0046 - val_loss: 1.1480\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0044 - val_loss: 1.1480\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0041 - val_loss: 1.1469\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0035 - val_loss: 1.1463\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0027 - val_loss: 1.1452\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0019 - val_loss: 1.1439\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0008 - val_loss: 1.1428\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9999 - val_loss: 1.1413\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9990 - val_loss: 1.1404\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9980 - val_loss: 1.1393\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9971 - val_loss: 1.1382\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9962 - val_loss: 1.1373\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9951 - val_loss: 1.1358\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9935 - val_loss: 1.1334\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9908 - val_loss: 1.1272\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9840 - val_loss: 1.1144\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9690 - val_loss: 1.0875\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9466 - val_loss: 1.0544\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9127 - val_loss: 1.0570\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8774 - val_loss: 0.9079\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7802 - val_loss: 0.8568\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7629 - val_loss: 0.7966\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8137 - val_loss: 0.8362\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7359 - val_loss: 0.8543\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7205 - val_loss: 0.7763\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7323 - val_loss: 0.9966\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7366 - val_loss: 0.7446\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6887 - val_loss: 0.7114\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7356 - val_loss: 0.9281\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7199 - val_loss: 0.8421\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8457 - val_loss: 0.8387\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6888 - val_loss: 0.6914\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8189 - val_loss: 1.1332\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9255 - val_loss: 0.8788\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7360 - val_loss: 0.7616\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7785 - val_loss: 1.0795\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8531 - val_loss: 0.8241\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7263 - val_loss: 0.7533\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6880 - val_loss: 0.7162\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9777 - val_loss: 1.1722\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0140 - val_loss: 1.1524\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0082 - val_loss: 1.1513\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0074 - val_loss: 1.1510\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0070 - val_loss: 1.1502\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0064 - val_loss: 1.1481\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0067 - val_loss: 1.1501\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0059 - val_loss: 1.1489\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0047 - val_loss: 1.1451\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0035 - val_loss: 1.1432\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 1.0009 - val_loss: 1.1348\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9944 - val_loss: 1.1187\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9829 - val_loss: 1.0970\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9689 - val_loss: 1.0690\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9507 - val_loss: 1.0364\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9307 - val_loss: 1.0139\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9174 - val_loss: 0.9938\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9124 - val_loss: 0.9765\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8944 - val_loss: 0.9559\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8729 - val_loss: 0.9367\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8739 - val_loss: 1.0599\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8615 - val_loss: 0.9122\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8766 - val_loss: 0.9161\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8416 - val_loss: 0.8974\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8718 - val_loss: 1.0778\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8457 - val_loss: 0.8725\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8082 - val_loss: 0.8695\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8081 - val_loss: 0.8933\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8505 - val_loss: 0.8785\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8008 - val_loss: 0.8580\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7874 - val_loss: 0.8381\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8093 - val_loss: 1.0077\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8070 - val_loss: 0.7640\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7483 - val_loss: 0.8349\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7621 - val_loss: 0.7604\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.6896 - val_loss: 0.7166\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7135 - val_loss: 0.7584\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.7060 - val_loss: 0.7286\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8781 - val_loss: 1.0826\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.9130 - val_loss: 0.9329\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8404 - val_loss: 0.8901\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 23s 1ms/step - loss: 0.8722 - val_loss: 0.9727\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 128)\n",
      "X_valid_encoded shape:  (8760, 168, 128)\n",
      "X_test_encoded shape:  (8784, 168, 128)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.6930 - mean_absolute_error: 0.6084 - val_loss: 0.6071 - val_mean_absolute_error: 0.5696\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.5242 - mean_absolute_error: 0.5299 - val_loss: 0.5024 - val_mean_absolute_error: 0.5165\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.4739 - mean_absolute_error: 0.5005 - val_loss: 0.4719 - val_mean_absolute_error: 0.4968\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.4429 - mean_absolute_error: 0.4815 - val_loss: 0.4448 - val_mean_absolute_error: 0.4812\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.4267 - mean_absolute_error: 0.4701 - val_loss: 0.4314 - val_mean_absolute_error: 0.4735\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.4134 - mean_absolute_error: 0.4608 - val_loss: 0.4159 - val_mean_absolute_error: 0.4575\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.4006 - mean_absolute_error: 0.4518 - val_loss: 0.4142 - val_mean_absolute_error: 0.4624\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.4019 - mean_absolute_error: 0.4535 - val_loss: 0.4070 - val_mean_absolute_error: 0.4543\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.3960 - mean_absolute_error: 0.4481 - val_loss: 0.3994 - val_mean_absolute_error: 0.4495\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3877 - mean_absolute_error: 0.4423 - val_loss: 0.3836 - val_mean_absolute_error: 0.4397\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.3851 - mean_absolute_error: 0.4402 - val_loss: 0.4103 - val_mean_absolute_error: 0.4595\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3793 - mean_absolute_error: 0.4373 - val_loss: 0.3751 - val_mean_absolute_error: 0.4342\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 15s 891us/step - loss: 0.3706 - mean_absolute_error: 0.4310 - val_loss: 0.3791 - val_mean_absolute_error: 0.4367\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3678 - mean_absolute_error: 0.4295 - val_loss: 0.3766 - val_mean_absolute_error: 0.4337\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3673 - mean_absolute_error: 0.4281 - val_loss: 0.3789 - val_mean_absolute_error: 0.4357\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3714 - mean_absolute_error: 0.4308 - val_loss: 0.3860 - val_mean_absolute_error: 0.4437\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.3647 - mean_absolute_error: 0.4270 - val_loss: 0.3705 - val_mean_absolute_error: 0.4315\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3604 - mean_absolute_error: 0.4233 - val_loss: 0.3696 - val_mean_absolute_error: 0.4283\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3675 - mean_absolute_error: 0.4273 - val_loss: 0.4326 - val_mean_absolute_error: 0.4623\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 15s 887us/step - loss: 0.3697 - mean_absolute_error: 0.4301 - val_loss: 0.3646 - val_mean_absolute_error: 0.4270\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3533 - mean_absolute_error: 0.4188 - val_loss: 0.3608 - val_mean_absolute_error: 0.4224\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3532 - mean_absolute_error: 0.4180 - val_loss: 0.3605 - val_mean_absolute_error: 0.4265\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3509 - mean_absolute_error: 0.4169 - val_loss: 0.3602 - val_mean_absolute_error: 0.4216\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3552 - mean_absolute_error: 0.4201 - val_loss: 0.3593 - val_mean_absolute_error: 0.4222\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.3597 - mean_absolute_error: 0.4231 - val_loss: 0.3651 - val_mean_absolute_error: 0.4269\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.3566 - mean_absolute_error: 0.4215 - val_loss: 0.3625 - val_mean_absolute_error: 0.4257\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3485 - mean_absolute_error: 0.4156 - val_loss: 0.3622 - val_mean_absolute_error: 0.4252\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3459 - mean_absolute_error: 0.4132 - val_loss: 0.3499 - val_mean_absolute_error: 0.4162\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.3449 - mean_absolute_error: 0.4123 - val_loss: 0.3531 - val_mean_absolute_error: 0.4192\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.3414 - mean_absolute_error: 0.4104 - val_loss: 0.3848 - val_mean_absolute_error: 0.4354\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3617 - mean_absolute_error: 0.4233 - val_loss: 0.3681 - val_mean_absolute_error: 0.4289\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.3453 - mean_absolute_error: 0.4125 - val_loss: 0.3528 - val_mean_absolute_error: 0.4178\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3650 - mean_absolute_error: 0.4255 - val_loss: 0.3824 - val_mean_absolute_error: 0.4379\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3578 - mean_absolute_error: 0.4234 - val_loss: 0.3573 - val_mean_absolute_error: 0.4229\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 16s 911us/step - loss: 0.3494 - mean_absolute_error: 0.4165 - val_loss: 0.3567 - val_mean_absolute_error: 0.4215\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3442 - mean_absolute_error: 0.4132 - val_loss: 0.3513 - val_mean_absolute_error: 0.4187\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3526 - mean_absolute_error: 0.4192 - val_loss: 0.3546 - val_mean_absolute_error: 0.4199\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.3409 - mean_absolute_error: 0.4113 - val_loss: 0.3459 - val_mean_absolute_error: 0.4130\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.3432 - mean_absolute_error: 0.4111 - val_loss: 0.3493 - val_mean_absolute_error: 0.4146\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3395 - mean_absolute_error: 0.4089 - val_loss: 0.3444 - val_mean_absolute_error: 0.4124\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3372 - mean_absolute_error: 0.4077 - val_loss: 0.3436 - val_mean_absolute_error: 0.4130\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3393 - mean_absolute_error: 0.4086 - val_loss: 0.3469 - val_mean_absolute_error: 0.4115\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.3358 - mean_absolute_error: 0.4058 - val_loss: 0.3491 - val_mean_absolute_error: 0.4157\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3330 - mean_absolute_error: 0.4046 - val_loss: 0.3436 - val_mean_absolute_error: 0.4106\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3323 - mean_absolute_error: 0.4039 - val_loss: 0.3544 - val_mean_absolute_error: 0.4145\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3333 - mean_absolute_error: 0.4041 - val_loss: 0.3396 - val_mean_absolute_error: 0.4084\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3329 - mean_absolute_error: 0.4042 - val_loss: 0.3502 - val_mean_absolute_error: 0.4133\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3335 - mean_absolute_error: 0.4044 - val_loss: 0.3452 - val_mean_absolute_error: 0.4107\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3313 - mean_absolute_error: 0.4032 - val_loss: 0.3362 - val_mean_absolute_error: 0.4061\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3281 - mean_absolute_error: 0.4004 - val_loss: 0.3436 - val_mean_absolute_error: 0.4100\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3281 - mean_absolute_error: 0.4009 - val_loss: 0.3410 - val_mean_absolute_error: 0.4098\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3275 - mean_absolute_error: 0.4004 - val_loss: 0.3401 - val_mean_absolute_error: 0.4091\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 15s 882us/step - loss: 0.3276 - mean_absolute_error: 0.4009 - val_loss: 0.3367 - val_mean_absolute_error: 0.4070\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3291 - mean_absolute_error: 0.4012 - val_loss: 0.3476 - val_mean_absolute_error: 0.4125\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3297 - mean_absolute_error: 0.4022 - val_loss: 0.3445 - val_mean_absolute_error: 0.4104\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3261 - mean_absolute_error: 0.3998 - val_loss: 0.3412 - val_mean_absolute_error: 0.4077\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3277 - mean_absolute_error: 0.4002 - val_loss: 0.3343 - val_mean_absolute_error: 0.4052\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3248 - mean_absolute_error: 0.3990 - val_loss: 0.3327 - val_mean_absolute_error: 0.4034\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3279 - mean_absolute_error: 0.3997 - val_loss: 0.3421 - val_mean_absolute_error: 0.4077\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3293 - mean_absolute_error: 0.4021 - val_loss: 0.3377 - val_mean_absolute_error: 0.4072\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3250 - mean_absolute_error: 0.3989 - val_loss: 0.3342 - val_mean_absolute_error: 0.4036\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3283 - mean_absolute_error: 0.4010 - val_loss: 0.3511 - val_mean_absolute_error: 0.4188\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3345 - mean_absolute_error: 0.4064 - val_loss: 0.3981 - val_mean_absolute_error: 0.4474\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3533 - mean_absolute_error: 0.4197 - val_loss: 0.3516 - val_mean_absolute_error: 0.4191\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.3329 - mean_absolute_error: 0.4053 - val_loss: 0.3354 - val_mean_absolute_error: 0.4079\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3295 - mean_absolute_error: 0.4022 - val_loss: 0.3395 - val_mean_absolute_error: 0.4073\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.3281 - mean_absolute_error: 0.4010 - val_loss: 0.3409 - val_mean_absolute_error: 0.4090\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 15s 890us/step - loss: 0.3242 - mean_absolute_error: 0.3982 - val_loss: 0.3312 - val_mean_absolute_error: 0.4026\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.3220 - mean_absolute_error: 0.3970 - val_loss: 0.3339 - val_mean_absolute_error: 0.4043\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3253 - mean_absolute_error: 0.3988 - val_loss: 0.3332 - val_mean_absolute_error: 0.4060\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3316 - mean_absolute_error: 0.4037 - val_loss: 0.3356 - val_mean_absolute_error: 0.4068\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3223 - mean_absolute_error: 0.3967 - val_loss: 0.3322 - val_mean_absolute_error: 0.4048\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.3271 - mean_absolute_error: 0.4002 - val_loss: 0.3327 - val_mean_absolute_error: 0.4050\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3211 - mean_absolute_error: 0.3960 - val_loss: 0.3293 - val_mean_absolute_error: 0.4014\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3223 - mean_absolute_error: 0.3967 - val_loss: 0.3347 - val_mean_absolute_error: 0.4057\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3203 - mean_absolute_error: 0.3953 - val_loss: 0.3506 - val_mean_absolute_error: 0.4142\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3460 - mean_absolute_error: 0.4108 - val_loss: 0.3929 - val_mean_absolute_error: 0.4379\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3431 - mean_absolute_error: 0.4105 - val_loss: 0.3449 - val_mean_absolute_error: 0.4121\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3278 - mean_absolute_error: 0.3998 - val_loss: 0.3366 - val_mean_absolute_error: 0.4066\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3293 - mean_absolute_error: 0.4013 - val_loss: 0.3402 - val_mean_absolute_error: 0.4098\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3305 - mean_absolute_error: 0.4016 - val_loss: 0.3379 - val_mean_absolute_error: 0.4072\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 16s 912us/step - loss: 0.3211 - mean_absolute_error: 0.3962 - val_loss: 0.3355 - val_mean_absolute_error: 0.4038\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.3228 - mean_absolute_error: 0.3970 - val_loss: 0.3313 - val_mean_absolute_error: 0.4028\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3204 - mean_absolute_error: 0.3951 - val_loss: 0.3331 - val_mean_absolute_error: 0.4035\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3192 - mean_absolute_error: 0.3947 - val_loss: 0.3276 - val_mean_absolute_error: 0.4008\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3186 - mean_absolute_error: 0.3935 - val_loss: 0.3288 - val_mean_absolute_error: 0.4003\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3187 - mean_absolute_error: 0.3939 - val_loss: 0.3341 - val_mean_absolute_error: 0.4033\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3245 - mean_absolute_error: 0.3980 - val_loss: 0.3360 - val_mean_absolute_error: 0.4055\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3395 - mean_absolute_error: 0.4051 - val_loss: 0.3629 - val_mean_absolute_error: 0.4211\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3338 - mean_absolute_error: 0.4048 - val_loss: 0.3400 - val_mean_absolute_error: 0.4078\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3220 - mean_absolute_error: 0.3966 - val_loss: 0.3297 - val_mean_absolute_error: 0.4021\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3226 - mean_absolute_error: 0.3965 - val_loss: 0.3331 - val_mean_absolute_error: 0.4046\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3178 - mean_absolute_error: 0.3934 - val_loss: 0.3260 - val_mean_absolute_error: 0.3989\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 16s 899us/step - loss: 0.3180 - mean_absolute_error: 0.3932 - val_loss: 0.3283 - val_mean_absolute_error: 0.4004\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3155 - mean_absolute_error: 0.3918 - val_loss: 0.3256 - val_mean_absolute_error: 0.3978\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3180 - mean_absolute_error: 0.3932 - val_loss: 0.3325 - val_mean_absolute_error: 0.4035\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3163 - mean_absolute_error: 0.3922 - val_loss: 0.3261 - val_mean_absolute_error: 0.3983\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3157 - mean_absolute_error: 0.3916 - val_loss: 0.3369 - val_mean_absolute_error: 0.4059\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 15s 888us/step - loss: 0.3165 - mean_absolute_error: 0.3929 - val_loss: 0.3255 - val_mean_absolute_error: 0.3984\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.3157 - mean_absolute_error: 0.3915 - val_loss: 0.3315 - val_mean_absolute_error: 0.4025\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3171 - mean_absolute_error: 0.3932 - val_loss: 0.3297 - val_mean_absolute_error: 0.3984\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3166 - mean_absolute_error: 0.3922 - val_loss: 0.3335 - val_mean_absolute_error: 0.4008\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 16s 896us/step - loss: 0.3160 - mean_absolute_error: 0.3918 - val_loss: 0.3245 - val_mean_absolute_error: 0.3982\n",
      "Epoch 104/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3147 - mean_absolute_error: 0.3911 - val_loss: 0.3241 - val_mean_absolute_error: 0.3980\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3136 - mean_absolute_error: 0.3897 - val_loss: 0.3235 - val_mean_absolute_error: 0.3984\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3214 - mean_absolute_error: 0.3953 - val_loss: 0.3309 - val_mean_absolute_error: 0.4018\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 16s 895us/step - loss: 0.3163 - mean_absolute_error: 0.3927 - val_loss: 0.3308 - val_mean_absolute_error: 0.4020\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3161 - mean_absolute_error: 0.3917 - val_loss: 0.3304 - val_mean_absolute_error: 0.4014\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 16s 897us/step - loss: 0.3252 - mean_absolute_error: 0.3982 - val_loss: 0.3353 - val_mean_absolute_error: 0.4053\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 15s 890us/step - loss: 0.3182 - mean_absolute_error: 0.3935 - val_loss: 0.3243 - val_mean_absolute_error: 0.3972\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3135 - mean_absolute_error: 0.3901 - val_loss: 0.3317 - val_mean_absolute_error: 0.4001\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3179 - mean_absolute_error: 0.3930 - val_loss: 0.3311 - val_mean_absolute_error: 0.4013\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3166 - mean_absolute_error: 0.3927 - val_loss: 0.3225 - val_mean_absolute_error: 0.3975\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3120 - mean_absolute_error: 0.3897 - val_loss: 0.3268 - val_mean_absolute_error: 0.3990\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.3113 - mean_absolute_error: 0.3890 - val_loss: 0.3241 - val_mean_absolute_error: 0.3977\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3097 - mean_absolute_error: 0.3887 - val_loss: 0.3270 - val_mean_absolute_error: 0.4001\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3112 - mean_absolute_error: 0.3886 - val_loss: 0.3249 - val_mean_absolute_error: 0.3980\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3117 - mean_absolute_error: 0.3886 - val_loss: 0.3197 - val_mean_absolute_error: 0.3952\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 16s 914us/step - loss: 0.3127 - mean_absolute_error: 0.3903 - val_loss: 0.3322 - val_mean_absolute_error: 0.4046\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3135 - mean_absolute_error: 0.3905 - val_loss: 0.3247 - val_mean_absolute_error: 0.3978\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.3139 - mean_absolute_error: 0.3903 - val_loss: 0.3246 - val_mean_absolute_error: 0.3973\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3148 - mean_absolute_error: 0.3902 - val_loss: 0.3274 - val_mean_absolute_error: 0.4001\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.3140 - mean_absolute_error: 0.3907 - val_loss: 0.3269 - val_mean_absolute_error: 0.3975\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 16s 900us/step - loss: 0.3131 - mean_absolute_error: 0.3897 - val_loss: 0.3315 - val_mean_absolute_error: 0.4017\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3143 - mean_absolute_error: 0.3910 - val_loss: 0.3223 - val_mean_absolute_error: 0.3957\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 16s 902us/step - loss: 0.3108 - mean_absolute_error: 0.3879 - val_loss: 0.3226 - val_mean_absolute_error: 0.3978\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3092 - mean_absolute_error: 0.3868 - val_loss: 0.3209 - val_mean_absolute_error: 0.3966\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3067 - mean_absolute_error: 0.3864 - val_loss: 0.3233 - val_mean_absolute_error: 0.3944\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3087 - mean_absolute_error: 0.3870 - val_loss: 0.3206 - val_mean_absolute_error: 0.3951\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3087 - mean_absolute_error: 0.3871 - val_loss: 0.3200 - val_mean_absolute_error: 0.3935\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 15s 893us/step - loss: 0.3083 - mean_absolute_error: 0.3863 - val_loss: 0.3241 - val_mean_absolute_error: 0.3973\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3087 - mean_absolute_error: 0.3873 - val_loss: 0.3236 - val_mean_absolute_error: 0.3967\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.3123 - mean_absolute_error: 0.3898 - val_loss: 0.3247 - val_mean_absolute_error: 0.3989\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 16s 901us/step - loss: 0.3109 - mean_absolute_error: 0.3888 - val_loss: 0.3341 - val_mean_absolute_error: 0.4042\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 16s 916us/step - loss: 0.3141 - mean_absolute_error: 0.3913 - val_loss: 0.3294 - val_mean_absolute_error: 0.4021\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3143 - mean_absolute_error: 0.3910 - val_loss: 0.3303 - val_mean_absolute_error: 0.4008\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 16s 894us/step - loss: 0.3072 - mean_absolute_error: 0.3861 - val_loss: 0.3207 - val_mean_absolute_error: 0.3941\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 16s 903us/step - loss: 0.3087 - mean_absolute_error: 0.3869 - val_loss: 0.3177 - val_mean_absolute_error: 0.3931\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 15s 890us/step - loss: 0.3045 - mean_absolute_error: 0.3843 - val_loss: 0.3219 - val_mean_absolute_error: 0.3963\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 16s 913us/step - loss: 0.3067 - mean_absolute_error: 0.3860 - val_loss: 0.3243 - val_mean_absolute_error: 0.3961\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 16s 905us/step - loss: 0.3099 - mean_absolute_error: 0.3878 - val_loss: 0.3234 - val_mean_absolute_error: 0.3975\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 16s 909us/step - loss: 0.3080 - mean_absolute_error: 0.3871 - val_loss: 0.3220 - val_mean_absolute_error: 0.3947\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 15s 892us/step - loss: 0.3062 - mean_absolute_error: 0.3855 - val_loss: 0.3199 - val_mean_absolute_error: 0.3940\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 16s 908us/step - loss: 0.3048 - mean_absolute_error: 0.3850 - val_loss: 0.3173 - val_mean_absolute_error: 0.3929\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.3052 - mean_absolute_error: 0.3839 - val_loss: 0.3157 - val_mean_absolute_error: 0.3924\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 16s 906us/step - loss: 0.3045 - mean_absolute_error: 0.3844 - val_loss: 0.3159 - val_mean_absolute_error: 0.3930\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 16s 898us/step - loss: 0.3043 - mean_absolute_error: 0.3841 - val_loss: 0.3181 - val_mean_absolute_error: 0.3939\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 16s 907us/step - loss: 0.3587 - mean_absolute_error: 0.4253 - val_loss: 0.3479 - val_mean_absolute_error: 0.4190\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 16s 904us/step - loss: 0.3232 - mean_absolute_error: 0.3980 - val_loss: 0.3288 - val_mean_absolute_error: 0.4017\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 16s 910us/step - loss: 0.3178 - mean_absolute_error: 0.3947 - val_loss: 0.3244 - val_mean_absolute_error: 0.3982\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 42s 5ms/step\n",
      "yhat shape:  (8760, 10)\n",
      "yhat_rescaled shape:  (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "First 2 predictions org. scale:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  356.368301  141.617645  -34.158081  1063.358170   \n",
      "2011-01-01 01:00:00  204.590385  131.308136 -183.108093   437.619431   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  454.102890  262.626129  387.894623  494.034683   \n",
      "2011-01-01 01:00:00  253.097107  106.760056  300.261414  291.627945   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  409.319458  199.556046  \n",
      "2011-01-01 01:00:00  179.644409   19.509109  \n",
      "RMSE per TS 0 for model: validation_set: 86.9812505489147\n",
      "RMSE per TS 1 for model: validation_set: 107.82555076860355\n",
      "RMSE per TS 2 for model: validation_set: 102.12372393042715\n",
      "RMSE per TS 3 for model: validation_set: 107.90473444325139\n",
      "RMSE per TS 4 for model: validation_set: 81.3555277328073\n",
      "RMSE per TS 5 for model: validation_set: 99.79363279366444\n",
      "RMSE per TS 6 for model: validation_set: 78.08462818519709\n",
      "RMSE per TS 7 for model: validation_set: 77.44127879451713\n",
      "RMSE per TS 8 for model: validation_set: 85.59470137397828\n",
      "RMSE per TS 9 for model: validation_set: 92.17026650607681\n",
      "Avg.RMSE for multivariate model: validation_set: 91.9275295077438\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 41s 5ms/step\n",
      "yhat shape:  (8784, 10)\n",
      "yhat_rescaled shape:  (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "First 2 predictions org. scale:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  394.176147  256.683578  -49.814346  1201.139488   \n",
      "2012-01-01 01:00:00  335.551804  198.762131 -116.140274   548.493454   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  478.174141  282.725922  484.258179  583.318954   \n",
      "2012-01-01 01:00:00  304.296310  166.143143  368.650955  376.903290   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  631.127533  252.931351  \n",
      "2012-01-01 01:00:00  316.241180   86.490128  \n",
      "RMSE per TS 0 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 89.23194134468459\n",
      "RMSE per TS 1 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 109.66632422775012\n",
      "RMSE per TS 2 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 100.37321277949785\n",
      "RMSE per TS 3 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 109.8405706815701\n",
      "RMSE per TS 4 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 83.37289538966361\n",
      "RMSE per TS 5 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 103.29030278033632\n",
      "RMSE per TS 6 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 77.00434689333008\n",
      "RMSE per TS 7 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 82.52346234406389\n",
      "RMSE per TS 8 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 90.8078685715964\n",
      "RMSE per TS 9 for model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 95.74777973761627\n",
      "Avg.RMSE for multivariate model: multivariate_encoder_2H_256_128_1D_128_predict_model_2H_128_64_batch512_drop03_scaling_std_W168_10largest_areas__y2012: 94.18587047501093\n",
      "######## START Experiment encoder_2H_512_256_1D_256_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_area237__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928,)\n",
      "complete test_set shape:  (8952,)\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Train_data shape:  (17519, 1)\n",
      "valid_data shape:  (8928, 1)\n",
      "Test_data shape:  (8952, 1)\n",
      "X_train shape for autoencoder:  (17351, 168, 1)\n",
      "X_valid shape for autoencoder:  (8760, 168, 1)\n",
      "X_test shape for autoencoder:  (8784, 168, 1)\n",
      "y_train shape before model creation:  (17351, 1)\n",
      "y_valid shape before model creation:  (8760, 1)\n",
      "y_test shape before model creation:  (8784, 1)\n",
      "create stacked autoencoder 2 layer:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 48s 3ms/step - loss: 0.0441 - val_loss: 0.0504\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0505\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0512\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0424 - val_loss: 0.0504\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0499\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0467 - val_loss: 0.0505\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0420 - val_loss: 0.0490\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0420 - val_loss: 0.0504\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 29/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0421 - val_loss: 0.0502\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0419 - val_loss: 0.0497\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0418 - val_loss: 0.0504\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0421 - val_loss: 0.0502\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0419 - val_loss: 0.0495\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0652 - val_loss: 0.0511\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0426 - val_loss: 0.0505\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0502\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0502\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0500\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0420 - val_loss: 0.0499\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0419 - val_loss: 0.0496\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0417 - val_loss: 0.0493\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0417 - val_loss: 0.0504\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0504\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0503\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0421 - val_loss: 0.0502\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0421 - val_loss: 0.0501\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0420 - val_loss: 0.0499\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0419 - val_loss: 0.0497\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0418 - val_loss: 0.0505\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0424 - val_loss: 0.0504\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0422 - val_loss: 0.0502\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0420 - val_loss: 0.0496\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0847 - val_loss: 0.0567\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0433 - val_loss: 0.0505\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 43s 2ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 43s 3ms/step - loss: 0.0423 - val_loss: 0.0504\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 256)\n",
      "X_valid_encoded shape:  (8760, 168, 256)\n",
      "X_test_encoded shape:  (8784, 168, 256)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 20s 1ms/step - loss: 0.0591 - mean_absolute_error: 0.1827 - val_loss: 0.0506 - val_mean_absolute_error: 0.1678\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 16s 947us/step - loss: 0.0435 - mean_absolute_error: 0.1535 - val_loss: 0.0503 - val_mean_absolute_error: 0.1627\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 16s 944us/step - loss: 0.0429 - mean_absolute_error: 0.1520 - val_loss: 0.0500 - val_mean_absolute_error: 0.1629\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0425 - mean_absolute_error: 0.1515 - val_loss: 0.0498 - val_mean_absolute_error: 0.1625\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 17s 954us/step - loss: 0.0425 - mean_absolute_error: 0.1512 - val_loss: 0.0496 - val_mean_absolute_error: 0.1645\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0421 - mean_absolute_error: 0.1507 - val_loss: 0.0495 - val_mean_absolute_error: 0.1620\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 16s 940us/step - loss: 0.0419 - mean_absolute_error: 0.1501 - val_loss: 0.0489 - val_mean_absolute_error: 0.1631\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0419 - mean_absolute_error: 0.1501 - val_loss: 0.0502 - val_mean_absolute_error: 0.1610\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0416 - mean_absolute_error: 0.1494 - val_loss: 0.0472 - val_mean_absolute_error: 0.1568\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 17s 951us/step - loss: 0.0409 - mean_absolute_error: 0.1482 - val_loss: 0.0460 - val_mean_absolute_error: 0.1576\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0398 - mean_absolute_error: 0.1468 - val_loss: 0.0437 - val_mean_absolute_error: 0.1528\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0393 - mean_absolute_error: 0.1462 - val_loss: 0.0432 - val_mean_absolute_error: 0.1518\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 17s 954us/step - loss: 0.0383 - mean_absolute_error: 0.1442 - val_loss: 0.0357 - val_mean_absolute_error: 0.1307\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0377 - mean_absolute_error: 0.1429 - val_loss: 0.0379 - val_mean_absolute_error: 0.1456\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0368 - mean_absolute_error: 0.1411 - val_loss: 0.0343 - val_mean_absolute_error: 0.1269\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 16s 947us/step - loss: 0.0363 - mean_absolute_error: 0.1401 - val_loss: 0.0368 - val_mean_absolute_error: 0.1315\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0349 - mean_absolute_error: 0.1374 - val_loss: 0.0336 - val_mean_absolute_error: 0.1333\n",
      "Epoch 18/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0354 - mean_absolute_error: 0.1382 - val_loss: 0.0329 - val_mean_absolute_error: 0.1375\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0339 - mean_absolute_error: 0.1359 - val_loss: 0.0388 - val_mean_absolute_error: 0.1531\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0357 - mean_absolute_error: 0.1389 - val_loss: 0.0346 - val_mean_absolute_error: 0.1305\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 17s 951us/step - loss: 0.0340 - mean_absolute_error: 0.1355 - val_loss: 0.0345 - val_mean_absolute_error: 0.1389\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0327 - mean_absolute_error: 0.1336 - val_loss: 0.0293 - val_mean_absolute_error: 0.1174\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 17s 965us/step - loss: 0.0334 - mean_absolute_error: 0.1340 - val_loss: 0.0303 - val_mean_absolute_error: 0.1201\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0319 - mean_absolute_error: 0.1316 - val_loss: 0.0278 - val_mean_absolute_error: 0.1179\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0314 - mean_absolute_error: 0.1301 - val_loss: 0.0295 - val_mean_absolute_error: 0.1221\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0317 - mean_absolute_error: 0.1303 - val_loss: 0.0281 - val_mean_absolute_error: 0.1137\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0313 - mean_absolute_error: 0.1300 - val_loss: 0.0262 - val_mean_absolute_error: 0.1164\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0314 - mean_absolute_error: 0.1302 - val_loss: 0.0296 - val_mean_absolute_error: 0.1305\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0314 - mean_absolute_error: 0.1303 - val_loss: 0.0339 - val_mean_absolute_error: 0.1287\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0314 - mean_absolute_error: 0.1301 - val_loss: 0.0285 - val_mean_absolute_error: 0.1151\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 16s 946us/step - loss: 0.0304 - mean_absolute_error: 0.1283 - val_loss: 0.0299 - val_mean_absolute_error: 0.1174\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0305 - mean_absolute_error: 0.1275 - val_loss: 0.0301 - val_mean_absolute_error: 0.1245\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0299 - mean_absolute_error: 0.1267 - val_loss: 0.0362 - val_mean_absolute_error: 0.1357\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0311 - mean_absolute_error: 0.1294 - val_loss: 0.0287 - val_mean_absolute_error: 0.1208\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0296 - mean_absolute_error: 0.1260 - val_loss: 0.0292 - val_mean_absolute_error: 0.1289\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 17s 966us/step - loss: 0.0305 - mean_absolute_error: 0.1283 - val_loss: 0.0288 - val_mean_absolute_error: 0.1312\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0291 - mean_absolute_error: 0.1250 - val_loss: 0.0254 - val_mean_absolute_error: 0.1191\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0293 - mean_absolute_error: 0.1251 - val_loss: 0.0234 - val_mean_absolute_error: 0.1035\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 17s 968us/step - loss: 0.0286 - mean_absolute_error: 0.1243 - val_loss: 0.0312 - val_mean_absolute_error: 0.1216\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0286 - mean_absolute_error: 0.1237 - val_loss: 0.0281 - val_mean_absolute_error: 0.1139\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 17s 969us/step - loss: 0.0286 - mean_absolute_error: 0.1239 - val_loss: 0.0232 - val_mean_absolute_error: 0.1110\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0291 - mean_absolute_error: 0.1252 - val_loss: 0.0287 - val_mean_absolute_error: 0.1167\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0295 - mean_absolute_error: 0.1259 - val_loss: 0.0302 - val_mean_absolute_error: 0.1191\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 17s 969us/step - loss: 0.0296 - mean_absolute_error: 0.1259 - val_loss: 0.0288 - val_mean_absolute_error: 0.1144\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0279 - mean_absolute_error: 0.1226 - val_loss: 0.0234 - val_mean_absolute_error: 0.1048\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0280 - mean_absolute_error: 0.1228 - val_loss: 0.0242 - val_mean_absolute_error: 0.1117\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0277 - mean_absolute_error: 0.1220 - val_loss: 0.0228 - val_mean_absolute_error: 0.1101\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0285 - mean_absolute_error: 0.1240 - val_loss: 0.0287 - val_mean_absolute_error: 0.1153\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0285 - mean_absolute_error: 0.1238 - val_loss: 0.0285 - val_mean_absolute_error: 0.1147\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0283 - mean_absolute_error: 0.1233 - val_loss: 0.0287 - val_mean_absolute_error: 0.1235\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 17s 973us/step - loss: 0.0274 - mean_absolute_error: 0.1210 - val_loss: 0.0246 - val_mean_absolute_error: 0.1091\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0270 - mean_absolute_error: 0.1205 - val_loss: 0.0237 - val_mean_absolute_error: 0.1087\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0276 - mean_absolute_error: 0.1216 - val_loss: 0.0311 - val_mean_absolute_error: 0.1227\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 17s 951us/step - loss: 0.0287 - mean_absolute_error: 0.1246 - val_loss: 0.0234 - val_mean_absolute_error: 0.1080\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0274 - mean_absolute_error: 0.1210 - val_loss: 0.0288 - val_mean_absolute_error: 0.1172\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0271 - mean_absolute_error: 0.1208 - val_loss: 0.0246 - val_mean_absolute_error: 0.1160\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0280 - mean_absolute_error: 0.1226 - val_loss: 0.0262 - val_mean_absolute_error: 0.1125\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 17s 971us/step - loss: 0.0266 - mean_absolute_error: 0.1194 - val_loss: 0.0264 - val_mean_absolute_error: 0.1110\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0264 - mean_absolute_error: 0.1185 - val_loss: 0.0211 - val_mean_absolute_error: 0.1005\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 17s 961us/step - loss: 0.0267 - mean_absolute_error: 0.1197 - val_loss: 0.0230 - val_mean_absolute_error: 0.1035\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0268 - mean_absolute_error: 0.1200 - val_loss: 0.0220 - val_mean_absolute_error: 0.1034\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0267 - mean_absolute_error: 0.1196 - val_loss: 0.0264 - val_mean_absolute_error: 0.1130\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0266 - mean_absolute_error: 0.1190 - val_loss: 0.0249 - val_mean_absolute_error: 0.1137\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0283 - mean_absolute_error: 0.1234 - val_loss: 0.0300 - val_mean_absolute_error: 0.1370\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0265 - mean_absolute_error: 0.1195 - val_loss: 0.0232 - val_mean_absolute_error: 0.1038\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0266 - mean_absolute_error: 0.1194 - val_loss: 0.0238 - val_mean_absolute_error: 0.1053\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0263 - mean_absolute_error: 0.1183 - val_loss: 0.0248 - val_mean_absolute_error: 0.1080\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 17s 965us/step - loss: 0.0264 - mean_absolute_error: 0.1187 - val_loss: 0.0230 - val_mean_absolute_error: 0.1134\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0268 - mean_absolute_error: 0.1196 - val_loss: 0.0280 - val_mean_absolute_error: 0.1276\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0279 - mean_absolute_error: 0.1231 - val_loss: 0.0329 - val_mean_absolute_error: 0.1289\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 17s 961us/step - loss: 0.0272 - mean_absolute_error: 0.1209 - val_loss: 0.0227 - val_mean_absolute_error: 0.1030\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0269 - mean_absolute_error: 0.1199 - val_loss: 0.0285 - val_mean_absolute_error: 0.1298\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0266 - mean_absolute_error: 0.1190 - val_loss: 0.0230 - val_mean_absolute_error: 0.1052\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0259 - mean_absolute_error: 0.1171 - val_loss: 0.0256 - val_mean_absolute_error: 0.1185\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0259 - mean_absolute_error: 0.1175 - val_loss: 0.0269 - val_mean_absolute_error: 0.1133\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0258 - mean_absolute_error: 0.1172 - val_loss: 0.0251 - val_mean_absolute_error: 0.1137\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 17s 955us/step - loss: 0.0255 - mean_absolute_error: 0.1168 - val_loss: 0.0230 - val_mean_absolute_error: 0.1033\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0251 - mean_absolute_error: 0.1151 - val_loss: 0.0224 - val_mean_absolute_error: 0.1024\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 17s 954us/step - loss: 0.0256 - mean_absolute_error: 0.1166 - val_loss: 0.0214 - val_mean_absolute_error: 0.1019\n",
      "Epoch 00079: early stopping\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 43s 5ms/step\n",
      "yhat shape:  (8760, 1)\n",
      "yhat_rescaled shape:  (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2011-01-01 00:00:00    476.504025\n",
      "2011-01-01 01:00:00    178.351227\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE validation_set: 132.33800460579388\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 42s 5ms/step\n",
      "yhat shape:  (8784, 1)\n",
      "yhat_rescaled shape:  (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "First 2 predictions org. scale:\n",
      "date\n",
      "2012-01-01 00:00:00    434.751396\n",
      "2012-01-01 01:00:00    379.388916\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE encoder_2H_512_256_1D_256_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_area237__y2012: 134.56155038244236\n",
      "######## START Experiment multivariate_encoder_2H_512_256_1D_256_predict_model_2H_128_64_batch512_drop03_scaling_tanh_W168_10largest_areas__y2012\n",
      "#Generate data for regular Model...\n",
      "complete valid_set shape:  (8928, 10)\n",
      "complete test_set shape:  (8952, 10)\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "Train_data shape:  (17519, 10)\n",
      "valid_data shape:  (8928, 10)\n",
      "Test_data shape:  (8952, 10)\n",
      "X_train shape for autoencoder:  (17351, 168, 10)\n",
      "X_valid shape for autoencoder:  (8760, 168, 10)\n",
      "X_test shape for autoencoder:  (8784, 168, 10)\n",
      "y_train shape before model creation:  (17351, 10)\n",
      "y_valid shape before model creation:  (8760, 10)\n",
      "y_test shape before model creation:  (8784, 10)\n",
      "create stacked autoencoder 2 layer:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 48s 3ms/step - loss: 0.0423 - val_loss: 0.0428\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0378 - val_loss: 0.0428\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0386 - val_loss: 0.0432\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0377 - val_loss: 0.0427\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0427\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0431\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0378 - val_loss: 0.0427\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0428\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 38/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0425\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0422\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0393 - val_loss: 0.0432\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0376 - val_loss: 0.0427\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0422\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0417\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0426\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0415\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0427\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0424\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0422\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0419\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0367 - val_loss: 0.0415\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0429\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0424\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0621 - val_loss: 0.0444\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0384 - val_loss: 0.0427\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0426\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0388 - val_loss: 0.0427\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0426\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0426\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0422\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0424\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0422\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0420\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0379 - val_loss: 0.0427\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0424\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0424\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0423\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0422\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0420\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0420\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0419\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0427\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0421\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0427\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0373 - val_loss: 0.0423\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0421\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0420\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0369 - val_loss: 0.0419\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0369 - val_loss: 0.0419\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0369 - val_loss: 0.0419\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0368 - val_loss: 0.0418\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0368 - val_loss: 0.0417\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0367 - val_loss: 0.0417\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0367 - val_loss: 0.0417\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0366 - val_loss: 0.0414\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0364 - val_loss: 0.0413\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0364 - val_loss: 0.0410\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0362 - val_loss: 0.0409\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0362 - val_loss: 0.0421\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0370 - val_loss: 0.0418\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0366 - val_loss: 0.0413\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0362 - val_loss: 0.0407\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0366 - val_loss: 0.0425\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0371 - val_loss: 0.0418\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0367 - val_loss: 0.0415\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0366 - val_loss: 0.0416\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0364 - val_loss: 0.0412\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0364 - val_loss: 0.0409\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0361 - val_loss: 0.0412\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0360 - val_loss: 0.0404\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0354 - val_loss: 0.0399\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0353 - val_loss: 0.0401\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0352 - val_loss: 0.0391\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0360 - val_loss: 0.0402\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0352 - val_loss: 0.0398\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0352 - val_loss: 0.0401\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0350 - val_loss: 0.0394\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0350 - val_loss: 0.0396\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0349 - val_loss: 0.0395\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0348 - val_loss: 0.0394\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0348 - val_loss: 0.0393\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0352 - val_loss: 0.0395\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0348 - val_loss: 0.0393\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0347 - val_loss: 0.0393\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0347 - val_loss: 0.0388\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0375 - val_loss: 0.0427\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0374 - val_loss: 0.0425\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0372 - val_loss: 0.0419\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0369 - val_loss: 0.0417\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0367 - val_loss: 0.0415\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 44s 3ms/step - loss: 0.0365 - val_loss: 0.0412\n",
      "create standalone encoder:\n",
      "encode data with standalone encoder:\n",
      "X_train_encoded shape:  (17351, 168, 256)\n",
      "X_valid_encoded shape:  (8760, 168, 256)\n",
      "X_test_encoded shape:  (8784, 168, 256)\n",
      "create prediction_model:\n",
      "Early Stopping applied\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 20s 1ms/step - loss: 0.0293 - mean_absolute_error: 0.1267 - val_loss: 0.0223 - val_mean_absolute_error: 0.1076\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0215 - mean_absolute_error: 0.1069 - val_loss: 0.0201 - val_mean_absolute_error: 0.1013\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 17s 967us/step - loss: 0.0197 - mean_absolute_error: 0.1018 - val_loss: 0.0193 - val_mean_absolute_error: 0.0997\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0188 - mean_absolute_error: 0.0993 - val_loss: 0.0190 - val_mean_absolute_error: 0.0990\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 17s 962us/step - loss: 0.0183 - mean_absolute_error: 0.0976 - val_loss: 0.0184 - val_mean_absolute_error: 0.0975\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0177 - mean_absolute_error: 0.0961 - val_loss: 0.0177 - val_mean_absolute_error: 0.0952\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 17s 965us/step - loss: 0.0172 - mean_absolute_error: 0.0943 - val_loss: 0.0174 - val_mean_absolute_error: 0.0944\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0169 - mean_absolute_error: 0.0933 - val_loss: 0.0171 - val_mean_absolute_error: 0.0933\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0166 - mean_absolute_error: 0.0925 - val_loss: 0.0172 - val_mean_absolute_error: 0.0938\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0164 - mean_absolute_error: 0.0916 - val_loss: 0.0171 - val_mean_absolute_error: 0.0935\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0163 - mean_absolute_error: 0.0913 - val_loss: 0.0167 - val_mean_absolute_error: 0.0920\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0160 - mean_absolute_error: 0.0905 - val_loss: 0.0161 - val_mean_absolute_error: 0.0905\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 17s 965us/step - loss: 0.0158 - mean_absolute_error: 0.0895 - val_loss: 0.0163 - val_mean_absolute_error: 0.0908\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0158 - mean_absolute_error: 0.0896 - val_loss: 0.0162 - val_mean_absolute_error: 0.0905\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0156 - mean_absolute_error: 0.0890 - val_loss: 0.0159 - val_mean_absolute_error: 0.0899\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0154 - mean_absolute_error: 0.0883 - val_loss: 0.0157 - val_mean_absolute_error: 0.0891\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 17s 961us/step - loss: 0.0153 - mean_absolute_error: 0.0879 - val_loss: 0.0157 - val_mean_absolute_error: 0.0890\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 17s 961us/step - loss: 0.0153 - mean_absolute_error: 0.0881 - val_loss: 0.0159 - val_mean_absolute_error: 0.0893\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0152 - mean_absolute_error: 0.0875 - val_loss: 0.0157 - val_mean_absolute_error: 0.0888\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0152 - mean_absolute_error: 0.0874 - val_loss: 0.0155 - val_mean_absolute_error: 0.0880\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 17s 966us/step - loss: 0.0151 - mean_absolute_error: 0.0874 - val_loss: 0.0160 - val_mean_absolute_error: 0.0894\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 16s 948us/step - loss: 0.0151 - mean_absolute_error: 0.0873 - val_loss: 0.0156 - val_mean_absolute_error: 0.0885\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 17s 961us/step - loss: 0.0147 - mean_absolute_error: 0.0860 - val_loss: 0.0158 - val_mean_absolute_error: 0.0890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0148 - mean_absolute_error: 0.0861 - val_loss: 0.0151 - val_mean_absolute_error: 0.0870\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 17s 967us/step - loss: 0.0145 - mean_absolute_error: 0.0855 - val_loss: 0.0149 - val_mean_absolute_error: 0.0863\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 17s 967us/step - loss: 0.0154 - mean_absolute_error: 0.0877 - val_loss: 0.0177 - val_mean_absolute_error: 0.0943\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 16s 951us/step - loss: 0.0166 - mean_absolute_error: 0.0915 - val_loss: 0.0159 - val_mean_absolute_error: 0.0893\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 17s 954us/step - loss: 0.0152 - mean_absolute_error: 0.0875 - val_loss: 0.0153 - val_mean_absolute_error: 0.0876\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 17s 956us/step - loss: 0.0148 - mean_absolute_error: 0.0865 - val_loss: 0.0152 - val_mean_absolute_error: 0.0873\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 17s 953us/step - loss: 0.0146 - mean_absolute_error: 0.0857 - val_loss: 0.0152 - val_mean_absolute_error: 0.0871\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0146 - mean_absolute_error: 0.0855 - val_loss: 0.0150 - val_mean_absolute_error: 0.0863\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0144 - mean_absolute_error: 0.0848 - val_loss: 0.0150 - val_mean_absolute_error: 0.0866\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0149 - mean_absolute_error: 0.0870 - val_loss: 0.0154 - val_mean_absolute_error: 0.0880\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0146 - mean_absolute_error: 0.0857 - val_loss: 0.0148 - val_mean_absolute_error: 0.0858\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 17s 963us/step - loss: 0.0144 - mean_absolute_error: 0.0851 - val_loss: 0.0146 - val_mean_absolute_error: 0.0852\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0144 - mean_absolute_error: 0.0848 - val_loss: 0.0146 - val_mean_absolute_error: 0.0853\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0142 - mean_absolute_error: 0.0843 - val_loss: 0.0145 - val_mean_absolute_error: 0.0849\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 17s 957us/step - loss: 0.0143 - mean_absolute_error: 0.0846 - val_loss: 0.0145 - val_mean_absolute_error: 0.0850\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 16s 950us/step - loss: 0.0142 - mean_absolute_error: 0.0842 - val_loss: 0.0145 - val_mean_absolute_error: 0.0850\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 17s 955us/step - loss: 0.0142 - mean_absolute_error: 0.0841 - val_loss: 0.0145 - val_mean_absolute_error: 0.0847\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 16s 949us/step - loss: 0.0140 - mean_absolute_error: 0.0835 - val_loss: 0.0145 - val_mean_absolute_error: 0.0848\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0141 - mean_absolute_error: 0.0837 - val_loss: 0.0145 - val_mean_absolute_error: 0.0847\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 16s 946us/step - loss: 0.0140 - mean_absolute_error: 0.0836 - val_loss: 0.0144 - val_mean_absolute_error: 0.0843\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 17s 952us/step - loss: 0.0140 - mean_absolute_error: 0.0834 - val_loss: 0.0144 - val_mean_absolute_error: 0.0844\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 17s 955us/step - loss: 0.0140 - mean_absolute_error: 0.0834 - val_loss: 0.0144 - val_mean_absolute_error: 0.0846\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 17s 958us/step - loss: 0.0140 - mean_absolute_error: 0.0835 - val_loss: 0.0145 - val_mean_absolute_error: 0.0853\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0140 - mean_absolute_error: 0.0834 - val_loss: 0.0146 - val_mean_absolute_error: 0.0851\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0138 - mean_absolute_error: 0.0828 - val_loss: 0.0144 - val_mean_absolute_error: 0.0843\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 17s 987us/step - loss: 0.0138 - mean_absolute_error: 0.0830 - val_loss: 0.0147 - val_mean_absolute_error: 0.0853\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 17s 968us/step - loss: 0.0139 - mean_absolute_error: 0.0832 - val_loss: 0.0151 - val_mean_absolute_error: 0.0865\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 17s 979us/step - loss: 0.0142 - mean_absolute_error: 0.0840 - val_loss: 0.0142 - val_mean_absolute_error: 0.0837\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 17s 979us/step - loss: 0.0138 - mean_absolute_error: 0.0829 - val_loss: 0.0143 - val_mean_absolute_error: 0.0839\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 17s 964us/step - loss: 0.0137 - mean_absolute_error: 0.0825 - val_loss: 0.0142 - val_mean_absolute_error: 0.0841\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 17s 960us/step - loss: 0.0139 - mean_absolute_error: 0.0830 - val_loss: 0.0141 - val_mean_absolute_error: 0.0835\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 17s 959us/step - loss: 0.0137 - mean_absolute_error: 0.0824 - val_loss: 0.0141 - val_mean_absolute_error: 0.0833\n",
      "Epoch 56/150\n",
      " 4096/17351 [======>.......................] - ETA: 10s - loss: 0.0137 - mean_absolute_error: 0.0821"
     ]
    }
   ],
   "source": [
    "'''Experiments vanilla Autoencoder with variation of hidden units & dimension of input data: '''\n",
    "\n",
    "\n",
    "#create dict to store results, history, models...\n",
    "autoencod_dict_results = {}\n",
    "\n",
    "\n",
    "#area 237 training: 2009-2010, validation:2011 test:2012\n",
    "ts_series_09_12_area237 = ts_10largest['2009':'2012'].iloc[:,0].copy()\n",
    "ts_series_multivar = ts_10largest['2009':'2012'].iloc[:,:].copy()\n",
    "\n",
    "#set parameters, valid for all experiments:\n",
    "train_set_year = '2010'\n",
    "validation_set_year = '2011'\n",
    "test_set_year = '2012'\n",
    "\n",
    "n_batch_size_autoencoder = 512\n",
    "n_batch_size_pred_model = 512\n",
    "dropout_rate = 0.3\n",
    "n_epochs = 150\n",
    "\n",
    "shuffle_flag = True    \n",
    "early_stopping_flag = True\n",
    "    \n",
    "\n",
    "#tuples: (stacked_encoder, n_timesteps_T, n_timesteps_F, n_preds, multivariate_flag, uber_flag, scale_range, standardizing_flag, n_hidden1_units_autoencod,n_hidden2_units_autoencod,n_hidden1_units_pred,n_hidden2_units_pred,n_hidden3_units_pred)\n",
    "experiment_settings = [(True, 168,24,1,False,False,(-1,1), False, 256,128,128,64,0),\n",
    "                       (True, 168,24,1,False,False,(-1,1), True, 256,128,128,64,0),\n",
    "                       (True, 168,24,10,True,False,(-1,1), False, 256,128,128,64,0),\n",
    "                       (True, 168,24,10,True,False,(-1,1), True, 256,128,128,64,0),\n",
    "                       (True, 168,24,1,False,False,(-1,1), False, 512,256,128,64,0),\n",
    "                       (True, 168,24,10,True,False,(-1,1), False, 512,256,128,64,0),\n",
    "                       (True, 24,24,1,False,False,(-1,1), False, 256,128,128,64,0),\n",
    "                       (True, 24,24,10,True,False,(-1,1), False, 256,128,128,64,0),\n",
    "                       (False, 168,24,1,False,False,(-1,1), False, 256,128,128,64,0),\n",
    "                       (False, 168,24,10,True,False,(-1,1), False, 512,256,128,64,0)]\n",
    "\n",
    "\n",
    "#get model & predictions:\n",
    "for i in range(len(experiment_settings)):\n",
    "      \n",
    "    #get parameters:\n",
    "    stacked_encoder_flag, n_timesteps_T, n_timesteps_F, n_preds, multivariate_flag, uber_flag, scale_range, standardizing_flag, n_hidden1_units_autoencod, n_hidden2_units_autoencod, n_hidden1_units_pred, n_hidden2_units_pred, n_hidden3_units_pred = experiment_settings[i]    \n",
    "    \n",
    "    #set model_name:\n",
    "    if standardizing_flag == True:\n",
    "        scaling = 'scaling_std'\n",
    "    else:\n",
    "        scaling = 'scaling_tanh'\n",
    " \n",
    "\n",
    "    #set model_name:\n",
    "    model_name = str(n_hidden1_units_autoencod) + '_' + str(n_hidden2_units_autoencod)+ '_1D_' + str(n_hidden2_units_autoencod) + '_predict_model_2H_' + str(n_hidden1_units_pred) + '_' + str(n_hidden2_units_pred) + '_batch512_drop03_' + scaling + '_W' + str(n_timesteps_T) \n",
    "    \n",
    "    \n",
    "    if stacked_encoder_flag == True:\n",
    "        prefix = 'encoder_2H_'\n",
    "    else:\n",
    "        prefix = 'encoder_1H_'\n",
    "        \n",
    "    if multivariate_flag == True:\n",
    "        prefix = 'multivariate_' + prefix\n",
    "        postfix = '_10largest_areas'\n",
    "        ts_series = ts_series_multivar\n",
    "    else:\n",
    "        postfix = '_area237'\n",
    "        ts_series = ts_series_09_12_area237\n",
    "   \n",
    "    #final name:\n",
    "    model_name = prefix + model_name + postfix + '__y2012'\n",
    "    \n",
    "    #add key to dict:\n",
    "    autoencod_dict_results[model_name] = []\n",
    "    \n",
    "    print('######## START Experiment {}'.format(model_name))\n",
    "        \n",
    "    #call function to generate model and receive predictions:\n",
    "    all_results_area_i = get_full_autoencoder_pred_model(ts_series, multivariate_flag, \n",
    "                                                         train_set_year, validation_set_year, test_set_year, \n",
    "                                                         model_name, n_timesteps_T, n_timesteps_F, n_preds, \n",
    "                                                         scale_range, standardizing_flag, stacked_encoder_flag, \n",
    "                                                         n_hidden1_units_autoencod, n_hidden2_units_autoencod, \n",
    "                                                         n_hidden1_units_pred, n_hidden2_units_pred, \n",
    "                                                         n_hidden3_units_pred, n_batch_size_autoencoder, \n",
    "                                                         n_batch_size_pred_model, dropout_rate, \n",
    "                                                         n_epochs, shuffle_flag, early_stopping_flag, \n",
    "                                                         uber_flag)       \n",
    "\n",
    "    #append results in dict:\n",
    "    autoencod_dict_results[model_name].append(all_results_area_i)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uber Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### START Experiment with  UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 1)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 11s 640us/step - loss: 0.0415 - val_loss: 0.0439\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0359 - val_loss: 0.0388\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0323 - val_loss: 0.0338\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0304 - val_loss: 0.0310\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0289 - val_loss: 0.0321\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0255 - val_loss: 0.0268\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0242 - val_loss: 0.0272\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0229 - val_loss: 0.0249\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0227 - val_loss: 0.0246\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0216 - val_loss: 0.0235\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0210 - val_loss: 0.0223\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0203 - val_loss: 0.0240\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0195 - val_loss: 0.0218\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0193 - val_loss: 0.0215\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 9s 511us/step - loss: 0.0187 - val_loss: 0.0206\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0177 - val_loss: 0.0186\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0171 - val_loss: 0.0221\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0175 - val_loss: 0.0184\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0159 - val_loss: 0.0155\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0151 - val_loss: 0.0174\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 9s 508us/step - loss: 0.0141 - val_loss: 0.0133\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0142 - val_loss: 0.0137\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0136 - val_loss: 0.0133\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0134 - val_loss: 0.0146\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0127 - val_loss: 0.0119\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 9s 507us/step - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0130 - val_loss: 0.0118\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 9s 506us/step - loss: 0.0119 - val_loss: 0.0109\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0112 - val_loss: 0.0112\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0112 - val_loss: 0.0114\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 9s 508us/step - loss: 0.0113 - val_loss: 0.0108\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0109 - val_loss: 0.0111\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0108 - val_loss: 0.0104\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0102 - val_loss: 0.0113\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 9s 507us/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0103 - val_loss: 0.0103\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0102 - val_loss: 0.0132\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0104 - val_loss: 0.0099\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 9s 509us/step - loss: 0.0095 - val_loss: 0.0102\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0092 - val_loss: 0.0097\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0098 - val_loss: 0.0113\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0094 - val_loss: 0.0095\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0090 - val_loss: 0.0095\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0094 - val_loss: 0.0099\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0092 - val_loss: 0.0095\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0090 - val_loss: 0.0093\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0087 - val_loss: 0.0090\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0088 - val_loss: 0.0107\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 8s 490us/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0084 - val_loss: 0.0090\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0083 - val_loss: 0.0088\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 8s 488us/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0084 - val_loss: 0.0089\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0082 - val_loss: 0.0088\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0080 - val_loss: 0.0087\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0083 - val_loss: 0.0091\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0082 - val_loss: 0.0093\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0080 - val_loss: 0.0085\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0078 - val_loss: 0.0088\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0078 - val_loss: 0.0089\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0077 - val_loss: 0.0085\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0076 - val_loss: 0.0086\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0077 - val_loss: 0.0085\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0075 - val_loss: 0.0085\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0074 - val_loss: 0.0085\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0075 - val_loss: 0.0085\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0080 - val_loss: 0.0083\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0072 - val_loss: 0.0083\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0071 - val_loss: 0.0087\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0073 - val_loss: 0.0088\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0071 - val_loss: 0.0085\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0070 - val_loss: 0.0084\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 9s 490us/step - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0078 - val_loss: 0.0089\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0070 - val_loss: 0.0086\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 9s 492us/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 8s 490us/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0064 - val_loss: 0.0091\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 9s 491us/step - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 9s 491us/step - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0061 - val_loss: 0.0085\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 145/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0063 - val_loss: 0.0084\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 8s 490us/step - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0059 - val_loss: 0.0084\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 256)\n",
      "X_valid_encoded shape:  (8760, 256)\n",
      "X_test_encoded shape:  (8784, 256)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 2s 109us/step - loss: 0.0969 - mean_absolute_error: 0.2383 - val_loss: 0.0201 - val_mean_absolute_error: 0.1050\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0375 - mean_absolute_error: 0.1500 - val_loss: 0.0173 - val_mean_absolute_error: 0.0972\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0286 - mean_absolute_error: 0.1305 - val_loss: 0.0159 - val_mean_absolute_error: 0.0943\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0229 - mean_absolute_error: 0.1172 - val_loss: 0.0152 - val_mean_absolute_error: 0.0919\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0209 - mean_absolute_error: 0.1108 - val_loss: 0.0146 - val_mean_absolute_error: 0.0899\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0186 - mean_absolute_error: 0.1045 - val_loss: 0.0142 - val_mean_absolute_error: 0.0887\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0179 - mean_absolute_error: 0.1023 - val_loss: 0.0139 - val_mean_absolute_error: 0.0883\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0170 - mean_absolute_error: 0.0989 - val_loss: 0.0143 - val_mean_absolute_error: 0.0886\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0163 - mean_absolute_error: 0.0971 - val_loss: 0.0133 - val_mean_absolute_error: 0.0859\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0154 - mean_absolute_error: 0.0943 - val_loss: 0.0131 - val_mean_absolute_error: 0.0855\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0151 - mean_absolute_error: 0.0934 - val_loss: 0.0132 - val_mean_absolute_error: 0.0859\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0145 - mean_absolute_error: 0.0915 - val_loss: 0.0130 - val_mean_absolute_error: 0.0846\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0142 - mean_absolute_error: 0.0901 - val_loss: 0.0128 - val_mean_absolute_error: 0.0838\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0139 - mean_absolute_error: 0.0893 - val_loss: 0.0127 - val_mean_absolute_error: 0.0835\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0133 - mean_absolute_error: 0.0874 - val_loss: 0.0124 - val_mean_absolute_error: 0.0824\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0135 - mean_absolute_error: 0.0875 - val_loss: 0.0126 - val_mean_absolute_error: 0.0832\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0132 - mean_absolute_error: 0.0865 - val_loss: 0.0125 - val_mean_absolute_error: 0.0828\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0129 - mean_absolute_error: 0.0855 - val_loss: 0.0123 - val_mean_absolute_error: 0.0830\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0127 - mean_absolute_error: 0.0844 - val_loss: 0.0122 - val_mean_absolute_error: 0.0814\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0126 - mean_absolute_error: 0.0846 - val_loss: 0.0122 - val_mean_absolute_error: 0.0812\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0124 - mean_absolute_error: 0.0838 - val_loss: 0.0122 - val_mean_absolute_error: 0.0814\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0123 - mean_absolute_error: 0.0832 - val_loss: 0.0120 - val_mean_absolute_error: 0.0814\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0121 - mean_absolute_error: 0.0826 - val_loss: 0.0120 - val_mean_absolute_error: 0.0808\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0122 - mean_absolute_error: 0.0829 - val_loss: 0.0122 - val_mean_absolute_error: 0.0817\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0119 - mean_absolute_error: 0.0817 - val_loss: 0.0118 - val_mean_absolute_error: 0.0804\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0120 - mean_absolute_error: 0.0817 - val_loss: 0.0119 - val_mean_absolute_error: 0.0802\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0118 - mean_absolute_error: 0.0811 - val_loss: 0.0124 - val_mean_absolute_error: 0.0822\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0116 - mean_absolute_error: 0.0805 - val_loss: 0.0118 - val_mean_absolute_error: 0.0797\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0113 - mean_absolute_error: 0.0797 - val_loss: 0.0118 - val_mean_absolute_error: 0.0798\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0114 - mean_absolute_error: 0.0800 - val_loss: 0.0114 - val_mean_absolute_error: 0.0786\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0116 - mean_absolute_error: 0.0804 - val_loss: 0.0118 - val_mean_absolute_error: 0.0795\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0114 - mean_absolute_error: 0.0795 - val_loss: 0.0116 - val_mean_absolute_error: 0.0786\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0112 - mean_absolute_error: 0.0790 - val_loss: 0.0117 - val_mean_absolute_error: 0.0798\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0111 - mean_absolute_error: 0.0786 - val_loss: 0.0115 - val_mean_absolute_error: 0.0791\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0114 - mean_absolute_error: 0.0794 - val_loss: 0.0116 - val_mean_absolute_error: 0.0795\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0110 - mean_absolute_error: 0.0780 - val_loss: 0.0119 - val_mean_absolute_error: 0.0812\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0113 - mean_absolute_error: 0.0788 - val_loss: 0.0117 - val_mean_absolute_error: 0.0801\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0111 - mean_absolute_error: 0.0786 - val_loss: 0.0113 - val_mean_absolute_error: 0.0776\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0110 - mean_absolute_error: 0.0782 - val_loss: 0.0115 - val_mean_absolute_error: 0.0781\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0767 - val_loss: 0.0114 - val_mean_absolute_error: 0.0781\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0108 - mean_absolute_error: 0.0774 - val_loss: 0.0114 - val_mean_absolute_error: 0.0783\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0108 - mean_absolute_error: 0.0769 - val_loss: 0.0114 - val_mean_absolute_error: 0.0779\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0108 - mean_absolute_error: 0.0774 - val_loss: 0.0115 - val_mean_absolute_error: 0.0778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0770 - val_loss: 0.0113 - val_mean_absolute_error: 0.0780\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0763 - val_loss: 0.0114 - val_mean_absolute_error: 0.0783\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0765 - val_loss: 0.0117 - val_mean_absolute_error: 0.0789\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0114 - val_mean_absolute_error: 0.0788\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0764 - val_loss: 0.0112 - val_mean_absolute_error: 0.0772\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0754 - val_loss: 0.0115 - val_mean_absolute_error: 0.0776\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0112 - val_mean_absolute_error: 0.0774\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0767 - val_loss: 0.0119 - val_mean_absolute_error: 0.0800\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0758 - val_loss: 0.0111 - val_mean_absolute_error: 0.0769\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0104 - mean_absolute_error: 0.0757 - val_loss: 0.0113 - val_mean_absolute_error: 0.0778\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0751 - val_loss: 0.0113 - val_mean_absolute_error: 0.0773\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0749 - val_loss: 0.0109 - val_mean_absolute_error: 0.0760\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0752 - val_loss: 0.0109 - val_mean_absolute_error: 0.0769\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0749 - val_loss: 0.0111 - val_mean_absolute_error: 0.0767\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0749 - val_loss: 0.0110 - val_mean_absolute_error: 0.0762\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0740 - val_loss: 0.0109 - val_mean_absolute_error: 0.0759\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0750 - val_loss: 0.0112 - val_mean_absolute_error: 0.0775\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0751 - val_loss: 0.0109 - val_mean_absolute_error: 0.0761\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0749 - val_loss: 0.0109 - val_mean_absolute_error: 0.0763\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0739 - val_loss: 0.0110 - val_mean_absolute_error: 0.0765\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0743 - val_loss: 0.0112 - val_mean_absolute_error: 0.0776\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0741 - val_loss: 0.0112 - val_mean_absolute_error: 0.0767\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0740 - val_loss: 0.0112 - val_mean_absolute_error: 0.0776\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0754 - val_loss: 0.0109 - val_mean_absolute_error: 0.0756\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0743 - val_loss: 0.0110 - val_mean_absolute_error: 0.0772\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0736 - val_loss: 0.0110 - val_mean_absolute_error: 0.0769\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0738 - val_loss: 0.0114 - val_mean_absolute_error: 0.0772\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0747 - val_loss: 0.0110 - val_mean_absolute_error: 0.0759\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0098 - mean_absolute_error: 0.0733 - val_loss: 0.0108 - val_mean_absolute_error: 0.0760\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0100 - mean_absolute_error: 0.0739 - val_loss: 0.0112 - val_mean_absolute_error: 0.0771\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0745 - val_loss: 0.0108 - val_mean_absolute_error: 0.0754\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0737 - val_loss: 0.0108 - val_mean_absolute_error: 0.0763\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0738 - val_loss: 0.0109 - val_mean_absolute_error: 0.0758\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0734 - val_loss: 0.0109 - val_mean_absolute_error: 0.0757\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0735 - val_loss: 0.0113 - val_mean_absolute_error: 0.0781\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0736 - val_loss: 0.0110 - val_mean_absolute_error: 0.0758\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0098 - mean_absolute_error: 0.0730 - val_loss: 0.0107 - val_mean_absolute_error: 0.0751\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0098 - mean_absolute_error: 0.0728 - val_loss: 0.0107 - val_mean_absolute_error: 0.0752\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0732 - val_loss: 0.0113 - val_mean_absolute_error: 0.0780\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0736 - val_loss: 0.0113 - val_mean_absolute_error: 0.0772\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0728 - val_loss: 0.0109 - val_mean_absolute_error: 0.0762\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0725 - val_loss: 0.0110 - val_mean_absolute_error: 0.0763\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0731 - val_loss: 0.0110 - val_mean_absolute_error: 0.0764\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0098 - mean_absolute_error: 0.0732 - val_loss: 0.0106 - val_mean_absolute_error: 0.0743\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0096 - mean_absolute_error: 0.0725 - val_loss: 0.0107 - val_mean_absolute_error: 0.0749\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0723 - val_loss: 0.0108 - val_mean_absolute_error: 0.0760\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0729 - val_loss: 0.0110 - val_mean_absolute_error: 0.0756\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0099 - mean_absolute_error: 0.0728 - val_loss: 0.0106 - val_mean_absolute_error: 0.0744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0732 - val_loss: 0.0108 - val_mean_absolute_error: 0.0752\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0727 - val_loss: 0.0105 - val_mean_absolute_error: 0.0741\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0729 - val_loss: 0.0109 - val_mean_absolute_error: 0.0757\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0725 - val_loss: 0.0107 - val_mean_absolute_error: 0.0749\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0720 - val_loss: 0.0109 - val_mean_absolute_error: 0.0764\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0720 - val_loss: 0.0106 - val_mean_absolute_error: 0.0741\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0722 - val_loss: 0.0107 - val_mean_absolute_error: 0.0751\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0726 - val_loss: 0.0107 - val_mean_absolute_error: 0.0749\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0106 - val_mean_absolute_error: 0.0753\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0725 - val_loss: 0.0107 - val_mean_absolute_error: 0.0747\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0722 - val_loss: 0.0107 - val_mean_absolute_error: 0.0748\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0724 - val_loss: 0.0107 - val_mean_absolute_error: 0.0753\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0718 - val_loss: 0.0108 - val_mean_absolute_error: 0.0754\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0105 - val_mean_absolute_error: 0.0743\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0725 - val_loss: 0.0107 - val_mean_absolute_error: 0.0745\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0721 - val_loss: 0.0105 - val_mean_absolute_error: 0.0744\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0712 - val_loss: 0.0105 - val_mean_absolute_error: 0.0742\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0714 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0717 - val_loss: 0.0109 - val_mean_absolute_error: 0.0758\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0720 - val_loss: 0.0107 - val_mean_absolute_error: 0.0749\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0716 - val_loss: 0.0104 - val_mean_absolute_error: 0.0742\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0719 - val_loss: 0.0111 - val_mean_absolute_error: 0.0774\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0718 - val_loss: 0.0108 - val_mean_absolute_error: 0.0758\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0107 - val_mean_absolute_error: 0.0745\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0721 - val_loss: 0.0103 - val_mean_absolute_error: 0.0732\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0106 - val_mean_absolute_error: 0.0749\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0106 - val_mean_absolute_error: 0.0742\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0711 - val_loss: 0.0108 - val_mean_absolute_error: 0.0762\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0722 - val_loss: 0.0106 - val_mean_absolute_error: 0.0743\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0714 - val_loss: 0.0107 - val_mean_absolute_error: 0.0744\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0107 - val_mean_absolute_error: 0.0754\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0712 - val_loss: 0.0104 - val_mean_absolute_error: 0.0736\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0714 - val_loss: 0.0107 - val_mean_absolute_error: 0.0749\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0095 - mean_absolute_error: 0.0715 - val_loss: 0.0105 - val_mean_absolute_error: 0.0740\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0716 - val_loss: 0.0105 - val_mean_absolute_error: 0.0741\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0715 - val_loss: 0.0109 - val_mean_absolute_error: 0.0764\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0721 - val_loss: 0.0105 - val_mean_absolute_error: 0.0742\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0106 - val_mean_absolute_error: 0.0747\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0712 - val_loss: 0.0106 - val_mean_absolute_error: 0.0742\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0106 - val_mean_absolute_error: 0.0752\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0714 - val_loss: 0.0105 - val_mean_absolute_error: 0.0744\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0108 - val_mean_absolute_error: 0.0758\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0105 - val_mean_absolute_error: 0.0749\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0713 - val_loss: 0.0105 - val_mean_absolute_error: 0.0739\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0705 - val_loss: 0.0111 - val_mean_absolute_error: 0.0759\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0713 - val_loss: 0.0105 - val_mean_absolute_error: 0.0740\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0092 - mean_absolute_error: 0.0700 - val_loss: 0.0107 - val_mean_absolute_error: 0.0752\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0710 - val_loss: 0.0106 - val_mean_absolute_error: 0.0742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0103 - val_mean_absolute_error: 0.0735\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0704 - val_loss: 0.0104 - val_mean_absolute_error: 0.0738\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0714 - val_loss: 0.0105 - val_mean_absolute_error: 0.0748\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0710 - val_loss: 0.0107 - val_mean_absolute_error: 0.0754\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0715 - val_loss: 0.0109 - val_mean_absolute_error: 0.0759\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0710 - val_loss: 0.0104 - val_mean_absolute_error: 0.0735\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0091 - mean_absolute_error: 0.0700 - val_loss: 0.0106 - val_mean_absolute_error: 0.0747\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0709 - val_loss: 0.0106 - val_mean_absolute_error: 0.0744\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0706 - val_loss: 0.0103 - val_mean_absolute_error: 0.0734\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0709 - val_loss: 0.0103 - val_mean_absolute_error: 0.0738\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0707 - val_loss: 0.0105 - val_mean_absolute_error: 0.0735\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 98us/step\n",
      "yhat shape:  (8760, 1)\n",
      "First 2 scaled predictions\n",
      "[[-85.04318]\n",
      " [-78.71765]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    428.956818\n",
      "2011-01-01 01:00:00    251.282349\n",
      "2011-01-01 02:00:00    311.140247\n",
      "2011-01-01 03:00:00    200.917084\n",
      "2011-01-01 04:00:00     96.526363\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: validation_set: 92.67481621499357\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 26us/step\n",
      "yhat shape:  (8784, 1)\n",
      "First 2 scaled predictions\n",
      "[[-101.98976 ]\n",
      " [-100.993706]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    444.010239\n",
      "2012-01-01 01:00:00    351.006294\n",
      "2012-01-01 02:00:00    405.185478\n",
      "2012-01-01 03:00:00    227.641586\n",
      "2012-01-01 04:00:00     89.700035\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012: 91.52820149955872\n",
      "### START Experiment with  UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 1)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 11s 646us/step - loss: 0.0422 - val_loss: 0.0440\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0362 - val_loss: 0.0392\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0326 - val_loss: 0.0358\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0310 - val_loss: 0.0317\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0277 - val_loss: 0.0315\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0257 - val_loss: 0.0274\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0247 - val_loss: 0.0267\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0230 - val_loss: 0.0256\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 9s 492us/step - loss: 0.0228 - val_loss: 0.0248\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 9s 506us/step - loss: 0.0216 - val_loss: 0.0233\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0214 - val_loss: 0.0231\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0204 - val_loss: 0.0228\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0200 - val_loss: 0.0233\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0185 - val_loss: 0.0191\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0185 - val_loss: 0.0199\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0183 - val_loss: 0.0216\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0175 - val_loss: 0.0186\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0163 - val_loss: 0.0179\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 9s 504us/step - loss: 0.0152 - val_loss: 0.0154\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0151 - val_loss: 0.0157\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0145 - val_loss: 0.0160\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0141 - val_loss: 0.0133\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0142 - val_loss: 0.0155\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0137 - val_loss: 0.0132\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0138 - val_loss: 0.0128\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0134 - val_loss: 0.0125\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0127 - val_loss: 0.0120\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0125 - val_loss: 0.0126\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0129 - val_loss: 0.0115\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0109 - val_loss: 0.0112\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0113 - val_loss: 0.0107\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0110 - val_loss: 0.0105\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0100 - val_loss: 0.0103\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 9s 491us/step - loss: 0.0098 - val_loss: 0.0101\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0097 - val_loss: 0.0103\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0094 - val_loss: 0.0096\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0092 - val_loss: 0.0096\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0091 - val_loss: 0.0095\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0096 - val_loss: 0.0096\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0091 - val_loss: 0.0092\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0089 - val_loss: 0.0093\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0087 - val_loss: 0.0090\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0086 - val_loss: 0.0088\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0088 - val_loss: 0.0091\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0085 - val_loss: 0.0089\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0085 - val_loss: 0.0090\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0084 - val_loss: 0.0087\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0084 - val_loss: 0.0087\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0086 - val_loss: 0.0090\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0083 - val_loss: 0.0088\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 8s 490us/step - loss: 0.0080 - val_loss: 0.0088\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0081 - val_loss: 0.0087\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0080 - val_loss: 0.0085\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0079 - val_loss: 0.0088\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0080 - val_loss: 0.0085\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0078 - val_loss: 0.0089\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 9s 493us/step - loss: 0.0077 - val_loss: 0.0083\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0077 - val_loss: 0.0100\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 9s 508us/step - loss: 0.0077 - val_loss: 0.0083\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0075 - val_loss: 0.0083\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0074 - val_loss: 0.0087\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0074 - val_loss: 0.0083\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0075 - val_loss: 0.0087\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0072 - val_loss: 0.0087\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0075 - val_loss: 0.0081\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0072 - val_loss: 0.0082\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0072 - val_loss: 0.0082\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0072 - val_loss: 0.0082\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0073 - val_loss: 0.0090\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0073 - val_loss: 0.0085\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 9s 492us/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0070 - val_loss: 0.0085\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0076 - val_loss: 0.0092\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0069 - val_loss: 0.0082\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0072 - val_loss: 0.0079\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0068 - val_loss: 0.0083\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0069 - val_loss: 0.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 9s 501us/step - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 9s 495us/step - loss: 0.0069 - val_loss: 0.0078\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 9s 500us/step - loss: 0.0067 - val_loss: 0.0079\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 9s 498us/step - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0064 - val_loss: 0.0088\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 9s 494us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 9s 499us/step - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 8s 487us/step - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 9s 503us/step - loss: 0.0060 - val_loss: 0.0079\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 9s 505us/step - loss: 0.0061 - val_loss: 0.0081\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 9s 502us/step - loss: 0.0062 - val_loss: 0.0083\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 9s 497us/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 9s 496us/step - loss: 0.0060 - val_loss: 0.0081\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 256)\n",
      "X_valid_encoded shape:  (8760, 256)\n",
      "X_test_encoded shape:  (8784, 256)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 2s 127us/step - loss: 0.0773 - mean_absolute_error: 0.2107 - val_loss: 0.0192 - val_mean_absolute_error: 0.1014\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0322 - mean_absolute_error: 0.1387 - val_loss: 0.0163 - val_mean_absolute_error: 0.0958\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0244 - mean_absolute_error: 0.1201 - val_loss: 0.0154 - val_mean_absolute_error: 0.0936\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0214 - mean_absolute_error: 0.1120 - val_loss: 0.0149 - val_mean_absolute_error: 0.0911\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0192 - mean_absolute_error: 0.1055 - val_loss: 0.0146 - val_mean_absolute_error: 0.0897\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0177 - mean_absolute_error: 0.1018 - val_loss: 0.0138 - val_mean_absolute_error: 0.0874\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0168 - mean_absolute_error: 0.0981 - val_loss: 0.0136 - val_mean_absolute_error: 0.0870\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0157 - mean_absolute_error: 0.0950 - val_loss: 0.0137 - val_mean_absolute_error: 0.0868\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0152 - mean_absolute_error: 0.0934 - val_loss: 0.0134 - val_mean_absolute_error: 0.0859\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0151 - mean_absolute_error: 0.0928 - val_loss: 0.0136 - val_mean_absolute_error: 0.0863\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0143 - mean_absolute_error: 0.0905 - val_loss: 0.0134 - val_mean_absolute_error: 0.0856\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0139 - mean_absolute_error: 0.0890 - val_loss: 0.0129 - val_mean_absolute_error: 0.0845\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0137 - mean_absolute_error: 0.0883 - val_loss: 0.0125 - val_mean_absolute_error: 0.0829\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0872 - val_loss: 0.0127 - val_mean_absolute_error: 0.0830\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0133 - mean_absolute_error: 0.0863 - val_loss: 0.0126 - val_mean_absolute_error: 0.0839\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0129 - mean_absolute_error: 0.0855 - val_loss: 0.0124 - val_mean_absolute_error: 0.0831\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0129 - mean_absolute_error: 0.0856 - val_loss: 0.0124 - val_mean_absolute_error: 0.0828\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0126 - mean_absolute_error: 0.0843 - val_loss: 0.0123 - val_mean_absolute_error: 0.0823\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0121 - mean_absolute_error: 0.0827 - val_loss: 0.0121 - val_mean_absolute_error: 0.0824\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0123 - mean_absolute_error: 0.0828 - val_loss: 0.0125 - val_mean_absolute_error: 0.0827\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0123 - mean_absolute_error: 0.0827 - val_loss: 0.0122 - val_mean_absolute_error: 0.0820\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0121 - mean_absolute_error: 0.0822 - val_loss: 0.0121 - val_mean_absolute_error: 0.0814\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0117 - mean_absolute_error: 0.0812 - val_loss: 0.0121 - val_mean_absolute_error: 0.0807\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0117 - mean_absolute_error: 0.0808 - val_loss: 0.0119 - val_mean_absolute_error: 0.0804\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0117 - mean_absolute_error: 0.0810 - val_loss: 0.0119 - val_mean_absolute_error: 0.0806\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0116 - mean_absolute_error: 0.0803 - val_loss: 0.0119 - val_mean_absolute_error: 0.0802\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0115 - mean_absolute_error: 0.0801 - val_loss: 0.0119 - val_mean_absolute_error: 0.0808\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0114 - mean_absolute_error: 0.0796 - val_loss: 0.0117 - val_mean_absolute_error: 0.0795\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0114 - mean_absolute_error: 0.0797 - val_loss: 0.0116 - val_mean_absolute_error: 0.0794\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0113 - mean_absolute_error: 0.0792 - val_loss: 0.0118 - val_mean_absolute_error: 0.0797\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0114 - mean_absolute_error: 0.0790 - val_loss: 0.0115 - val_mean_absolute_error: 0.0785\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0111 - mean_absolute_error: 0.0785 - val_loss: 0.0118 - val_mean_absolute_error: 0.0805\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0111 - mean_absolute_error: 0.0783 - val_loss: 0.0117 - val_mean_absolute_error: 0.0799\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0110 - mean_absolute_error: 0.0788 - val_loss: 0.0116 - val_mean_absolute_error: 0.0795\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0108 - mean_absolute_error: 0.0776 - val_loss: 0.0118 - val_mean_absolute_error: 0.0814\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0113 - mean_absolute_error: 0.0792 - val_loss: 0.0111 - val_mean_absolute_error: 0.0777\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0109 - mean_absolute_error: 0.0778 - val_loss: 0.0113 - val_mean_absolute_error: 0.0782\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0109 - mean_absolute_error: 0.0777 - val_loss: 0.0112 - val_mean_absolute_error: 0.0776\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0108 - mean_absolute_error: 0.0771 - val_loss: 0.0112 - val_mean_absolute_error: 0.0780\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0770 - val_loss: 0.0112 - val_mean_absolute_error: 0.0776\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0112 - val_mean_absolute_error: 0.0778\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0767 - val_loss: 0.0114 - val_mean_absolute_error: 0.0780\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0761 - val_loss: 0.0111 - val_mean_absolute_error: 0.0771\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0107 - mean_absolute_error: 0.0766 - val_loss: 0.0111 - val_mean_absolute_error: 0.0780\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0105 - mean_absolute_error: 0.0759 - val_loss: 0.0110 - val_mean_absolute_error: 0.0771\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0105 - mean_absolute_error: 0.0762 - val_loss: 0.0111 - val_mean_absolute_error: 0.0776\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0747 - val_loss: 0.0112 - val_mean_absolute_error: 0.0773\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0105 - mean_absolute_error: 0.0757 - val_loss: 0.0111 - val_mean_absolute_error: 0.0773\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0105 - mean_absolute_error: 0.0760 - val_loss: 0.0117 - val_mean_absolute_error: 0.0794\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0105 - mean_absolute_error: 0.0761 - val_loss: 0.0111 - val_mean_absolute_error: 0.0773\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0111 - val_mean_absolute_error: 0.0771\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0755 - val_loss: 0.0111 - val_mean_absolute_error: 0.0772\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0746 - val_loss: 0.0108 - val_mean_absolute_error: 0.0761\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0748 - val_loss: 0.0106 - val_mean_absolute_error: 0.0760\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0104 - mean_absolute_error: 0.0757 - val_loss: 0.0110 - val_mean_absolute_error: 0.0774\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0104 - mean_absolute_error: 0.0756 - val_loss: 0.0110 - val_mean_absolute_error: 0.0772\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0101 - mean_absolute_error: 0.0744 - val_loss: 0.0106 - val_mean_absolute_error: 0.0760\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0747 - val_loss: 0.0111 - val_mean_absolute_error: 0.0775\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0744 - val_loss: 0.0110 - val_mean_absolute_error: 0.0771\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0741 - val_loss: 0.0110 - val_mean_absolute_error: 0.0768\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0748 - val_loss: 0.0112 - val_mean_absolute_error: 0.0787\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0102 - mean_absolute_error: 0.0753 - val_loss: 0.0113 - val_mean_absolute_error: 0.0780\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0106 - mean_absolute_error: 0.0762 - val_loss: 0.0114 - val_mean_absolute_error: 0.0786\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0734 - val_loss: 0.0107 - val_mean_absolute_error: 0.0763\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0738 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0735 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0099 - mean_absolute_error: 0.0737 - val_loss: 0.0110 - val_mean_absolute_error: 0.0770\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0737 - val_loss: 0.0108 - val_mean_absolute_error: 0.0766\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0740 - val_loss: 0.0113 - val_mean_absolute_error: 0.0784\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0101 - mean_absolute_error: 0.0745 - val_loss: 0.0110 - val_mean_absolute_error: 0.0778\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0732 - val_loss: 0.0112 - val_mean_absolute_error: 0.0777\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0745 - val_loss: 0.0108 - val_mean_absolute_error: 0.0765\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0737 - val_loss: 0.0112 - val_mean_absolute_error: 0.0780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0735 - val_loss: 0.0108 - val_mean_absolute_error: 0.0761\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0729 - val_loss: 0.0109 - val_mean_absolute_error: 0.0770\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0731 - val_loss: 0.0107 - val_mean_absolute_error: 0.0757\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0734 - val_loss: 0.0106 - val_mean_absolute_error: 0.0764\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0734 - val_loss: 0.0108 - val_mean_absolute_error: 0.0765\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0734 - val_loss: 0.0107 - val_mean_absolute_error: 0.0755\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0096 - mean_absolute_error: 0.0725 - val_loss: 0.0109 - val_mean_absolute_error: 0.0766\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0732 - val_loss: 0.0110 - val_mean_absolute_error: 0.0771\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0728 - val_loss: 0.0107 - val_mean_absolute_error: 0.0759\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0738 - val_loss: 0.0105 - val_mean_absolute_error: 0.0752\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0098 - mean_absolute_error: 0.0729 - val_loss: 0.0107 - val_mean_absolute_error: 0.0757\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0730 - val_loss: 0.0105 - val_mean_absolute_error: 0.0750\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0724 - val_loss: 0.0106 - val_mean_absolute_error: 0.0756\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0731 - val_loss: 0.0111 - val_mean_absolute_error: 0.0790\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0728 - val_loss: 0.0107 - val_mean_absolute_error: 0.0755\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0099 - mean_absolute_error: 0.0732 - val_loss: 0.0105 - val_mean_absolute_error: 0.0748\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0726 - val_loss: 0.0109 - val_mean_absolute_error: 0.0760\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0096 - mean_absolute_error: 0.0723 - val_loss: 0.0106 - val_mean_absolute_error: 0.0756\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0724 - val_loss: 0.0104 - val_mean_absolute_error: 0.0747\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0726 - val_loss: 0.0107 - val_mean_absolute_error: 0.0767\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0724 - val_loss: 0.0116 - val_mean_absolute_error: 0.0805\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0730 - val_loss: 0.0107 - val_mean_absolute_error: 0.0766\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0722 - val_loss: 0.0103 - val_mean_absolute_error: 0.0748\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0726 - val_loss: 0.0104 - val_mean_absolute_error: 0.0746\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0723 - val_loss: 0.0104 - val_mean_absolute_error: 0.0747\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0726 - val_loss: 0.0107 - val_mean_absolute_error: 0.0764\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0722 - val_loss: 0.0106 - val_mean_absolute_error: 0.0750\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0719 - val_loss: 0.0108 - val_mean_absolute_error: 0.0768\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0717 - val_loss: 0.0106 - val_mean_absolute_error: 0.0753\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0725 - val_loss: 0.0104 - val_mean_absolute_error: 0.0746\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0724 - val_loss: 0.0107 - val_mean_absolute_error: 0.0753\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0106 - val_mean_absolute_error: 0.0765\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0717 - val_loss: 0.0106 - val_mean_absolute_error: 0.0754\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0719 - val_loss: 0.0110 - val_mean_absolute_error: 0.0771\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0723 - val_loss: 0.0107 - val_mean_absolute_error: 0.0755\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0714 - val_loss: 0.0105 - val_mean_absolute_error: 0.0747\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0732 - val_loss: 0.0109 - val_mean_absolute_error: 0.0760\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0721 - val_loss: 0.0107 - val_mean_absolute_error: 0.0767\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0104 - val_mean_absolute_error: 0.0749\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0720 - val_loss: 0.0105 - val_mean_absolute_error: 0.0750\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0105 - val_mean_absolute_error: 0.0753\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0103 - val_mean_absolute_error: 0.0743\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0712 - val_loss: 0.0105 - val_mean_absolute_error: 0.0755\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0105 - val_mean_absolute_error: 0.0751\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0108 - val_mean_absolute_error: 0.0759\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0095 - mean_absolute_error: 0.0718 - val_loss: 0.0107 - val_mean_absolute_error: 0.0759\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0714 - val_loss: 0.0106 - val_mean_absolute_error: 0.0751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0104 - val_mean_absolute_error: 0.0746\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0720 - val_loss: 0.0108 - val_mean_absolute_error: 0.0764\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0715 - val_loss: 0.0102 - val_mean_absolute_error: 0.0741\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0712 - val_loss: 0.0104 - val_mean_absolute_error: 0.0742\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0106 - val_mean_absolute_error: 0.0754\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0710 - val_loss: 0.0107 - val_mean_absolute_error: 0.0762\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0092 - mean_absolute_error: 0.0711 - val_loss: 0.0106 - val_mean_absolute_error: 0.0768\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 11us/step - loss: 0.0094 - mean_absolute_error: 0.0715 - val_loss: 0.0103 - val_mean_absolute_error: 0.0746\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0718 - val_loss: 0.0104 - val_mean_absolute_error: 0.0751\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0711 - val_loss: 0.0105 - val_mean_absolute_error: 0.0748\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0713 - val_loss: 0.0107 - val_mean_absolute_error: 0.0755\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0712 - val_loss: 0.0106 - val_mean_absolute_error: 0.0750\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0710 - val_loss: 0.0102 - val_mean_absolute_error: 0.0745\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0706 - val_loss: 0.0104 - val_mean_absolute_error: 0.0746\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0708 - val_loss: 0.0104 - val_mean_absolute_error: 0.0747\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0706 - val_loss: 0.0108 - val_mean_absolute_error: 0.0760\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0703 - val_loss: 0.0102 - val_mean_absolute_error: 0.0741\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0707 - val_loss: 0.0104 - val_mean_absolute_error: 0.0747\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0714 - val_loss: 0.0102 - val_mean_absolute_error: 0.0742\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0713 - val_loss: 0.0110 - val_mean_absolute_error: 0.0770\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0713 - val_loss: 0.0101 - val_mean_absolute_error: 0.0735\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0707 - val_loss: 0.0103 - val_mean_absolute_error: 0.0751\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0718 - val_loss: 0.0106 - val_mean_absolute_error: 0.0752\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0717 - val_loss: 0.0104 - val_mean_absolute_error: 0.0745\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0093 - mean_absolute_error: 0.0708 - val_loss: 0.0105 - val_mean_absolute_error: 0.0745\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0707 - val_loss: 0.0103 - val_mean_absolute_error: 0.0745\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0094 - mean_absolute_error: 0.0717 - val_loss: 0.0106 - val_mean_absolute_error: 0.0753\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0710 - val_loss: 0.0102 - val_mean_absolute_error: 0.0745\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0091 - mean_absolute_error: 0.0703 - val_loss: 0.0104 - val_mean_absolute_error: 0.0747\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 107us/step\n",
      "yhat shape:  (8760, 1)\n",
      "First 2 scaled predictions\n",
      "[[-53.15941]\n",
      " [-47.94705]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    460.840591\n",
      "2011-01-01 01:00:00    282.052952\n",
      "2011-01-01 02:00:00    346.978546\n",
      "2011-01-01 03:00:00    255.845721\n",
      "2011-01-01 04:00:00    169.326797\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: validation_set: 92.39742251727637\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 26us/step\n",
      "yhat shape:  (8784, 1)\n",
      "First 2 scaled predictions\n",
      "[[-126.94601]\n",
      " [-107.82107]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    419.053993\n",
      "2012-01-01 01:00:00    344.178932\n",
      "2012-01-01 02:00:00    421.899887\n",
      "2012-01-01 03:00:00    257.850349\n",
      "2012-01-01 04:00:00    140.043636\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012: 93.34959690263914\n",
      "### START Experiment with  UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 1)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 16s 915us/step - loss: 0.0426 - val_loss: 0.0450\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0358 - val_loss: 0.0385\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0328 - val_loss: 0.0355\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 13s 742us/step - loss: 0.0313 - val_loss: 0.0343\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0327 - val_loss: 0.0390\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0315 - val_loss: 0.0323\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0282 - val_loss: 0.0302\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0266 - val_loss: 0.0296\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0255 - val_loss: 0.0270\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0237 - val_loss: 0.0258\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0225 - val_loss: 0.0256\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0225 - val_loss: 0.0240\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0212 - val_loss: 0.0244\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0211 - val_loss: 0.0219\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0201 - val_loss: 0.0222\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0193 - val_loss: 0.0240\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0189 - val_loss: 0.0197\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 13s 754us/step - loss: 0.0182 - val_loss: 0.0221\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0178 - val_loss: 0.0187\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0176 - val_loss: 0.0180\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0170 - val_loss: 0.0190\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0163 - val_loss: 0.0158\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0158 - val_loss: 0.0204\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0166 - val_loss: 0.0145\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0151 - val_loss: 0.0134\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0159 - val_loss: 0.0137\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0140 - val_loss: 0.0137\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0134 - val_loss: 0.0127\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0131 - val_loss: 0.0126\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0129 - val_loss: 0.0119\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0121 - val_loss: 0.0121\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0119 - val_loss: 0.0113\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0121 - val_loss: 0.0117\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0114 - val_loss: 0.0110\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 13s 741us/step - loss: 0.0110 - val_loss: 0.0113\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0109 - val_loss: 0.0110\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0110 - val_loss: 0.0115\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 13s 742us/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0100 - val_loss: 0.0106\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0105 - val_loss: 0.0101\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0097 - val_loss: 0.0104\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0093 - val_loss: 0.0092\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 13s 741us/step - loss: 0.0089 - val_loss: 0.0094\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0086 - val_loss: 0.0093\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0085 - val_loss: 0.0091\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0086 - val_loss: 0.0092\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0082 - val_loss: 0.0093\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0081 - val_loss: 0.0087\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0084 - val_loss: 0.0110\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 13s 740us/step - loss: 0.0085 - val_loss: 0.0088\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0081 - val_loss: 0.0086\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0079 - val_loss: 0.0089\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0078 - val_loss: 0.0088\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0077 - val_loss: 0.0086\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0077 - val_loss: 0.0083\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0075 - val_loss: 0.0082\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0074 - val_loss: 0.0081\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0074 - val_loss: 0.0083\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0080 - val_loss: 0.0082\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0072 - val_loss: 0.0081\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0083 - val_loss: 0.0109\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0096 - val_loss: 0.0092\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0078 - val_loss: 0.0086\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0073 - val_loss: 0.0084\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0070 - val_loss: 0.0081\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 13s 752us/step - loss: 0.0071 - val_loss: 0.0080\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0070 - val_loss: 0.0080\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0070 - val_loss: 0.0079\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 13s 740us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 88/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0067 - val_loss: 0.0080\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 13s 753us/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0069 - val_loss: 0.0079\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0065 - val_loss: 0.0081\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0066 - val_loss: 0.0079\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0066 - val_loss: 0.0085\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0065 - val_loss: 0.0078\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 13s 753us/step - loss: 0.0064 - val_loss: 0.0081\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0070 - val_loss: 0.0087\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0065 - val_loss: 0.0080\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0062 - val_loss: 0.0081\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0063 - val_loss: 0.0083\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0061 - val_loss: 0.0086\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0059 - val_loss: 0.0083\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 13s 753us/step - loss: 0.0067 - val_loss: 0.0083\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0057 - val_loss: 0.0087\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0055 - val_loss: 0.0086\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 13s 752us/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0054 - val_loss: 0.0087\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0056 - val_loss: 0.0085\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0055 - val_loss: 0.0085\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 13s 746us/step - loss: 0.0055 - val_loss: 0.0083\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 13s 748us/step - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0052 - val_loss: 0.0087\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0052 - val_loss: 0.0084\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 13s 751us/step - loss: 0.0051 - val_loss: 0.0088\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 13s 747us/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 13s 749us/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 13s 742us/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 13s 743us/step - loss: 0.0050 - val_loss: 0.0089\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 13s 744us/step - loss: 0.0050 - val_loss: 0.0087\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 13s 745us/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 13s 741us/step - loss: 0.0048 - val_loss: 0.0088\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 512)\n",
      "X_valid_encoded shape:  (8760, 512)\n",
      "X_test_encoded shape:  (8784, 512)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 3s 144us/step - loss: 0.0914 - mean_absolute_error: 0.2319 - val_loss: 0.0174 - val_mean_absolute_error: 0.0995\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0364 - mean_absolute_error: 0.1474 - val_loss: 0.0154 - val_mean_absolute_error: 0.0929\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0270 - mean_absolute_error: 0.1266 - val_loss: 0.0142 - val_mean_absolute_error: 0.0902\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0228 - mean_absolute_error: 0.1157 - val_loss: 0.0139 - val_mean_absolute_error: 0.0883\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0200 - mean_absolute_error: 0.1079 - val_loss: 0.0131 - val_mean_absolute_error: 0.0862\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0183 - mean_absolute_error: 0.1035 - val_loss: 0.0128 - val_mean_absolute_error: 0.0847\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0170 - mean_absolute_error: 0.0990 - val_loss: 0.0123 - val_mean_absolute_error: 0.0830\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0156 - mean_absolute_error: 0.0953 - val_loss: 0.0125 - val_mean_absolute_error: 0.0831\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0152 - mean_absolute_error: 0.0935 - val_loss: 0.0119 - val_mean_absolute_error: 0.0808\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0143 - mean_absolute_error: 0.0903 - val_loss: 0.0119 - val_mean_absolute_error: 0.0813\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0138 - mean_absolute_error: 0.0889 - val_loss: 0.0114 - val_mean_absolute_error: 0.0788\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0872 - val_loss: 0.0113 - val_mean_absolute_error: 0.0788\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0865 - val_loss: 0.0115 - val_mean_absolute_error: 0.0791\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0126 - mean_absolute_error: 0.0844 - val_loss: 0.0112 - val_mean_absolute_error: 0.0782\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0122 - mean_absolute_error: 0.0828 - val_loss: 0.0109 - val_mean_absolute_error: 0.0770\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0122 - mean_absolute_error: 0.0825 - val_loss: 0.0109 - val_mean_absolute_error: 0.0773\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0116 - mean_absolute_error: 0.0807 - val_loss: 0.0108 - val_mean_absolute_error: 0.0763\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0115 - mean_absolute_error: 0.0800 - val_loss: 0.0108 - val_mean_absolute_error: 0.0763\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0116 - mean_absolute_error: 0.0804 - val_loss: 0.0107 - val_mean_absolute_error: 0.0760\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0111 - mean_absolute_error: 0.0787 - val_loss: 0.0109 - val_mean_absolute_error: 0.0764\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0109 - mean_absolute_error: 0.0777 - val_loss: 0.0107 - val_mean_absolute_error: 0.0757\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0108 - mean_absolute_error: 0.0772 - val_loss: 0.0107 - val_mean_absolute_error: 0.0759\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0109 - mean_absolute_error: 0.0777 - val_loss: 0.0111 - val_mean_absolute_error: 0.0773\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0106 - mean_absolute_error: 0.0766 - val_loss: 0.0105 - val_mean_absolute_error: 0.0752\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0105 - mean_absolute_error: 0.0760 - val_loss: 0.0106 - val_mean_absolute_error: 0.0751\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0103 - mean_absolute_error: 0.0755 - val_loss: 0.0105 - val_mean_absolute_error: 0.0751\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0103 - mean_absolute_error: 0.0753 - val_loss: 0.0109 - val_mean_absolute_error: 0.0760\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0103 - mean_absolute_error: 0.0753 - val_loss: 0.0108 - val_mean_absolute_error: 0.0759\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0104 - mean_absolute_error: 0.0754 - val_loss: 0.0108 - val_mean_absolute_error: 0.0760\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0102 - mean_absolute_error: 0.0751 - val_loss: 0.0108 - val_mean_absolute_error: 0.0756\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0737 - val_loss: 0.0104 - val_mean_absolute_error: 0.0744\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0100 - mean_absolute_error: 0.0741 - val_loss: 0.0106 - val_mean_absolute_error: 0.0748\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0100 - mean_absolute_error: 0.0741 - val_loss: 0.0107 - val_mean_absolute_error: 0.0751\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0097 - mean_absolute_error: 0.0735 - val_loss: 0.0105 - val_mean_absolute_error: 0.0747\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0730 - val_loss: 0.0103 - val_mean_absolute_error: 0.0738\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0734 - val_loss: 0.0104 - val_mean_absolute_error: 0.0745\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0097 - mean_absolute_error: 0.0732 - val_loss: 0.0107 - val_mean_absolute_error: 0.0748\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0097 - mean_absolute_error: 0.0724 - val_loss: 0.0106 - val_mean_absolute_error: 0.0747\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0716 - val_loss: 0.0105 - val_mean_absolute_error: 0.0744\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0713 - val_loss: 0.0104 - val_mean_absolute_error: 0.0744\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0722 - val_loss: 0.0104 - val_mean_absolute_error: 0.0741\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0712 - val_loss: 0.0102 - val_mean_absolute_error: 0.0737\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0709 - val_loss: 0.0103 - val_mean_absolute_error: 0.0734\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0712 - val_loss: 0.0110 - val_mean_absolute_error: 0.0765\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0723 - val_loss: 0.0103 - val_mean_absolute_error: 0.0738\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0707 - val_loss: 0.0103 - val_mean_absolute_error: 0.0739\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0092 - mean_absolute_error: 0.0706 - val_loss: 0.0103 - val_mean_absolute_error: 0.0735\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0695 - val_loss: 0.0104 - val_mean_absolute_error: 0.0743\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0091 - mean_absolute_error: 0.0704 - val_loss: 0.0102 - val_mean_absolute_error: 0.0731\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0091 - mean_absolute_error: 0.0695 - val_loss: 0.0103 - val_mean_absolute_error: 0.0736\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0090 - mean_absolute_error: 0.0699 - val_loss: 0.0102 - val_mean_absolute_error: 0.0734\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0696 - val_loss: 0.0105 - val_mean_absolute_error: 0.0744\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0091 - mean_absolute_error: 0.0701 - val_loss: 0.0103 - val_mean_absolute_error: 0.0739\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0091 - mean_absolute_error: 0.0704 - val_loss: 0.0103 - val_mean_absolute_error: 0.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0090 - mean_absolute_error: 0.0696 - val_loss: 0.0105 - val_mean_absolute_error: 0.0741\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0697 - val_loss: 0.0104 - val_mean_absolute_error: 0.0736\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0090 - mean_absolute_error: 0.0696 - val_loss: 0.0101 - val_mean_absolute_error: 0.0726\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0687 - val_loss: 0.0104 - val_mean_absolute_error: 0.0736\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0687 - val_loss: 0.0101 - val_mean_absolute_error: 0.0730\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0689 - val_loss: 0.0109 - val_mean_absolute_error: 0.0756\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0090 - mean_absolute_error: 0.0699 - val_loss: 0.0110 - val_mean_absolute_error: 0.0758\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0687 - val_loss: 0.0103 - val_mean_absolute_error: 0.0741\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0690 - val_loss: 0.0102 - val_mean_absolute_error: 0.0731\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0683 - val_loss: 0.0101 - val_mean_absolute_error: 0.0725\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0686 - val_loss: 0.0106 - val_mean_absolute_error: 0.0746\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0692 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0688 - val_loss: 0.0101 - val_mean_absolute_error: 0.0734\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0682 - val_loss: 0.0101 - val_mean_absolute_error: 0.0726\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0087 - mean_absolute_error: 0.0685 - val_loss: 0.0100 - val_mean_absolute_error: 0.0724\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0679 - val_loss: 0.0103 - val_mean_absolute_error: 0.0732\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0682 - val_loss: 0.0101 - val_mean_absolute_error: 0.0728\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0677 - val_loss: 0.0102 - val_mean_absolute_error: 0.0731\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0677 - val_loss: 0.0103 - val_mean_absolute_error: 0.0733\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0680 - val_loss: 0.0106 - val_mean_absolute_error: 0.0747\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0687 - val_loss: 0.0101 - val_mean_absolute_error: 0.0728\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0681 - val_loss: 0.0100 - val_mean_absolute_error: 0.0725\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0678 - val_loss: 0.0101 - val_mean_absolute_error: 0.0729\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0676 - val_loss: 0.0104 - val_mean_absolute_error: 0.0738\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0084 - mean_absolute_error: 0.0674 - val_loss: 0.0100 - val_mean_absolute_error: 0.0725\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0085 - mean_absolute_error: 0.0676 - val_loss: 0.0102 - val_mean_absolute_error: 0.0727\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0680 - val_loss: 0.0106 - val_mean_absolute_error: 0.0748\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0676 - val_loss: 0.0103 - val_mean_absolute_error: 0.0735\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0670 - val_loss: 0.0101 - val_mean_absolute_error: 0.0729\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0105 - val_mean_absolute_error: 0.0736\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0673 - val_loss: 0.0102 - val_mean_absolute_error: 0.0724\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0675 - val_loss: 0.0102 - val_mean_absolute_error: 0.0732\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0676 - val_loss: 0.0104 - val_mean_absolute_error: 0.0737\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0083 - mean_absolute_error: 0.0669 - val_loss: 0.0103 - val_mean_absolute_error: 0.0732\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0083 - mean_absolute_error: 0.0669 - val_loss: 0.0102 - val_mean_absolute_error: 0.0728\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0083 - mean_absolute_error: 0.0671 - val_loss: 0.0101 - val_mean_absolute_error: 0.0722\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0672 - val_loss: 0.0105 - val_mean_absolute_error: 0.0738\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0671 - val_loss: 0.0101 - val_mean_absolute_error: 0.0728\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0104 - val_mean_absolute_error: 0.0732\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0671 - val_loss: 0.0104 - val_mean_absolute_error: 0.0734\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0673 - val_loss: 0.0103 - val_mean_absolute_error: 0.0737\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0084 - mean_absolute_error: 0.0672 - val_loss: 0.0100 - val_mean_absolute_error: 0.0723\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0674 - val_loss: 0.0102 - val_mean_absolute_error: 0.0736\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0671 - val_loss: 0.0102 - val_mean_absolute_error: 0.0727\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0101 - val_mean_absolute_error: 0.0727\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0101 - val_mean_absolute_error: 0.0723\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0666 - val_loss: 0.0104 - val_mean_absolute_error: 0.0736\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0084 - mean_absolute_error: 0.0670 - val_loss: 0.0101 - val_mean_absolute_error: 0.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0657 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0084 - mean_absolute_error: 0.0667 - val_loss: 0.0102 - val_mean_absolute_error: 0.0728\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0102 - val_mean_absolute_error: 0.0734\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0660 - val_loss: 0.0101 - val_mean_absolute_error: 0.0722\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0083 - mean_absolute_error: 0.0669 - val_loss: 0.0104 - val_mean_absolute_error: 0.0729\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0102 - val_mean_absolute_error: 0.0730\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0101 - val_mean_absolute_error: 0.0723\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0100 - val_mean_absolute_error: 0.0727\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0666 - val_loss: 0.0100 - val_mean_absolute_error: 0.0721\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0103 - val_mean_absolute_error: 0.0730\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0664 - val_loss: 0.0102 - val_mean_absolute_error: 0.0725\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0101 - val_mean_absolute_error: 0.0718\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0100 - val_mean_absolute_error: 0.0719\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0663 - val_loss: 0.0100 - val_mean_absolute_error: 0.0723\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0101 - val_mean_absolute_error: 0.0725\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0656 - val_loss: 0.0101 - val_mean_absolute_error: 0.0723\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0661 - val_loss: 0.0103 - val_mean_absolute_error: 0.0729\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0660 - val_loss: 0.0102 - val_mean_absolute_error: 0.0724\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0099 - val_mean_absolute_error: 0.0720\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0660 - val_loss: 0.0102 - val_mean_absolute_error: 0.0726\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0658 - val_loss: 0.0104 - val_mean_absolute_error: 0.0733\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0655 - val_loss: 0.0100 - val_mean_absolute_error: 0.0730\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0101 - val_mean_absolute_error: 0.0725\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0100 - val_mean_absolute_error: 0.0721\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0658 - val_loss: 0.0103 - val_mean_absolute_error: 0.0730\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0658 - val_loss: 0.0100 - val_mean_absolute_error: 0.0719\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0101 - val_mean_absolute_error: 0.0726\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0660 - val_loss: 0.0104 - val_mean_absolute_error: 0.0733\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0656 - val_loss: 0.0103 - val_mean_absolute_error: 0.0728\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0083 - mean_absolute_error: 0.0666 - val_loss: 0.0102 - val_mean_absolute_error: 0.0726\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0661 - val_loss: 0.0105 - val_mean_absolute_error: 0.0743\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0106 - val_mean_absolute_error: 0.0739\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0081 - mean_absolute_error: 0.0661 - val_loss: 0.0103 - val_mean_absolute_error: 0.0736\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0101 - val_mean_absolute_error: 0.0727\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0662 - val_loss: 0.0102 - val_mean_absolute_error: 0.0732\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0102 - val_mean_absolute_error: 0.0725\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0106 - val_mean_absolute_error: 0.0752\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0100 - val_mean_absolute_error: 0.0720\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0660 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0660 - val_loss: 0.0102 - val_mean_absolute_error: 0.0721\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0081 - mean_absolute_error: 0.0657 - val_loss: 0.0104 - val_mean_absolute_error: 0.0731\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0103 - val_mean_absolute_error: 0.0734\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0082 - mean_absolute_error: 0.0660 - val_loss: 0.0100 - val_mean_absolute_error: 0.0723\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0653 - val_loss: 0.0103 - val_mean_absolute_error: 0.0731\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0103 - val_mean_absolute_error: 0.0728\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0655 - val_loss: 0.0102 - val_mean_absolute_error: 0.0721\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0653 - val_loss: 0.0101 - val_mean_absolute_error: 0.0723\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 126us/step\n",
      "yhat shape:  (8760, 1)\n",
      "First 2 scaled predictions\n",
      "[[-121.24637]\n",
      " [ -97.03969]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    392.753632\n",
      "2011-01-01 01:00:00    232.960312\n",
      "2011-01-01 02:00:00    267.350594\n",
      "2011-01-01 03:00:00    189.612785\n",
      "2011-01-01 04:00:00    100.095306\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: validation_set: 90.81199657088328\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 29us/step\n",
      "yhat shape:  (8784, 1)\n",
      "First 2 scaled predictions\n",
      "[[-23.754246]\n",
      " [-40.69924 ]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    522.245754\n",
      "2012-01-01 01:00:00    411.300758\n",
      "2012-01-01 02:00:00    428.213615\n",
      "2012-01-01 03:00:00    292.112782\n",
      "2012-01-01 04:00:00    171.482316\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012: 89.13143071351512\n",
      "### START Experiment with  UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 1)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 16s 942us/step - loss: 0.0407 - val_loss: 0.0425\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0352 - val_loss: 0.0375\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0316 - val_loss: 0.0333\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0303 - val_loss: 0.0343\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0291 - val_loss: 0.0295\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0286 - val_loss: 0.0328\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0273 - val_loss: 0.0276\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0262 - val_loss: 0.0272\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0252 - val_loss: 0.0265\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0234 - val_loss: 0.0250\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0229 - val_loss: 0.0262\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0220 - val_loss: 0.0227\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0218 - val_loss: 0.0257\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 13s 769us/step - loss: 0.0207 - val_loss: 0.0207\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0179 - val_loss: 0.0188\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0176 - val_loss: 0.0179\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0168 - val_loss: 0.0211\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0165 - val_loss: 0.0165\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0153 - val_loss: 0.0159\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0156 - val_loss: 0.0164\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 13s 757us/step - loss: 0.0144 - val_loss: 0.0158\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0160 - val_loss: 0.0158\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0136 - val_loss: 0.0127\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0129 - val_loss: 0.0122\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0125 - val_loss: 0.0118\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0123 - val_loss: 0.0127\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 13s 759us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0101 - val_loss: 0.0106\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0103 - val_loss: 0.0105\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0097 - val_loss: 0.0110\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 13s 760us/step - loss: 0.0094 - val_loss: 0.0097\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0094 - val_loss: 0.0109\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0087 - val_loss: 0.0096\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0088 - val_loss: 0.0094\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0089 - val_loss: 0.0091\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0083 - val_loss: 0.0088\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0084 - val_loss: 0.0090\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0083 - val_loss: 0.0089\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0080 - val_loss: 0.0084\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0078 - val_loss: 0.0088\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0081 - val_loss: 0.0085\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 13s 769us/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 13s 759us/step - loss: 0.0078 - val_loss: 0.0084\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0076 - val_loss: 0.0080\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0074 - val_loss: 0.0079\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0075 - val_loss: 0.0091\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0073 - val_loss: 0.0084\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 13s 760us/step - loss: 0.0073 - val_loss: 0.0084\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0070 - val_loss: 0.0079\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0070 - val_loss: 0.0081\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 13s 769us/step - loss: 0.0072 - val_loss: 0.0082\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0067 - val_loss: 0.0079\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0068 - val_loss: 0.0082\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0066 - val_loss: 0.0081\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0074 - val_loss: 0.0083\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0070 - val_loss: 0.0082\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0066 - val_loss: 0.0091\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0065 - val_loss: 0.0078\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0062 - val_loss: 0.0077\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0071 - val_loss: 0.0084\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0069 - val_loss: 0.0077\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0062 - val_loss: 0.0076\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0070 - val_loss: 0.0105\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0060 - val_loss: 0.0078\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 13s 760us/step - loss: 0.0059 - val_loss: 0.0079\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0061 - val_loss: 0.0077\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0058 - val_loss: 0.0080\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0059 - val_loss: 0.0095\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0059 - val_loss: 0.0079\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0057 - val_loss: 0.0079\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0100 - val_loss: 0.0082\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0058 - val_loss: 0.0079\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0058 - val_loss: 0.0078\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0057 - val_loss: 0.0076\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0057 - val_loss: 0.0076\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0057 - val_loss: 0.0082\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0055 - val_loss: 0.0080\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0054 - val_loss: 0.0078\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 13s 767us/step - loss: 0.0053 - val_loss: 0.0078\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0052 - val_loss: 0.0078\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 13s 759us/step - loss: 0.0055 - val_loss: 0.0081\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0057 - val_loss: 0.0081\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0056 - val_loss: 0.0084\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0054 - val_loss: 0.0078\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0053 - val_loss: 0.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0058 - val_loss: 0.0080\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0058 - val_loss: 0.0079\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0052 - val_loss: 0.0077\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0051 - val_loss: 0.0079\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 13s 760us/step - loss: 0.0052 - val_loss: 0.0083\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 13s 761us/step - loss: 0.0052 - val_loss: 0.0078\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 13s 759us/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0054 - val_loss: 0.0102\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 13s 765us/step - loss: 0.0056 - val_loss: 0.0079\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0050 - val_loss: 0.0079\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0050 - val_loss: 0.0081\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0049 - val_loss: 0.0079\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 13s 764us/step - loss: 0.0050 - val_loss: 0.0080\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0048 - val_loss: 0.0080\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 13s 768us/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 13s 766us/step - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 13s 763us/step - loss: 0.0050 - val_loss: 0.0082\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 512)\n",
      "X_valid_encoded shape:  (8760, 512)\n",
      "X_test_encoded shape:  (8784, 512)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 3s 161us/step - loss: 0.0652 - mean_absolute_error: 0.1933 - val_loss: 0.0153 - val_mean_absolute_error: 0.0930\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0272 - mean_absolute_error: 0.1276 - val_loss: 0.0135 - val_mean_absolute_error: 0.0873\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0210 - mean_absolute_error: 0.1121 - val_loss: 0.0121 - val_mean_absolute_error: 0.0827\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0181 - mean_absolute_error: 0.1036 - val_loss: 0.0111 - val_mean_absolute_error: 0.0792\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0164 - mean_absolute_error: 0.0980 - val_loss: 0.0112 - val_mean_absolute_error: 0.0788\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0151 - mean_absolute_error: 0.0936 - val_loss: 0.0103 - val_mean_absolute_error: 0.0760\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0904 - val_loss: 0.0103 - val_mean_absolute_error: 0.0760\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0870 - val_loss: 0.0103 - val_mean_absolute_error: 0.0756\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0127 - mean_absolute_error: 0.0856 - val_loss: 0.0100 - val_mean_absolute_error: 0.0742\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0123 - mean_absolute_error: 0.0839 - val_loss: 0.0102 - val_mean_absolute_error: 0.0744\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0119 - mean_absolute_error: 0.0824 - val_loss: 0.0102 - val_mean_absolute_error: 0.0751\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0117 - mean_absolute_error: 0.0812 - val_loss: 0.0098 - val_mean_absolute_error: 0.0731\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0111 - mean_absolute_error: 0.0794 - val_loss: 0.0099 - val_mean_absolute_error: 0.0726\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0112 - mean_absolute_error: 0.0794 - val_loss: 0.0098 - val_mean_absolute_error: 0.0736\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0111 - mean_absolute_error: 0.0795 - val_loss: 0.0101 - val_mean_absolute_error: 0.0755\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0109 - mean_absolute_error: 0.0782 - val_loss: 0.0096 - val_mean_absolute_error: 0.0725\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0107 - mean_absolute_error: 0.0772 - val_loss: 0.0096 - val_mean_absolute_error: 0.0728\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0103 - mean_absolute_error: 0.0759 - val_loss: 0.0096 - val_mean_absolute_error: 0.0720\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0102 - mean_absolute_error: 0.0753 - val_loss: 0.0093 - val_mean_absolute_error: 0.0716\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0102 - mean_absolute_error: 0.0750 - val_loss: 0.0097 - val_mean_absolute_error: 0.0720\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0102 - mean_absolute_error: 0.0747 - val_loss: 0.0096 - val_mean_absolute_error: 0.0720\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0099 - mean_absolute_error: 0.0738 - val_loss: 0.0093 - val_mean_absolute_error: 0.0712\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0100 - mean_absolute_error: 0.0741 - val_loss: 0.0093 - val_mean_absolute_error: 0.0706\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0098 - mean_absolute_error: 0.0735 - val_loss: 0.0094 - val_mean_absolute_error: 0.0724\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0097 - mean_absolute_error: 0.0732 - val_loss: 0.0093 - val_mean_absolute_error: 0.0712\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0097 - mean_absolute_error: 0.0728 - val_loss: 0.0094 - val_mean_absolute_error: 0.0715\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0096 - mean_absolute_error: 0.0723 - val_loss: 0.0093 - val_mean_absolute_error: 0.0710\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0095 - mean_absolute_error: 0.0722 - val_loss: 0.0091 - val_mean_absolute_error: 0.0702\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0096 - mean_absolute_error: 0.0722 - val_loss: 0.0094 - val_mean_absolute_error: 0.0719\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0096 - mean_absolute_error: 0.0720 - val_loss: 0.0097 - val_mean_absolute_error: 0.0726\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0096 - mean_absolute_error: 0.0723 - val_loss: 0.0092 - val_mean_absolute_error: 0.0709\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0092 - mean_absolute_error: 0.0709 - val_loss: 0.0091 - val_mean_absolute_error: 0.0703\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0712 - val_loss: 0.0091 - val_mean_absolute_error: 0.0695\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0093 - mean_absolute_error: 0.0711 - val_loss: 0.0094 - val_mean_absolute_error: 0.0712\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0091 - mean_absolute_error: 0.0704 - val_loss: 0.0093 - val_mean_absolute_error: 0.0710\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0092 - mean_absolute_error: 0.0710 - val_loss: 0.0090 - val_mean_absolute_error: 0.0699\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0092 - mean_absolute_error: 0.0711 - val_loss: 0.0091 - val_mean_absolute_error: 0.0699\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0695 - val_loss: 0.0094 - val_mean_absolute_error: 0.0712\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0091 - mean_absolute_error: 0.0702 - val_loss: 0.0094 - val_mean_absolute_error: 0.0713\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0090 - mean_absolute_error: 0.0699 - val_loss: 0.0093 - val_mean_absolute_error: 0.0711\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0695 - val_loss: 0.0090 - val_mean_absolute_error: 0.0694\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0689 - val_loss: 0.0091 - val_mean_absolute_error: 0.0699\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0089 - mean_absolute_error: 0.0695 - val_loss: 0.0092 - val_mean_absolute_error: 0.0706\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0089 - mean_absolute_error: 0.0694 - val_loss: 0.0093 - val_mean_absolute_error: 0.0709\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0690 - val_loss: 0.0091 - val_mean_absolute_error: 0.0704\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0690 - val_loss: 0.0091 - val_mean_absolute_error: 0.0697\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0687 - val_loss: 0.0094 - val_mean_absolute_error: 0.0719\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0090 - mean_absolute_error: 0.0699 - val_loss: 0.0095 - val_mean_absolute_error: 0.0709\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0089 - mean_absolute_error: 0.0689 - val_loss: 0.0091 - val_mean_absolute_error: 0.0698\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0686 - val_loss: 0.0091 - val_mean_absolute_error: 0.0698\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0688 - val_loss: 0.0092 - val_mean_absolute_error: 0.0702\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0087 - mean_absolute_error: 0.0685 - val_loss: 0.0095 - val_mean_absolute_error: 0.0726\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0686 - val_loss: 0.0091 - val_mean_absolute_error: 0.0699\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0689 - val_loss: 0.0090 - val_mean_absolute_error: 0.0689\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0087 - mean_absolute_error: 0.0683 - val_loss: 0.0098 - val_mean_absolute_error: 0.0725\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0088 - mean_absolute_error: 0.0688 - val_loss: 0.0091 - val_mean_absolute_error: 0.0698\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0087 - mean_absolute_error: 0.0685 - val_loss: 0.0092 - val_mean_absolute_error: 0.0704\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0086 - mean_absolute_error: 0.0681 - val_loss: 0.0092 - val_mean_absolute_error: 0.0702\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0676 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0681 - val_loss: 0.0091 - val_mean_absolute_error: 0.0695\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0678 - val_loss: 0.0090 - val_mean_absolute_error: 0.0691\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0679 - val_loss: 0.0091 - val_mean_absolute_error: 0.0701\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0679 - val_loss: 0.0091 - val_mean_absolute_error: 0.0695\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0679 - val_loss: 0.0091 - val_mean_absolute_error: 0.0696\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0681 - val_loss: 0.0091 - val_mean_absolute_error: 0.0699\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0677 - val_loss: 0.0090 - val_mean_absolute_error: 0.0698\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0682 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0085 - mean_absolute_error: 0.0680 - val_loss: 0.0095 - val_mean_absolute_error: 0.0720\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0677 - val_loss: 0.0091 - val_mean_absolute_error: 0.0701\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0673 - val_loss: 0.0093 - val_mean_absolute_error: 0.0699\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0671 - val_loss: 0.0090 - val_mean_absolute_error: 0.0693\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0663 - val_loss: 0.0091 - val_mean_absolute_error: 0.0702\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0091 - val_mean_absolute_error: 0.0697\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0670 - val_loss: 0.0095 - val_mean_absolute_error: 0.0714\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0085 - mean_absolute_error: 0.0673 - val_loss: 0.0089 - val_mean_absolute_error: 0.0694\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0085 - mean_absolute_error: 0.0674 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0669 - val_loss: 0.0090 - val_mean_absolute_error: 0.0695\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0092 - val_mean_absolute_error: 0.0703\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0672 - val_loss: 0.0091 - val_mean_absolute_error: 0.0698\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0083 - mean_absolute_error: 0.0670 - val_loss: 0.0090 - val_mean_absolute_error: 0.0700\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0669 - val_loss: 0.0096 - val_mean_absolute_error: 0.0714\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0091 - val_mean_absolute_error: 0.0694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0674 - val_loss: 0.0092 - val_mean_absolute_error: 0.0703\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0092 - val_mean_absolute_error: 0.0697\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0099 - val_mean_absolute_error: 0.0727\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0086 - mean_absolute_error: 0.0678 - val_loss: 0.0092 - val_mean_absolute_error: 0.0699\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0670 - val_loss: 0.0092 - val_mean_absolute_error: 0.0704\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0666 - val_loss: 0.0089 - val_mean_absolute_error: 0.0690\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0094 - val_mean_absolute_error: 0.0705\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0671 - val_loss: 0.0089 - val_mean_absolute_error: 0.0695\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0671 - val_loss: 0.0089 - val_mean_absolute_error: 0.0691\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0666 - val_loss: 0.0088 - val_mean_absolute_error: 0.0690\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0666 - val_loss: 0.0092 - val_mean_absolute_error: 0.0698\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0090 - val_mean_absolute_error: 0.0696\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0672 - val_loss: 0.0090 - val_mean_absolute_error: 0.0693\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0662 - val_loss: 0.0089 - val_mean_absolute_error: 0.0694\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0090 - val_mean_absolute_error: 0.0701\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0673 - val_loss: 0.0093 - val_mean_absolute_error: 0.0707\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0090 - val_mean_absolute_error: 0.0693\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0660 - val_loss: 0.0090 - val_mean_absolute_error: 0.0695\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0091 - val_mean_absolute_error: 0.0700\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0662 - val_loss: 0.0091 - val_mean_absolute_error: 0.0704\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0094 - val_mean_absolute_error: 0.0715\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0092 - val_mean_absolute_error: 0.0701\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0090 - val_mean_absolute_error: 0.0692\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0668 - val_loss: 0.0089 - val_mean_absolute_error: 0.0692\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0668 - val_loss: 0.0090 - val_mean_absolute_error: 0.0689\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0096 - val_mean_absolute_error: 0.0715\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0091 - val_mean_absolute_error: 0.0696\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0657 - val_loss: 0.0090 - val_mean_absolute_error: 0.0690\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0659 - val_loss: 0.0090 - val_mean_absolute_error: 0.0689\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0090 - val_mean_absolute_error: 0.0695\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0091 - val_mean_absolute_error: 0.0693\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0090 - val_mean_absolute_error: 0.0694\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0661 - val_loss: 0.0092 - val_mean_absolute_error: 0.0696\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0087 - val_mean_absolute_error: 0.0684\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0084 - mean_absolute_error: 0.0667 - val_loss: 0.0089 - val_mean_absolute_error: 0.0690\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0657 - val_loss: 0.0090 - val_mean_absolute_error: 0.0698\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0663 - val_loss: 0.0090 - val_mean_absolute_error: 0.0689\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0655 - val_loss: 0.0089 - val_mean_absolute_error: 0.0695\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0091 - val_mean_absolute_error: 0.0695\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0090 - val_mean_absolute_error: 0.0702\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0659 - val_loss: 0.0092 - val_mean_absolute_error: 0.0695\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0667 - val_loss: 0.0092 - val_mean_absolute_error: 0.0698\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0669 - val_loss: 0.0090 - val_mean_absolute_error: 0.0690\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0654 - val_loss: 0.0090 - val_mean_absolute_error: 0.0695\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0662 - val_loss: 0.0089 - val_mean_absolute_error: 0.0695\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0092 - val_mean_absolute_error: 0.0701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0656 - val_loss: 0.0089 - val_mean_absolute_error: 0.0694\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0660 - val_loss: 0.0089 - val_mean_absolute_error: 0.0686\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0081 - mean_absolute_error: 0.0654 - val_loss: 0.0095 - val_mean_absolute_error: 0.0713\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0654 - val_loss: 0.0089 - val_mean_absolute_error: 0.0688\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0665 - val_loss: 0.0091 - val_mean_absolute_error: 0.0699\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0083 - mean_absolute_error: 0.0663 - val_loss: 0.0095 - val_mean_absolute_error: 0.0709\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0657 - val_loss: 0.0089 - val_mean_absolute_error: 0.0684\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0080 - mean_absolute_error: 0.0654 - val_loss: 0.0088 - val_mean_absolute_error: 0.0685\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0652 - val_loss: 0.0090 - val_mean_absolute_error: 0.0696\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0664 - val_loss: 0.0090 - val_mean_absolute_error: 0.0689\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0659 - val_loss: 0.0089 - val_mean_absolute_error: 0.0691\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0663 - val_loss: 0.0090 - val_mean_absolute_error: 0.0694\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0656 - val_loss: 0.0095 - val_mean_absolute_error: 0.0708\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0082 - mean_absolute_error: 0.0660 - val_loss: 0.0088 - val_mean_absolute_error: 0.0688\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0078 - mean_absolute_error: 0.0646 - val_loss: 0.0090 - val_mean_absolute_error: 0.0694\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0080 - mean_absolute_error: 0.0657 - val_loss: 0.0090 - val_mean_absolute_error: 0.0697\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0657 - val_loss: 0.0089 - val_mean_absolute_error: 0.0691\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0659 - val_loss: 0.0091 - val_mean_absolute_error: 0.0701\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0081 - mean_absolute_error: 0.0657 - val_loss: 0.0089 - val_mean_absolute_error: 0.0687\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0079 - mean_absolute_error: 0.0651 - val_loss: 0.0089 - val_mean_absolute_error: 0.0687\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 137us/step\n",
      "yhat shape:  (8760, 1)\n",
      "First 2 scaled predictions\n",
      "[[-70.02654 ]\n",
      " [-42.086254]]\n",
      "Shape of predictions: (8760, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2011-01-01 00:00:00    443.973457\n",
      "2011-01-01 01:00:00    287.913746\n",
      "2011-01-01 02:00:00    325.734478\n",
      "2011-01-01 03:00:00    242.496773\n",
      "2011-01-01 04:00:00    155.062434\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: validation_set: 85.4524358384205\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 30us/step\n",
      "yhat shape:  (8784, 1)\n",
      "First 2 scaled predictions\n",
      "[[-79.496155]\n",
      " [-83.97404 ]]\n",
      "Shape of predictions: (8784, 1)\n",
      "Invert Differencing of predictions...\n",
      "predictions preview:\n",
      "date\n",
      "2012-01-01 00:00:00    466.503845\n",
      "2012-01-01 01:00:00    368.025963\n",
      "2012-01-01 02:00:00    399.162102\n",
      "2012-01-01 03:00:00    250.702957\n",
      "2012-01-01 04:00:00    125.987602\n",
      "Freq: H, Name: 237, dtype: float64\n",
      "RMSE for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012: 87.17030189674676\n",
      "### START Experiment with  UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area161 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area230 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area79 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area236 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area162 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area170 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area234 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area48 is prepared...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area186 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 10)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 13s 750us/step - loss: 0.0368 - val_loss: 0.0335\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0292 - val_loss: 0.0302\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0266 - val_loss: 0.0275\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0241 - val_loss: 0.0248\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 9s 542us/step - loss: 0.0223 - val_loss: 0.0229\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0208 - val_loss: 0.0213\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 9s 540us/step - loss: 0.0199 - val_loss: 0.0210\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0188 - val_loss: 0.0190\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0178 - val_loss: 0.0179\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0168 - val_loss: 0.0175\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0150 - val_loss: 0.0152\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0142 - val_loss: 0.0148\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0140 - val_loss: 0.0143\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0129 - val_loss: 0.0134\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0127 - val_loss: 0.0135\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0126 - val_loss: 0.0127\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0121 - val_loss: 0.0125\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0115 - val_loss: 0.0122\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0111 - val_loss: 0.0118\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 10s 565us/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0108 - val_loss: 0.0120\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0107 - val_loss: 0.0115\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0106 - val_loss: 0.0114\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0105 - val_loss: 0.0114\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0105 - val_loss: 0.0117\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0100 - val_loss: 0.0111\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0100 - val_loss: 0.0108\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0099 - val_loss: 0.0111\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0095 - val_loss: 0.0106\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0094 - val_loss: 0.0104\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0094 - val_loss: 0.0105\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0093 - val_loss: 0.0103\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0097 - val_loss: 0.0117\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0093 - val_loss: 0.0103\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0091 - val_loss: 0.0104\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 10s 560us/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0089 - val_loss: 0.0102\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0088 - val_loss: 0.0101\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0087 - val_loss: 0.0103\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0086 - val_loss: 0.0101\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0085 - val_loss: 0.0098\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0085 - val_loss: 0.0101\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0084 - val_loss: 0.0098\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0083 - val_loss: 0.0100\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0082 - val_loss: 0.0098\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 10s 561us/step - loss: 0.0082 - val_loss: 0.0098\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0082 - val_loss: 0.0097\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0081 - val_loss: 0.0097\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0081 - val_loss: 0.0097\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0080 - val_loss: 0.0099\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0079 - val_loss: 0.0097\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 9s 544us/step - loss: 0.0079 - val_loss: 0.0097\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0078 - val_loss: 0.0099\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0078 - val_loss: 0.0101\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0078 - val_loss: 0.0098\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0078 - val_loss: 0.0096\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0077 - val_loss: 0.0097\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0077 - val_loss: 0.0098\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0077 - val_loss: 0.0100\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0077 - val_loss: 0.0099\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0078 - val_loss: 0.0100\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0079 - val_loss: 0.0096\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0076 - val_loss: 0.0096\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0076 - val_loss: 0.0096\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0075 - val_loss: 0.0097\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0075 - val_loss: 0.0098\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0075 - val_loss: 0.0097\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0074 - val_loss: 0.0099\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0073 - val_loss: 0.0100\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0074 - val_loss: 0.0100\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0073 - val_loss: 0.0098\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 9s 539us/step - loss: 0.0072 - val_loss: 0.0099\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0072 - val_loss: 0.0099\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0072 - val_loss: 0.0099\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0077 - val_loss: 0.0103\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0076 - val_loss: 0.0097\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0072 - val_loss: 0.0102\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 9s 542us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0070 - val_loss: 0.0099\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0070 - val_loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0078 - val_loss: 0.0099\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 10s 563us/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0070 - val_loss: 0.0096\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 10s 557us/step - loss: 0.0069 - val_loss: 0.0099\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 256)\n",
      "X_valid_encoded shape:  (8760, 256)\n",
      "X_test_encoded shape:  (8784, 256)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 3s 174us/step - loss: 0.1334 - mean_absolute_error: 0.2804 - val_loss: 0.0263 - val_mean_absolute_error: 0.1216\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0539 - mean_absolute_error: 0.1804 - val_loss: 0.0211 - val_mean_absolute_error: 0.1066\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0379 - mean_absolute_error: 0.1501 - val_loss: 0.0192 - val_mean_absolute_error: 0.1006\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0311 - mean_absolute_error: 0.1351 - val_loss: 0.0184 - val_mean_absolute_error: 0.0980\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0277 - mean_absolute_error: 0.1265 - val_loss: 0.0173 - val_mean_absolute_error: 0.0946\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0252 - mean_absolute_error: 0.1200 - val_loss: 0.0169 - val_mean_absolute_error: 0.0932\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0234 - mean_absolute_error: 0.1154 - val_loss: 0.0166 - val_mean_absolute_error: 0.0922\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0222 - mean_absolute_error: 0.1118 - val_loss: 0.0162 - val_mean_absolute_error: 0.0911\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0211 - mean_absolute_error: 0.1086 - val_loss: 0.0160 - val_mean_absolute_error: 0.0900\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0204 - mean_absolute_error: 0.1062 - val_loss: 0.0158 - val_mean_absolute_error: 0.0896\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0197 - mean_absolute_error: 0.1043 - val_loss: 0.0155 - val_mean_absolute_error: 0.0885\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0190 - mean_absolute_error: 0.1024 - val_loss: 0.0155 - val_mean_absolute_error: 0.0883\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0187 - mean_absolute_error: 0.1011 - val_loss: 0.0152 - val_mean_absolute_error: 0.0876\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0182 - mean_absolute_error: 0.0998 - val_loss: 0.0154 - val_mean_absolute_error: 0.0880\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0178 - mean_absolute_error: 0.0982 - val_loss: 0.0151 - val_mean_absolute_error: 0.0875\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0176 - mean_absolute_error: 0.0975 - val_loss: 0.0150 - val_mean_absolute_error: 0.0868\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0173 - mean_absolute_error: 0.0969 - val_loss: 0.0149 - val_mean_absolute_error: 0.0869\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0170 - mean_absolute_error: 0.0958 - val_loss: 0.0147 - val_mean_absolute_error: 0.0861\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0167 - mean_absolute_error: 0.0948 - val_loss: 0.0147 - val_mean_absolute_error: 0.0859\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0166 - mean_absolute_error: 0.0943 - val_loss: 0.0147 - val_mean_absolute_error: 0.0861\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0163 - mean_absolute_error: 0.0936 - val_loss: 0.0147 - val_mean_absolute_error: 0.0859\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0163 - mean_absolute_error: 0.0932 - val_loss: 0.0146 - val_mean_absolute_error: 0.0859\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0161 - mean_absolute_error: 0.0930 - val_loss: 0.0145 - val_mean_absolute_error: 0.0856\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0160 - mean_absolute_error: 0.0923 - val_loss: 0.0146 - val_mean_absolute_error: 0.0859\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0158 - mean_absolute_error: 0.0917 - val_loss: 0.0145 - val_mean_absolute_error: 0.0856\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0158 - mean_absolute_error: 0.0916 - val_loss: 0.0145 - val_mean_absolute_error: 0.0856\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0156 - mean_absolute_error: 0.0910 - val_loss: 0.0144 - val_mean_absolute_error: 0.0851\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0155 - mean_absolute_error: 0.0907 - val_loss: 0.0143 - val_mean_absolute_error: 0.0848\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0154 - mean_absolute_error: 0.0904 - val_loss: 0.0144 - val_mean_absolute_error: 0.0851\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0154 - mean_absolute_error: 0.0902 - val_loss: 0.0144 - val_mean_absolute_error: 0.0855\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0152 - mean_absolute_error: 0.0896 - val_loss: 0.0141 - val_mean_absolute_error: 0.0844\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0152 - mean_absolute_error: 0.0895 - val_loss: 0.0142 - val_mean_absolute_error: 0.0846\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0151 - mean_absolute_error: 0.0892 - val_loss: 0.0141 - val_mean_absolute_error: 0.0845\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0151 - mean_absolute_error: 0.0892 - val_loss: 0.0142 - val_mean_absolute_error: 0.0849\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0149 - mean_absolute_error: 0.0886 - val_loss: 0.0141 - val_mean_absolute_error: 0.0843\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0149 - mean_absolute_error: 0.0887 - val_loss: 0.0140 - val_mean_absolute_error: 0.0842\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0148 - mean_absolute_error: 0.0886 - val_loss: 0.0141 - val_mean_absolute_error: 0.0842\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0148 - mean_absolute_error: 0.0881 - val_loss: 0.0141 - val_mean_absolute_error: 0.0846\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0147 - mean_absolute_error: 0.0880 - val_loss: 0.0141 - val_mean_absolute_error: 0.0841\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0145 - mean_absolute_error: 0.0875 - val_loss: 0.0140 - val_mean_absolute_error: 0.0840\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0146 - mean_absolute_error: 0.0876 - val_loss: 0.0139 - val_mean_absolute_error: 0.0837\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0145 - mean_absolute_error: 0.0872 - val_loss: 0.0138 - val_mean_absolute_error: 0.0834\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0145 - mean_absolute_error: 0.0873 - val_loss: 0.0139 - val_mean_absolute_error: 0.0836\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0145 - mean_absolute_error: 0.0873 - val_loss: 0.0139 - val_mean_absolute_error: 0.0837\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0145 - mean_absolute_error: 0.0870 - val_loss: 0.0140 - val_mean_absolute_error: 0.0840\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0144 - mean_absolute_error: 0.0868 - val_loss: 0.0141 - val_mean_absolute_error: 0.0842\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0144 - mean_absolute_error: 0.0868 - val_loss: 0.0139 - val_mean_absolute_error: 0.0836\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0144 - mean_absolute_error: 0.0866 - val_loss: 0.0140 - val_mean_absolute_error: 0.0841\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0143 - mean_absolute_error: 0.0866 - val_loss: 0.0139 - val_mean_absolute_error: 0.0839\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0863 - val_loss: 0.0139 - val_mean_absolute_error: 0.0839\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0862 - val_loss: 0.0138 - val_mean_absolute_error: 0.0834\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0862 - val_loss: 0.0139 - val_mean_absolute_error: 0.0836\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0859 - val_loss: 0.0138 - val_mean_absolute_error: 0.0835\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0861 - val_loss: 0.0138 - val_mean_absolute_error: 0.0836\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0859 - val_loss: 0.0138 - val_mean_absolute_error: 0.0835\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0860 - val_loss: 0.0137 - val_mean_absolute_error: 0.0833\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0858 - val_loss: 0.0138 - val_mean_absolute_error: 0.0835\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0857 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0854 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0858 - val_loss: 0.0139 - val_mean_absolute_error: 0.0838\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0856 - val_loss: 0.0138 - val_mean_absolute_error: 0.0833\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0855 - val_loss: 0.0137 - val_mean_absolute_error: 0.0834\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0855 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0140 - mean_absolute_error: 0.0854 - val_loss: 0.0138 - val_mean_absolute_error: 0.0833\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0852 - val_loss: 0.0138 - val_mean_absolute_error: 0.0834\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0855 - val_loss: 0.0138 - val_mean_absolute_error: 0.0834\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0852 - val_loss: 0.0136 - val_mean_absolute_error: 0.0831\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0853 - val_loss: 0.0138 - val_mean_absolute_error: 0.0837\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0852 - val_loss: 0.0137 - val_mean_absolute_error: 0.0835\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0849 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0849 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0848 - val_loss: 0.0138 - val_mean_absolute_error: 0.0834\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0849 - val_loss: 0.0137 - val_mean_absolute_error: 0.0834\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0846 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0847 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0852 - val_loss: 0.0136 - val_mean_absolute_error: 0.0834\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0847 - val_loss: 0.0137 - val_mean_absolute_error: 0.0833\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0845 - val_loss: 0.0136 - val_mean_absolute_error: 0.0831\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0846 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0842 - val_loss: 0.0136 - val_mean_absolute_error: 0.0830\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0846 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0137 - mean_absolute_error: 0.0844 - val_loss: 0.0136 - val_mean_absolute_error: 0.0831\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0849 - val_loss: 0.0137 - val_mean_absolute_error: 0.0834\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0844 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0842 - val_loss: 0.0137 - val_mean_absolute_error: 0.0835\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0844 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0844 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0841 - val_loss: 0.0136 - val_mean_absolute_error: 0.0831\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0844 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0844 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0844 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0841 - val_loss: 0.0136 - val_mean_absolute_error: 0.0830\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0844 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0136 - mean_absolute_error: 0.0842 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0840 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0843 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0841 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0841 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0837 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0841 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0836 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0840 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0829\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0136 - val_mean_absolute_error: 0.0832\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0136 - val_mean_absolute_error: 0.0828\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0136 - val_mean_absolute_error: 0.0834\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0136 - val_mean_absolute_error: 0.0831\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0837 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0839 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0133 - val_mean_absolute_error: 0.0820\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0835 - val_loss: 0.0133 - val_mean_absolute_error: 0.0821\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0838 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0821\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0837 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0835 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0134 - val_mean_absolute_error: 0.0821\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 154us/step\n",
      "yhat shape:  (8760, 10)\n",
      "First 2 scaled predictions\n",
      "[[ -96.7253    -68.24697   -60.84225   -58.851357  -87.536     -91.91255\n",
      "  -104.17264   -71.32411   -67.99357   -57.563667]\n",
      " [ -69.37604   -45.981472  -40.94819   -90.9812    -58.93825   -78.91493\n",
      "   -98.33778   -69.19321   -60.104256  -34.604275]]\n",
      "Shape of predictions: (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  417.274696  302.753029  135.157749  1109.148643   \n",
      "2011-01-01 01:00:00  260.623962  273.018528    5.051811   506.018799   \n",
      "2011-01-01 02:00:00  307.550011  198.858984   78.701275   536.628544   \n",
      "2011-01-01 03:00:00  224.870152  172.790693   77.422387   426.330238   \n",
      "2011-01-01 04:00:00  129.025352  162.040348  116.467412   451.536064   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  480.463997  349.087448  451.827362  575.675888   \n",
      "2011-01-01 01:00:00  276.061749  182.085068  354.662216  369.806793   \n",
      "2011-01-01 02:00:00  348.928097  265.277332  439.187866  268.888580   \n",
      "2011-01-01 03:00:00  267.699909  242.718540  381.306976  144.621662   \n",
      "2011-01-01 04:00:00  140.038168  235.909466  350.426292  155.528061   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  512.006432  295.436333  \n",
      "2011-01-01 01:00:00  296.895744  143.395725  \n",
      "2011-01-01 02:00:00  273.768562  207.605915  \n",
      "2011-01-01 03:00:00  286.857231  163.171629  \n",
      "2011-01-01 04:00:00  376.746788  202.096872  \n",
      "RMSE per TS 0 for model: validation_set: 94.10112173088166\n",
      "RMSE per TS 1 for model: validation_set: 107.88469143779479\n",
      "RMSE per TS 2 for model: validation_set: 104.8665617255941\n",
      "RMSE per TS 3 for model: validation_set: 127.69528776973172\n",
      "RMSE per TS 4 for model: validation_set: 92.95034296664431\n",
      "RMSE per TS 5 for model: validation_set: 96.09189598662512\n",
      "RMSE per TS 6 for model: validation_set: 81.95055236676026\n",
      "RMSE per TS 7 for model: validation_set: 82.1598999650988\n",
      "RMSE per TS 8 for model: validation_set: 89.67640989909862\n",
      "RMSE per TS 9 for model: validation_set: 102.90777584911278\n",
      "Avg.RMSE for multivariate model: validation_set: 98.02845396973422\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 27us/step\n",
      "yhat shape:  (8784, 10)\n",
      "First 2 scaled predictions\n",
      "[[-65.44534   -78.15011    20.627981   68.362465  -43.135086  -85.39424\n",
      "  -59.08151   -26.875652   17.286354   -2.4679568]\n",
      " [-39.738365  -64.62189   -27.334505  -23.41115   -19.460552  -69.565636\n",
      "  -59.229015  -44.138332  -23.41629   -14.364788 ]]\n",
      "Shape of predictions: (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  480.554657  398.849892  202.627981  1379.362465   \n",
      "2012-01-01 01:00:00  412.261635  303.378113   70.665495   678.588850   \n",
      "2012-01-01 02:00:00  459.105511  217.333199   25.165817   622.759789   \n",
      "2012-01-01 03:00:00  294.146866  216.023943   -3.169762   514.600273   \n",
      "2012-01-01 04:00:00  139.339491  207.532522  105.139137   395.998749   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  543.864914  369.605759  592.918491  708.124348   \n",
      "2012-01-01 01:00:00  362.539448  239.434364  452.770985  469.861668   \n",
      "2012-01-01 02:00:00  405.827766  298.443501  490.039185  266.318638   \n",
      "2012-01-01 03:00:00  330.187588  307.981438  456.986610  196.301052   \n",
      "2012-01-01 04:00:00  162.730078  215.546009  426.132965  192.573425   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  825.286354  407.532043  \n",
      "2012-01-01 01:00:00  462.583710  230.635212  \n",
      "2012-01-01 02:00:00  338.812527  234.269535  \n",
      "2012-01-01 03:00:00  306.388672  225.429066  \n",
      "2012-01-01 04:00:00  422.092041  176.562263  \n",
      "RMSE per TS 0 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 96.8096085704403\n",
      "RMSE per TS 1 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 108.8062515964907\n",
      "RMSE per TS 2 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 104.65584088986974\n",
      "RMSE per TS 3 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 130.50458587515334\n",
      "RMSE per TS 4 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 96.58208032619339\n",
      "RMSE per TS 5 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 100.43706118878733\n",
      "RMSE per TS 6 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 80.47848873299789\n",
      "RMSE per TS 7 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 86.36289803954385\n",
      "RMSE per TS 8 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 93.71511371959687\n",
      "RMSE per TS 9 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 105.64359438181954\n",
      "Avg.RMSE for multivariate model: UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 100.3995523320893\n",
      "### START Experiment with  UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area161 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area230 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area79 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area236 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area162 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area170 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area234 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area48 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area186 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 10)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 13s 762us/step - loss: 0.0360 - val_loss: 0.0334\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0292 - val_loss: 0.0304\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0269 - val_loss: 0.0277\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0245 - val_loss: 0.0252\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0228 - val_loss: 0.0237\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0213 - val_loss: 0.0220\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 10s 560us/step - loss: 0.0207 - val_loss: 0.0217\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0197 - val_loss: 0.0206\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 9s 540us/step - loss: 0.0175 - val_loss: 0.0178\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0158 - val_loss: 0.0162\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0150 - val_loss: 0.0159\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 9s 542us/step - loss: 0.0143 - val_loss: 0.0149\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0139 - val_loss: 0.0142\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0134 - val_loss: 0.0137\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0129 - val_loss: 0.0135\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0129 - val_loss: 0.0132\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0124 - val_loss: 0.0129\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0123 - val_loss: 0.0128\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0120 - val_loss: 0.0124\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 9s 544us/step - loss: 0.0118 - val_loss: 0.0123\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0116 - val_loss: 0.0124\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0116 - val_loss: 0.0121\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 9s 543us/step - loss: 0.0115 - val_loss: 0.0121\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0114 - val_loss: 0.0121\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0112 - val_loss: 0.0117\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0110 - val_loss: 0.0117\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0109 - val_loss: 0.0115\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0108 - val_loss: 0.0119\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 9s 543us/step - loss: 0.0106 - val_loss: 0.0113\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0106 - val_loss: 0.0112\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0103 - val_loss: 0.0111\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0102 - val_loss: 0.0109\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0099 - val_loss: 0.0109\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0098 - val_loss: 0.0109\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0098 - val_loss: 0.0107\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0097 - val_loss: 0.0109\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0097 - val_loss: 0.0106\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0095 - val_loss: 0.0105\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0094 - val_loss: 0.0105\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0094 - val_loss: 0.0104\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0093 - val_loss: 0.0106\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0093 - val_loss: 0.0104\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 9s 544us/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 10s 558us/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0090 - val_loss: 0.0101\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0089 - val_loss: 0.0102\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0088 - val_loss: 0.0102\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0087 - val_loss: 0.0101\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0085 - val_loss: 0.0100\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0085 - val_loss: 0.0098\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0084 - val_loss: 0.0098\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 9s 544us/step - loss: 0.0084 - val_loss: 0.0099\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0082 - val_loss: 0.0098\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0082 - val_loss: 0.0098\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 9s 544us/step - loss: 0.0082 - val_loss: 0.0099\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0082 - val_loss: 0.0098\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0081 - val_loss: 0.0097\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0081 - val_loss: 0.0097\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0081 - val_loss: 0.0098\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0081 - val_loss: 0.0100\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0079 - val_loss: 0.0099\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0079 - val_loss: 0.0097\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0078 - val_loss: 0.0096\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0079 - val_loss: 0.0096\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0078 - val_loss: 0.0098\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0079 - val_loss: 0.0099\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0078 - val_loss: 0.0097\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0077 - val_loss: 0.0097\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0077 - val_loss: 0.0097\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0077 - val_loss: 0.0098\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 10s 554us/step - loss: 0.0077 - val_loss: 0.0098\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0077 - val_loss: 0.0098\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0077 - val_loss: 0.0097\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0076 - val_loss: 0.0097\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0075 - val_loss: 0.0098\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0075 - val_loss: 0.0097\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0075 - val_loss: 0.0099\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0075 - val_loss: 0.0099\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0075 - val_loss: 0.0097\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0074 - val_loss: 0.0099\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 9s 543us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 10s 551us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0073 - val_loss: 0.0098\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 10s 548us/step - loss: 0.0073 - val_loss: 0.0099\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0073 - val_loss: 0.0097\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0072 - val_loss: 0.0097\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0072 - val_loss: 0.0099\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0072 - val_loss: 0.0097\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 10s 553us/step - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0071 - val_loss: 0.0098\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0070 - val_loss: 0.0099\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 9s 543us/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0071 - val_loss: 0.0097\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 10s 556us/step - loss: 0.0070 - val_loss: 0.0099\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 10s 555us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 9s 539us/step - loss: 0.0070 - val_loss: 0.0099\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0069 - val_loss: 0.0099\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 10s 559us/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 10s 549us/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 9s 546us/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 9s 547us/step - loss: 0.0067 - val_loss: 0.0100\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 10s 552us/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 9s 545us/step - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 9s 541us/step - loss: 0.0067 - val_loss: 0.0098\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 10s 550us/step - loss: 0.0067 - val_loss: 0.0100\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 256)\n",
      "X_valid_encoded shape:  (8760, 256)\n",
      "X_test_encoded shape:  (8784, 256)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 3s 192us/step - loss: 0.1105 - mean_absolute_error: 0.2564 - val_loss: 0.0211 - val_mean_absolute_error: 0.1073\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0459 - mean_absolute_error: 0.1673 - val_loss: 0.0186 - val_mean_absolute_error: 0.0989\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0334 - mean_absolute_error: 0.1412 - val_loss: 0.0175 - val_mean_absolute_error: 0.0956\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0279 - mean_absolute_error: 0.1278 - val_loss: 0.0167 - val_mean_absolute_error: 0.0928\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0248 - mean_absolute_error: 0.1197 - val_loss: 0.0159 - val_mean_absolute_error: 0.0901\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0228 - mean_absolute_error: 0.1140 - val_loss: 0.0155 - val_mean_absolute_error: 0.0888\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0214 - mean_absolute_error: 0.1099 - val_loss: 0.0153 - val_mean_absolute_error: 0.0880\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0202 - mean_absolute_error: 0.1062 - val_loss: 0.0150 - val_mean_absolute_error: 0.0872\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0194 - mean_absolute_error: 0.1042 - val_loss: 0.0149 - val_mean_absolute_error: 0.0867\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0188 - mean_absolute_error: 0.1019 - val_loss: 0.0148 - val_mean_absolute_error: 0.0869\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0182 - mean_absolute_error: 0.1001 - val_loss: 0.0147 - val_mean_absolute_error: 0.0861\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0178 - mean_absolute_error: 0.0987 - val_loss: 0.0145 - val_mean_absolute_error: 0.0858\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0171 - mean_absolute_error: 0.0965 - val_loss: 0.0143 - val_mean_absolute_error: 0.0854\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0169 - mean_absolute_error: 0.0958 - val_loss: 0.0142 - val_mean_absolute_error: 0.0850\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0167 - mean_absolute_error: 0.0951 - val_loss: 0.0144 - val_mean_absolute_error: 0.0852\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0163 - mean_absolute_error: 0.0937 - val_loss: 0.0142 - val_mean_absolute_error: 0.0847\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0162 - mean_absolute_error: 0.0931 - val_loss: 0.0141 - val_mean_absolute_error: 0.0842\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0159 - mean_absolute_error: 0.0926 - val_loss: 0.0142 - val_mean_absolute_error: 0.0844\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0158 - mean_absolute_error: 0.0918 - val_loss: 0.0139 - val_mean_absolute_error: 0.0840\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0154 - mean_absolute_error: 0.0909 - val_loss: 0.0138 - val_mean_absolute_error: 0.0836\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0153 - mean_absolute_error: 0.0903 - val_loss: 0.0139 - val_mean_absolute_error: 0.0838\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0153 - mean_absolute_error: 0.0903 - val_loss: 0.0139 - val_mean_absolute_error: 0.0838\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0152 - mean_absolute_error: 0.0900 - val_loss: 0.0138 - val_mean_absolute_error: 0.0837\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0151 - mean_absolute_error: 0.0895 - val_loss: 0.0139 - val_mean_absolute_error: 0.0842\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0149 - mean_absolute_error: 0.0890 - val_loss: 0.0137 - val_mean_absolute_error: 0.0834\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0149 - mean_absolute_error: 0.0889 - val_loss: 0.0137 - val_mean_absolute_error: 0.0835\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0148 - mean_absolute_error: 0.0886 - val_loss: 0.0138 - val_mean_absolute_error: 0.0835\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0147 - mean_absolute_error: 0.0882 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0146 - mean_absolute_error: 0.0880 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0145 - mean_absolute_error: 0.0876 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0147 - mean_absolute_error: 0.0879 - val_loss: 0.0137 - val_mean_absolute_error: 0.0831\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0145 - mean_absolute_error: 0.0876 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0143 - mean_absolute_error: 0.0870 - val_loss: 0.0136 - val_mean_absolute_error: 0.0827\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0144 - mean_absolute_error: 0.0871 - val_loss: 0.0135 - val_mean_absolute_error: 0.0824\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0143 - mean_absolute_error: 0.0869 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0143 - mean_absolute_error: 0.0867 - val_loss: 0.0135 - val_mean_absolute_error: 0.0829\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0864 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0863 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0142 - mean_absolute_error: 0.0863 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0863 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0859 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0857 - val_loss: 0.0134 - val_mean_absolute_error: 0.0821\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0857 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0855 - val_loss: 0.0133 - val_mean_absolute_error: 0.0823\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0854 - val_loss: 0.0136 - val_mean_absolute_error: 0.0830\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0854 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0851 - val_loss: 0.0134 - val_mean_absolute_error: 0.0829\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0138 - mean_absolute_error: 0.0852 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0139 - mean_absolute_error: 0.0852 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0848 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0848 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0849 - val_loss: 0.0136 - val_mean_absolute_error: 0.0830\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0848 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0846 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0845 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0846 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0137 - mean_absolute_error: 0.0848 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0830\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0136 - mean_absolute_error: 0.0846 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0134 - val_mean_absolute_error: 0.0822\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0133 - val_mean_absolute_error: 0.0821\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0841 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0840 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0134 - mean_absolute_error: 0.0839 - val_loss: 0.0133 - val_mean_absolute_error: 0.0826\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0838 - val_loss: 0.0133 - val_mean_absolute_error: 0.0821\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0838 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0839 - val_loss: 0.0133 - val_mean_absolute_error: 0.0823\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0838 - val_loss: 0.0134 - val_mean_absolute_error: 0.0830\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0134 - mean_absolute_error: 0.0836 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0832 - val_loss: 0.0133 - val_mean_absolute_error: 0.0820\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0133 - val_mean_absolute_error: 0.0828\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0133 - val_mean_absolute_error: 0.0819\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0831 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0834 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0133 - val_mean_absolute_error: 0.0820\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0833 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0831 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0819\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0831 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0817\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 12us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0829 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0829 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0829 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0829 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0131 - val_mean_absolute_error: 0.0817\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0131 - val_mean_absolute_error: 0.0820\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0132 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0825 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0816\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0130 - val_mean_absolute_error: 0.0816\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0826 - val_loss: 0.0131 - val_mean_absolute_error: 0.0814\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0131 - val_mean_absolute_error: 0.0817\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0130 - val_mean_absolute_error: 0.0816\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0131 - val_mean_absolute_error: 0.0814\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0130 - val_mean_absolute_error: 0.0816\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0130 - val_mean_absolute_error: 0.0813\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0131 - val_mean_absolute_error: 0.0820\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0827 - val_loss: 0.0131 - val_mean_absolute_error: 0.0822\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0814\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0823 - val_loss: 0.0131 - val_mean_absolute_error: 0.0817\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0825 - val_loss: 0.0130 - val_mean_absolute_error: 0.0815\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0133 - val_mean_absolute_error: 0.0821\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0824 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0824 - val_loss: 0.0130 - val_mean_absolute_error: 0.0813\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0130 - val_mean_absolute_error: 0.0813\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 1s 165us/step\n",
      "yhat shape:  (8760, 10)\n",
      "First 2 scaled predictions\n",
      "[[-115.306915  -81.11601   -44.744354  -71.03688   -98.63894  -105.80907\n",
      "  -124.40024   -85.24691   -67.30493   -70.40333 ]\n",
      " [-129.9837    -96.24133   -43.343468  -77.86688  -109.003075 -117.14828\n",
      "  -135.98787   -94.9942    -64.3381    -62.68569 ]]\n",
      "Shape of predictions: (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  398.693085  289.883987  151.255646  1096.963120   \n",
      "2011-01-01 01:00:00  200.016296  222.758667    2.656532   519.133118   \n",
      "2011-01-01 02:00:00  286.587570  190.786407   74.950054   538.585789   \n",
      "2011-01-01 03:00:00  173.878326  124.166409   32.685276   438.716965   \n",
      "2011-01-01 04:00:00   84.592155  117.136215   69.264236   461.476059   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  469.361061  335.190933  431.599762  561.753090   \n",
      "2011-01-01 01:00:00  225.996925  143.851723  317.012131  344.005798   \n",
      "2011-01-01 02:00:00  323.849686  271.406334  436.792870  267.470020   \n",
      "2011-01-01 03:00:00  223.758270  220.157677  364.313774  125.365242   \n",
      "2011-01-01 04:00:00  102.006775  216.327538  337.355927  138.088940   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  512.695068  282.596672  \n",
      "2011-01-01 01:00:00  292.661903  115.314308  \n",
      "2011-01-01 02:00:00  265.804688  179.214909  \n",
      "2011-01-01 03:00:00  264.947960  117.602570  \n",
      "2011-01-01 04:00:00  353.306358  156.340462  \n",
      "RMSE per TS 0 for model: validation_set: 94.97262607399601\n",
      "RMSE per TS 1 for model: validation_set: 105.45725229747684\n",
      "RMSE per TS 2 for model: validation_set: 103.17806316534106\n",
      "RMSE per TS 3 for model: validation_set: 126.56302781997424\n",
      "RMSE per TS 4 for model: validation_set: 89.95348076601846\n",
      "RMSE per TS 5 for model: validation_set: 94.20227831375563\n",
      "RMSE per TS 6 for model: validation_set: 80.04197070171294\n",
      "RMSE per TS 7 for model: validation_set: 81.62022494764298\n",
      "RMSE per TS 8 for model: validation_set: 89.29215061861764\n",
      "RMSE per TS 9 for model: validation_set: 101.3242944346867\n",
      "Avg.RMSE for multivariate model: validation_set: 96.66053691392224\n",
      "get predictions of model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8784/8784 [==============================] - 0s 29us/step\n",
      "yhat shape:  (8784, 10)\n",
      "First 2 scaled predictions\n",
      "[[-122.37598   -115.48686    -35.73037    -31.681618  -106.30282\n",
      "  -129.27153   -131.35136    -84.42953    -68.03256   -104.98566  ]\n",
      " [ -98.2394     -76.95021     -6.0432825  -43.997524   -92.91729\n",
      "  -106.42983   -120.900635   -70.59845    -63.338528  -101.472275 ]]\n",
      "Shape of predictions: (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  423.624023  361.513138  146.269630  1279.318382   \n",
      "2012-01-01 01:00:00  353.760597  291.049789   91.956717   658.002476   \n",
      "2012-01-01 02:00:00  446.938019  256.679648  113.106949   637.696030   \n",
      "2012-01-01 03:00:00  305.064337  267.415747   96.445171   540.238098   \n",
      "2012-01-01 04:00:00  148.274597  243.646827  194.216771   427.488174   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  480.697182  325.728470  520.648636  650.570473   \n",
      "2012-01-01 01:00:00  289.082710  202.570168  391.099365  443.401550   \n",
      "2012-01-01 02:00:00  371.015694  298.397903  462.780693  276.745596   \n",
      "2012-01-01 03:00:00  320.130199  320.751802  446.891129  218.932892   \n",
      "2012-01-01 04:00:00  158.300751  222.814707  418.427200  212.798412   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  739.967438  305.014343  \n",
      "2012-01-01 01:00:00  422.661472  143.527725  \n",
      "2012-01-01 02:00:00  338.881107  177.600464  \n",
      "2012-01-01 03:00:00  318.474617  179.264145  \n",
      "2012-01-01 04:00:00  439.520664  142.712654  \n",
      "RMSE per TS 0 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 96.35462182985023\n",
      "RMSE per TS 1 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 106.34452204100134\n",
      "RMSE per TS 2 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 102.59143703865638\n",
      "RMSE per TS 3 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 127.23657663634057\n",
      "RMSE per TS 4 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 92.36264588160758\n",
      "RMSE per TS 5 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 97.58774909974545\n",
      "RMSE per TS 6 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 78.01765718456228\n",
      "RMSE per TS 7 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 85.4429799394635\n",
      "RMSE per TS 8 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 92.43541989440146\n",
      "RMSE per TS 9 for model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 104.09311827214934\n",
      "Avg.RMSE for multivariate model: UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 98.2466727817778\n",
      "### START Experiment with  UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area161 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area230 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area79 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area236 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area162 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area170 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area234 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area48 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area186 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape of X_train:  (17351, 168, 10)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.0359 - val_loss: 0.0329\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0286 - val_loss: 0.0296\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0260 - val_loss: 0.0267\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0237 - val_loss: 0.0256\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0228 - val_loss: 0.0235\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0212 - val_loss: 0.0225\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0199 - val_loss: 0.0209\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0188 - val_loss: 0.0195\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0187 - val_loss: 0.0214\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0186 - val_loss: 0.0190\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0173 - val_loss: 0.0178\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0165 - val_loss: 0.0189\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0158 - val_loss: 0.0155\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0154 - val_loss: 0.0151\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0145 - val_loss: 0.0154\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0134 - val_loss: 0.0136\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 13s 778us/step - loss: 0.0127 - val_loss: 0.0131\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0126 - val_loss: 0.0132\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0122 - val_loss: 0.0126\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0122 - val_loss: 0.0128\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0117 - val_loss: 0.0121\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0115 - val_loss: 0.0124\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0116 - val_loss: 0.0122\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0114 - val_loss: 0.0122\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 13s 776us/step - loss: 0.0115 - val_loss: 0.0119\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0108 - val_loss: 0.0117\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0115 - val_loss: 0.0115\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0107 - val_loss: 0.0114\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0105 - val_loss: 0.0113\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0103 - val_loss: 0.0113\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0102 - val_loss: 0.0111\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0102 - val_loss: 0.0112\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0101 - val_loss: 0.0109\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0101 - val_loss: 0.0108\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0098 - val_loss: 0.0108\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 14s 789us/step - loss: 0.0097 - val_loss: 0.0108\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0096 - val_loss: 0.0107\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0096 - val_loss: 0.0106\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0095 - val_loss: 0.0104\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0094 - val_loss: 0.0104\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 13s 778us/step - loss: 0.0094 - val_loss: 0.0105\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0093 - val_loss: 0.0102\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0091 - val_loss: 0.0103\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0090 - val_loss: 0.0102\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0088 - val_loss: 0.0103\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0087 - val_loss: 0.0099\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0088 - val_loss: 0.0100\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0086 - val_loss: 0.0100\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0085 - val_loss: 0.0099\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0084 - val_loss: 0.0100\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0083 - val_loss: 0.0101\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0084 - val_loss: 0.0101\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0083 - val_loss: 0.0098\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0082 - val_loss: 0.0100\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0081 - val_loss: 0.0100\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0083 - val_loss: 0.0099\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0081 - val_loss: 0.0098\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0080 - val_loss: 0.0099\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0081 - val_loss: 0.0100\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0081 - val_loss: 0.0099\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0079 - val_loss: 0.0101\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0078 - val_loss: 0.0099\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0078 - val_loss: 0.0100\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0078 - val_loss: 0.0099\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0078 - val_loss: 0.0098\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0076 - val_loss: 0.0102\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 14s 789us/step - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0075 - val_loss: 0.0100\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0074 - val_loss: 0.0099\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0073 - val_loss: 0.0100\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0073 - val_loss: 0.0100\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0073 - val_loss: 0.0099\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0072 - val_loss: 0.0103\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0074 - val_loss: 0.0104\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0072 - val_loss: 0.0100\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 14s 790us/step - loss: 0.0071 - val_loss: 0.0101\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0070 - val_loss: 0.0102\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0070 - val_loss: 0.0099\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0069 - val_loss: 0.0103\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0070 - val_loss: 0.0103\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0070 - val_loss: 0.0101\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0069 - val_loss: 0.0100\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0068 - val_loss: 0.0101\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0068 - val_loss: 0.0103\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0067 - val_loss: 0.0100\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0101 - val_loss: 0.0191\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0131 - val_loss: 0.0109\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 14s 778us/step - loss: 0.0096 - val_loss: 0.0100\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0088 - val_loss: 0.0098\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0084 - val_loss: 0.0096\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 13s 775us/step - loss: 0.0081 - val_loss: 0.0096\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0078 - val_loss: 0.0096\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0076 - val_loss: 0.0096\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0073 - val_loss: 0.0099\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0067 - val_loss: 0.0101\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0067 - val_loss: 0.0100\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 14s 790us/step - loss: 0.0066 - val_loss: 0.0100\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 14s 784us/step - loss: 0.0066 - val_loss: 0.0099\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0065 - val_loss: 0.0099\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0064 - val_loss: 0.0101\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0064 - val_loss: 0.0100\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0064 - val_loss: 0.0101\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 14s 782us/step - loss: 0.0063 - val_loss: 0.0100\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 14s 779us/step - loss: 0.0063 - val_loss: 0.0102\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 14s 788us/step - loss: 0.0064 - val_loss: 0.0100\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0063 - val_loss: 0.0101\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0062 - val_loss: 0.0100\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 14s 787us/step - loss: 0.0062 - val_loss: 0.0101\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0061 - val_loss: 0.0101\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0062 - val_loss: 0.0101\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 14s 780us/step - loss: 0.0061 - val_loss: 0.0101\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0062 - val_loss: 0.0103\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 14s 781us/step - loss: 0.0064 - val_loss: 0.0101\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 14s 786us/step - loss: 0.0061 - val_loss: 0.0102\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0060 - val_loss: 0.0104\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 14s 783us/step - loss: 0.0060 - val_loss: 0.0101\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 13s 777us/step - loss: 0.0060 - val_loss: 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 14s 785us/step - loss: 0.0059 - val_loss: 0.0101\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 512)\n",
      "X_valid_encoded shape:  (8760, 512)\n",
      "X_test_encoded shape:  (8784, 512)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 4s 210us/step - loss: 0.1044 - mean_absolute_error: 0.2486 - val_loss: 0.0253 - val_mean_absolute_error: 0.1187\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0464 - mean_absolute_error: 0.1667 - val_loss: 0.0203 - val_mean_absolute_error: 0.1046\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0343 - mean_absolute_error: 0.1421 - val_loss: 0.0179 - val_mean_absolute_error: 0.0972\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0286 - mean_absolute_error: 0.1287 - val_loss: 0.0169 - val_mean_absolute_error: 0.0936\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0253 - mean_absolute_error: 0.1205 - val_loss: 0.0160 - val_mean_absolute_error: 0.0905\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0231 - mean_absolute_error: 0.1143 - val_loss: 0.0158 - val_mean_absolute_error: 0.0900\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0216 - mean_absolute_error: 0.1103 - val_loss: 0.0153 - val_mean_absolute_error: 0.0881\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0205 - mean_absolute_error: 0.1068 - val_loss: 0.0150 - val_mean_absolute_error: 0.0871\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0196 - mean_absolute_error: 0.1043 - val_loss: 0.0147 - val_mean_absolute_error: 0.0863\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0189 - mean_absolute_error: 0.1020 - val_loss: 0.0146 - val_mean_absolute_error: 0.0860\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0183 - mean_absolute_error: 0.1001 - val_loss: 0.0144 - val_mean_absolute_error: 0.0852\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0178 - mean_absolute_error: 0.0986 - val_loss: 0.0141 - val_mean_absolute_error: 0.0843\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0173 - mean_absolute_error: 0.0970 - val_loss: 0.0142 - val_mean_absolute_error: 0.0844\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0168 - mean_absolute_error: 0.0955 - val_loss: 0.0140 - val_mean_absolute_error: 0.0836\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0165 - mean_absolute_error: 0.0945 - val_loss: 0.0139 - val_mean_absolute_error: 0.0835\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0161 - mean_absolute_error: 0.0933 - val_loss: 0.0139 - val_mean_absolute_error: 0.0833\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0158 - mean_absolute_error: 0.0924 - val_loss: 0.0138 - val_mean_absolute_error: 0.0836\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0157 - mean_absolute_error: 0.0916 - val_loss: 0.0137 - val_mean_absolute_error: 0.0830\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0154 - mean_absolute_error: 0.0909 - val_loss: 0.0137 - val_mean_absolute_error: 0.0830\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0153 - mean_absolute_error: 0.0905 - val_loss: 0.0135 - val_mean_absolute_error: 0.0823\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0151 - mean_absolute_error: 0.0898 - val_loss: 0.0135 - val_mean_absolute_error: 0.0822\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0151 - mean_absolute_error: 0.0895 - val_loss: 0.0137 - val_mean_absolute_error: 0.0831\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0148 - mean_absolute_error: 0.0889 - val_loss: 0.0134 - val_mean_absolute_error: 0.0821\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0147 - mean_absolute_error: 0.0882 - val_loss: 0.0133 - val_mean_absolute_error: 0.0817\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0146 - mean_absolute_error: 0.0880 - val_loss: 0.0133 - val_mean_absolute_error: 0.0816\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0146 - mean_absolute_error: 0.0879 - val_loss: 0.0133 - val_mean_absolute_error: 0.0818\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0143 - mean_absolute_error: 0.0872 - val_loss: 0.0133 - val_mean_absolute_error: 0.0819\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0142 - mean_absolute_error: 0.0868 - val_loss: 0.0131 - val_mean_absolute_error: 0.0811\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0141 - mean_absolute_error: 0.0864 - val_loss: 0.0133 - val_mean_absolute_error: 0.0820\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0140 - mean_absolute_error: 0.0862 - val_loss: 0.0131 - val_mean_absolute_error: 0.0810\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0139 - mean_absolute_error: 0.0860 - val_loss: 0.0131 - val_mean_absolute_error: 0.0809\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0139 - mean_absolute_error: 0.0856 - val_loss: 0.0131 - val_mean_absolute_error: 0.0812\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0138 - mean_absolute_error: 0.0855 - val_loss: 0.0132 - val_mean_absolute_error: 0.0817\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0138 - mean_absolute_error: 0.0855 - val_loss: 0.0130 - val_mean_absolute_error: 0.0806\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0852 - val_loss: 0.0130 - val_mean_absolute_error: 0.0806\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0850 - val_loss: 0.0130 - val_mean_absolute_error: 0.0809\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0848 - val_loss: 0.0131 - val_mean_absolute_error: 0.0812\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0136 - mean_absolute_error: 0.0846 - val_loss: 0.0131 - val_mean_absolute_error: 0.0813\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0135 - mean_absolute_error: 0.0843 - val_loss: 0.0130 - val_mean_absolute_error: 0.0808\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0842 - val_loss: 0.0129 - val_mean_absolute_error: 0.0806\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0841 - val_loss: 0.0130 - val_mean_absolute_error: 0.0809\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0134 - mean_absolute_error: 0.0840 - val_loss: 0.0129 - val_mean_absolute_error: 0.0805\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0129 - val_mean_absolute_error: 0.0806\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0837 - val_loss: 0.0128 - val_mean_absolute_error: 0.0800\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0130 - val_mean_absolute_error: 0.0812\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0126 - val_mean_absolute_error: 0.0796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0129 - val_mean_absolute_error: 0.0804\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0831 - val_loss: 0.0127 - val_mean_absolute_error: 0.0798\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0127 - val_mean_absolute_error: 0.0800\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0127 - val_mean_absolute_error: 0.0799\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0126 - val_mean_absolute_error: 0.0797\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0127 - val_mean_absolute_error: 0.0800\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0823 - val_loss: 0.0128 - val_mean_absolute_error: 0.0801\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0127 - val_mean_absolute_error: 0.0799\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0823 - val_loss: 0.0126 - val_mean_absolute_error: 0.0799\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0127 - val_mean_absolute_error: 0.0800\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0821 - val_loss: 0.0127 - val_mean_absolute_error: 0.0799\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0821 - val_loss: 0.0127 - val_mean_absolute_error: 0.0802\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0819 - val_loss: 0.0127 - val_mean_absolute_error: 0.0799\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0818 - val_loss: 0.0128 - val_mean_absolute_error: 0.0805\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0818 - val_loss: 0.0125 - val_mean_absolute_error: 0.0797\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0821 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0816 - val_loss: 0.0126 - val_mean_absolute_error: 0.0797\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0815 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0819 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0815 - val_loss: 0.0126 - val_mean_absolute_error: 0.0797\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0816 - val_loss: 0.0126 - val_mean_absolute_error: 0.0795\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0817 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0127 - mean_absolute_error: 0.0816 - val_loss: 0.0126 - val_mean_absolute_error: 0.0796\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0817 - val_loss: 0.0131 - val_mean_absolute_error: 0.0815\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0817 - val_loss: 0.0126 - val_mean_absolute_error: 0.0796\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0127 - val_mean_absolute_error: 0.0799\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0816 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0813 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0126 - val_mean_absolute_error: 0.0795\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0815 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0813 - val_loss: 0.0126 - val_mean_absolute_error: 0.0795\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0813 - val_loss: 0.0125 - val_mean_absolute_error: 0.0794\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0125 - val_mean_absolute_error: 0.0794\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0126 - val_mean_absolute_error: 0.0795\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0814 - val_loss: 0.0127 - val_mean_absolute_error: 0.0802\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0814 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0125 - val_mean_absolute_error: 0.0794\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0127 - val_mean_absolute_error: 0.0797\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0124 - val_mean_absolute_error: 0.0791\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0127 - val_mean_absolute_error: 0.0803\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0126 - val_mean_absolute_error: 0.0796\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0812 - val_loss: 0.0126 - val_mean_absolute_error: 0.0797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0125 - val_mean_absolute_error: 0.0792\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0811 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0805 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0126 - val_mean_absolute_error: 0.0802\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0807 - val_loss: 0.0125 - val_mean_absolute_error: 0.0794\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0808 - val_loss: 0.0125 - val_mean_absolute_error: 0.0794\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0808 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0808 - val_loss: 0.0126 - val_mean_absolute_error: 0.0795\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0807 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0126 - val_mean_absolute_error: 0.0801\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0811 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0811 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0808 - val_loss: 0.0124 - val_mean_absolute_error: 0.0792\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0125 - val_mean_absolute_error: 0.0798\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0807 - val_loss: 0.0124 - val_mean_absolute_error: 0.0795\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0126 - val_mean_absolute_error: 0.0799\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0808 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0805 - val_loss: 0.0124 - val_mean_absolute_error: 0.0792\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0809 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0807 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0807 - val_loss: 0.0124 - val_mean_absolute_error: 0.0793\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0807 - val_loss: 0.0126 - val_mean_absolute_error: 0.0799\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0809 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0124 - val_mean_absolute_error: 0.0792\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0804 - val_loss: 0.0124 - val_mean_absolute_error: 0.0792\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0804 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0810 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0126 - val_mean_absolute_error: 0.0796\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0804 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0809 - val_loss: 0.0125 - val_mean_absolute_error: 0.0793\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0803 - val_loss: 0.0124 - val_mean_absolute_error: 0.0795\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0807 - val_loss: 0.0126 - val_mean_absolute_error: 0.0798\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0809 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0123 - val_mean_absolute_error: 0.0791\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0805 - val_loss: 0.0124 - val_mean_absolute_error: 0.0792\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0806 - val_loss: 0.0124 - val_mean_absolute_error: 0.0793\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0804 - val_loss: 0.0123 - val_mean_absolute_error: 0.0790\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0804 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0806 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0124 - val_mean_absolute_error: 0.0793\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0805 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0122 - mean_absolute_error: 0.0802 - val_loss: 0.0123 - val_mean_absolute_error: 0.0790\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0805 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0804 - val_loss: 0.0124 - val_mean_absolute_error: 0.0794\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0806 - val_loss: 0.0125 - val_mean_absolute_error: 0.0795\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 2s 174us/step\n",
      "yhat shape:  (8760, 10)\n",
      "First 2 scaled predictions\n",
      "[[ 33.177776 -35.2894   -11.648901 -61.248913  43.293537 -25.077925\n",
      "  -18.916653 -27.939312 -26.483356 -33.418137]\n",
      " [ 30.142363 -24.974636 -16.728209 -98.84032   33.18405  -20.713467\n",
      "  -31.72527  -35.441555 -45.21344  -42.602005]]\n",
      "Shape of predictions: (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2011-01-01 00:00:00  547.177776  335.710602  184.351099  1106.751087   \n",
      "2011-01-01 01:00:00  360.142363  294.025364   29.271791   498.159683   \n",
      "2011-01-01 02:00:00  341.894909  166.088326   15.925430   494.996101   \n",
      "2011-01-01 03:00:00  262.124892  154.823925   23.733994   393.611412   \n",
      "2011-01-01 04:00:00  158.877472  142.008472   49.169147   406.025009   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  611.293537  415.922075  537.083347  619.060688   \n",
      "2011-01-01 01:00:00  368.184052  240.286533  421.274731  403.558445   \n",
      "2011-01-01 02:00:00  377.603386  291.237282  460.746239  266.222542   \n",
      "2011-01-01 03:00:00  296.008355  274.733414  408.989510  147.831223   \n",
      "2011-01-01 04:00:00  162.087768  259.012001  367.098782  149.661011   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  553.516644  319.581863  \n",
      "2011-01-01 01:00:00  311.786560  135.397995  \n",
      "2011-01-01 02:00:00  228.389786  153.427269  \n",
      "2011-01-01 03:00:00  246.747536  109.066185  \n",
      "2011-01-01 04:00:00  323.156509  136.386024  \n",
      "RMSE per TS 0 for model: validation_set: 90.83716690422452\n",
      "RMSE per TS 1 for model: validation_set: 105.99049633897035\n",
      "RMSE per TS 2 for model: validation_set: 99.00179967951343\n",
      "RMSE per TS 3 for model: validation_set: 124.86601831022118\n",
      "RMSE per TS 4 for model: validation_set: 86.42048257858161\n",
      "RMSE per TS 5 for model: validation_set: 93.52041580981762\n",
      "RMSE per TS 6 for model: validation_set: 79.05928443660302\n",
      "RMSE per TS 7 for model: validation_set: 81.71201221981273\n",
      "RMSE per TS 8 for model: validation_set: 85.86852922664906\n",
      "RMSE per TS 9 for model: validation_set: 97.10861648566708\n",
      "Avg.RMSE for multivariate model: validation_set: 94.43848219900607\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 29us/step\n",
      "yhat shape:  (8784, 10)\n",
      "First 2 scaled predictions\n",
      "[[  4.1532354 -18.759298   81.962326  143.51477     9.47525   -29.419638\n",
      "   17.018406   33.485874   72.428185   23.794825 ]\n",
      " [-16.637835  -42.231586    2.717557   45.3729     -7.798639  -39.794506\n",
      "  -16.619883   -5.7102075   9.517582   -3.5391085]]\n",
      "Shape of predictions: (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  550.153235  458.240702  263.962326  1454.514771   \n",
      "2012-01-01 01:00:00  435.362165  325.768414  100.717557   747.372898   \n",
      "2012-01-01 02:00:00  425.600937  175.412018   -4.837402   650.396973   \n",
      "2012-01-01 03:00:00  264.650833  168.152855  -34.542198   515.260094   \n",
      "2012-01-01 04:00:00  125.378231  162.823830   77.985611   380.729919   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  596.475250  425.580362  669.018406  768.485874   \n",
      "2012-01-01 01:00:00  374.201361  269.205494  495.380117  508.289793   \n",
      "2012-01-01 02:00:00  370.173065  291.610298  481.606651  265.856159   \n",
      "2012-01-01 03:00:00  301.233650  296.404099  439.337593  185.266781   \n",
      "2012-01-01 04:00:00  154.223644  202.842655  409.160004  177.732361   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  880.428185  433.794825  \n",
      "2012-01-01 01:00:00  495.517582  241.460891  \n",
      "2012-01-01 02:00:00  326.923164  220.914909  \n",
      "2012-01-01 03:00:00  286.596039  208.156578  \n",
      "2012-01-01 04:00:00  403.067970  163.323868  \n",
      "RMSE per TS 0 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 93.62873870061001\n",
      "RMSE per TS 1 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 107.07845781387739\n",
      "RMSE per TS 2 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 98.46565018617518\n",
      "RMSE per TS 3 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 128.73653019473883\n",
      "RMSE per TS 4 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 89.83893523709692\n",
      "RMSE per TS 5 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 97.43736386573272\n",
      "RMSE per TS 6 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 79.01947053332641\n",
      "RMSE per TS 7 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 86.57227014270613\n",
      "RMSE per TS 8 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 90.60124842994414\n",
      "RMSE per TS 9 for model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 100.03977756276304\n",
      "Avg.RMSE for multivariate model: UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 97.14184426669706\n",
      "### START Experiment with  UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "#Generate data for UBER Model...\n",
      "data for area237 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area161 is prepared...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area230 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area79 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area236 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area162 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area170 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area234 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area48 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "data for area186 is prepared...\n",
      "Data is scaled...\n",
      "MinMax-Scaling used...\n",
      "shape of X_train single area:  (17351, 168, 1)\n",
      "shape of X_valid single area:  (8760, 168, 1)\n",
      "shape of X_test single area:  (8784, 168, 1)\n",
      "shape of y_train single area:  (17351, 1)\n",
      "shape of y_valid single area:  (8760, 1)\n",
      "shape of y_test single area:  (8784, 1)\n",
      "shape of prev_lags_F_train single area:  (17351, 24, 1)\n",
      "shape of fut_F_values_train single area:  (17351, 24, 1)\n",
      "final shape of X_train:  (17351, 168, 10)\n",
      "create UBER Autoencoder...\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 18s 1ms/step - loss: 0.0345 - val_loss: 0.0318\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 14s 795us/step - loss: 0.0279 - val_loss: 0.0287\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 14s 790us/step - loss: 0.0252 - val_loss: 0.0254\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0241 - val_loss: 0.0263\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0225 - val_loss: 0.0230\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 14s 794us/step - loss: 0.0206 - val_loss: 0.0215\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0185 - val_loss: 0.0193\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0177 - val_loss: 0.0198\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0156 - val_loss: 0.0163\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0145 - val_loss: 0.0148\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0144 - val_loss: 0.0155\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0136 - val_loss: 0.0162\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0138 - val_loss: 0.0153\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0130 - val_loss: 0.0133\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0128 - val_loss: 0.0136\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0121 - val_loss: 0.0128\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0119 - val_loss: 0.0126\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0116 - val_loss: 0.0121\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0113 - val_loss: 0.0134\n",
      "Epoch 23/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0113 - val_loss: 0.0118\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0110 - val_loss: 0.0118\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0108 - val_loss: 0.0115\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0105 - val_loss: 0.0111\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0104 - val_loss: 0.0113\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0102 - val_loss: 0.0110\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 14s 806us/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0104 - val_loss: 0.0107\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0098 - val_loss: 0.0110\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0098 - val_loss: 0.0105\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0094 - val_loss: 0.0104\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0097 - val_loss: 0.0104\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0094 - val_loss: 0.0102\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0092 - val_loss: 0.0101\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0090 - val_loss: 0.0100\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 14s 805us/step - loss: 0.0091 - val_loss: 0.0102\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0131 - val_loss: 0.0124\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0115 - val_loss: 0.0111\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0102 - val_loss: 0.0106\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0096 - val_loss: 0.0103\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0102 - val_loss: 0.0108\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0095 - val_loss: 0.0101\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0092 - val_loss: 0.0100\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 14s 807us/step - loss: 0.0091 - val_loss: 0.0100\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0089 - val_loss: 0.0100\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0089 - val_loss: 0.0099\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0088 - val_loss: 0.0099\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0087 - val_loss: 0.0097\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0086 - val_loss: 0.0098\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 14s 805us/step - loss: 0.0086 - val_loss: 0.0097\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0086 - val_loss: 0.0097\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0085 - val_loss: 0.0097\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0084 - val_loss: 0.0098\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0085 - val_loss: 0.0098\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 14s 805us/step - loss: 0.0084 - val_loss: 0.0097\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 14s 807us/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0083 - val_loss: 0.0097\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0084 - val_loss: 0.0097\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0083 - val_loss: 0.0096\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0082 - val_loss: 0.0096\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0082 - val_loss: 0.0097\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0081 - val_loss: 0.0095\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0081 - val_loss: 0.0094\n",
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0080 - val_loss: 0.0094\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0079 - val_loss: 0.0096\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0080 - val_loss: 0.0096\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0080 - val_loss: 0.0095\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0078 - val_loss: 0.0096\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 14s 805us/step - loss: 0.0078 - val_loss: 0.0094\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0078 - val_loss: 0.0094\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0077 - val_loss: 0.0094\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0079 - val_loss: 0.0094\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0076 - val_loss: 0.0095\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0077 - val_loss: 0.0094\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0075 - val_loss: 0.0094\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 14s 793us/step - loss: 0.0074 - val_loss: 0.0095\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 14s 805us/step - loss: 0.0074 - val_loss: 0.0093\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0074 - val_loss: 0.0097\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 14s 795us/step - loss: 0.0074 - val_loss: 0.0095\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0072 - val_loss: 0.0095\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0072 - val_loss: 0.0096\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0072 - val_loss: 0.0097\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 14s 794us/step - loss: 0.0072 - val_loss: 0.0096\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0071 - val_loss: 0.0095\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0072 - val_loss: 0.0095\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 14s 795us/step - loss: 0.0070 - val_loss: 0.0095\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0070 - val_loss: 0.0095\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0069 - val_loss: 0.0097\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0069 - val_loss: 0.0096\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0067 - val_loss: 0.0095\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0066 - val_loss: 0.0096\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0065 - val_loss: 0.0100\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0070 - val_loss: 0.0097\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0066 - val_loss: 0.0095\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0064 - val_loss: 0.0099\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0063 - val_loss: 0.0098\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0064 - val_loss: 0.0097\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0062 - val_loss: 0.0098\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 14s 795us/step - loss: 0.0062 - val_loss: 0.0097\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0063 - val_loss: 0.0097\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 14s 796us/step - loss: 0.0063 - val_loss: 0.0100\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0062 - val_loss: 0.0099\n",
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0061 - val_loss: 0.0099\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0061 - val_loss: 0.0103\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0061 - val_loss: 0.0098\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0059 - val_loss: 0.0099\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0060 - val_loss: 0.0098\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0059 - val_loss: 0.0099\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0059 - val_loss: 0.0099\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0061 - val_loss: 0.0099\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0057 - val_loss: 0.0101\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0057 - val_loss: 0.0102\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0056 - val_loss: 0.0101\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0056 - val_loss: 0.0100\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0057 - val_loss: 0.0102\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0059 - val_loss: 0.0104\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0058 - val_loss: 0.0102\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0054 - val_loss: 0.0102\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 14s 800us/step - loss: 0.0054 - val_loss: 0.0102\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0053 - val_loss: 0.0102\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 14s 804us/step - loss: 0.0054 - val_loss: 0.0104\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0061 - val_loss: 0.0107\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0060 - val_loss: 0.0102\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0065 - val_loss: 0.0102\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 14s 802us/step - loss: 0.0056 - val_loss: 0.0102\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 14s 794us/step - loss: 0.0053 - val_loss: 0.0103\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 14s 803us/step - loss: 0.0051 - val_loss: 0.0103\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 14s 801us/step - loss: 0.0050 - val_loss: 0.0104\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 14s 797us/step - loss: 0.0050 - val_loss: 0.0103\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0049 - val_loss: 0.0104\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 14s 799us/step - loss: 0.0049 - val_loss: 0.0105\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 14s 798us/step - loss: 0.0048 - val_loss: 0.0104\n",
      "create standalone UBER encoder...\n",
      "encode data with UBER standalone encoder:\n",
      "X_train_encoded shape:  (17351, 512)\n",
      "X_valid_encoded shape:  (8760, 512)\n",
      "X_test_encoded shape:  (8784, 512)\n",
      "create UBER prediction_model:\n",
      "Train on 17351 samples, validate on 8760 samples\n",
      "Epoch 1/150\n",
      "17351/17351 [==============================] - 4s 227us/step - loss: 0.1091 - mean_absolute_error: 0.2552 - val_loss: 0.0221 - val_mean_absolute_error: 0.1111\n",
      "Epoch 2/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0482 - mean_absolute_error: 0.1707 - val_loss: 0.0186 - val_mean_absolute_error: 0.0998\n",
      "Epoch 3/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0349 - mean_absolute_error: 0.1447 - val_loss: 0.0171 - val_mean_absolute_error: 0.0955\n",
      "Epoch 4/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0290 - mean_absolute_error: 0.1307 - val_loss: 0.0161 - val_mean_absolute_error: 0.0916\n",
      "Epoch 5/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0255 - mean_absolute_error: 0.1217 - val_loss: 0.0157 - val_mean_absolute_error: 0.0901\n",
      "Epoch 6/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0233 - mean_absolute_error: 0.1156 - val_loss: 0.0154 - val_mean_absolute_error: 0.0889\n",
      "Epoch 7/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0218 - mean_absolute_error: 0.1115 - val_loss: 0.0152 - val_mean_absolute_error: 0.0881\n",
      "Epoch 8/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0207 - mean_absolute_error: 0.1080 - val_loss: 0.0149 - val_mean_absolute_error: 0.0870\n",
      "Epoch 9/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0196 - mean_absolute_error: 0.1048 - val_loss: 0.0146 - val_mean_absolute_error: 0.0863\n",
      "Epoch 10/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0188 - mean_absolute_error: 0.1025 - val_loss: 0.0148 - val_mean_absolute_error: 0.0869\n",
      "Epoch 11/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0182 - mean_absolute_error: 0.1005 - val_loss: 0.0144 - val_mean_absolute_error: 0.0857\n",
      "Epoch 12/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0176 - mean_absolute_error: 0.0986 - val_loss: 0.0142 - val_mean_absolute_error: 0.0848\n",
      "Epoch 13/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0173 - mean_absolute_error: 0.0976 - val_loss: 0.0144 - val_mean_absolute_error: 0.0856\n",
      "Epoch 14/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0166 - mean_absolute_error: 0.0956 - val_loss: 0.0143 - val_mean_absolute_error: 0.0850\n",
      "Epoch 15/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0165 - mean_absolute_error: 0.0949 - val_loss: 0.0143 - val_mean_absolute_error: 0.0849\n",
      "Epoch 16/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0163 - mean_absolute_error: 0.0942 - val_loss: 0.0140 - val_mean_absolute_error: 0.0844\n",
      "Epoch 17/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0159 - mean_absolute_error: 0.0930 - val_loss: 0.0140 - val_mean_absolute_error: 0.0841\n",
      "Epoch 18/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0158 - mean_absolute_error: 0.0926 - val_loss: 0.0139 - val_mean_absolute_error: 0.0841\n",
      "Epoch 19/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0156 - mean_absolute_error: 0.0918 - val_loss: 0.0141 - val_mean_absolute_error: 0.0847\n",
      "Epoch 20/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0154 - mean_absolute_error: 0.0910 - val_loss: 0.0138 - val_mean_absolute_error: 0.0836\n",
      "Epoch 21/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0151 - mean_absolute_error: 0.0903 - val_loss: 0.0138 - val_mean_absolute_error: 0.0837\n",
      "Epoch 22/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0151 - mean_absolute_error: 0.0903 - val_loss: 0.0137 - val_mean_absolute_error: 0.0832\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0150 - mean_absolute_error: 0.0898 - val_loss: 0.0138 - val_mean_absolute_error: 0.0838\n",
      "Epoch 24/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0149 - mean_absolute_error: 0.0896 - val_loss: 0.0138 - val_mean_absolute_error: 0.0836\n",
      "Epoch 25/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0147 - mean_absolute_error: 0.0889 - val_loss: 0.0137 - val_mean_absolute_error: 0.0835\n",
      "Epoch 26/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0146 - mean_absolute_error: 0.0883 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
      "Epoch 27/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0145 - mean_absolute_error: 0.0883 - val_loss: 0.0139 - val_mean_absolute_error: 0.0840\n",
      "Epoch 28/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0144 - mean_absolute_error: 0.0879 - val_loss: 0.0138 - val_mean_absolute_error: 0.0842\n",
      "Epoch 29/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0144 - mean_absolute_error: 0.0876 - val_loss: 0.0137 - val_mean_absolute_error: 0.0836\n",
      "Epoch 30/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0143 - mean_absolute_error: 0.0876 - val_loss: 0.0136 - val_mean_absolute_error: 0.0832\n",
      "Epoch 31/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0142 - mean_absolute_error: 0.0871 - val_loss: 0.0135 - val_mean_absolute_error: 0.0829\n",
      "Epoch 32/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0142 - mean_absolute_error: 0.0871 - val_loss: 0.0135 - val_mean_absolute_error: 0.0829\n",
      "Epoch 33/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0141 - mean_absolute_error: 0.0867 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 34/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0140 - mean_absolute_error: 0.0863 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 35/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0141 - mean_absolute_error: 0.0864 - val_loss: 0.0136 - val_mean_absolute_error: 0.0830\n",
      "Epoch 36/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0140 - mean_absolute_error: 0.0861 - val_loss: 0.0135 - val_mean_absolute_error: 0.0832\n",
      "Epoch 37/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0139 - mean_absolute_error: 0.0860 - val_loss: 0.0135 - val_mean_absolute_error: 0.0825\n",
      "Epoch 38/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0139 - mean_absolute_error: 0.0856 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 39/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0139 - mean_absolute_error: 0.0858 - val_loss: 0.0137 - val_mean_absolute_error: 0.0835\n",
      "Epoch 40/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0138 - mean_absolute_error: 0.0854 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
      "Epoch 41/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0137 - mean_absolute_error: 0.0853 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 42/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0138 - mean_absolute_error: 0.0852 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 43/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0136 - mean_absolute_error: 0.0848 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 44/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0136 - mean_absolute_error: 0.0847 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 45/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0136 - mean_absolute_error: 0.0847 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 46/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0846 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 47/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0136 - mean_absolute_error: 0.0845 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 48/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0832\n",
      "Epoch 49/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0844 - val_loss: 0.0135 - val_mean_absolute_error: 0.0831\n",
      "Epoch 50/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0135 - mean_absolute_error: 0.0843 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 51/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0134 - mean_absolute_error: 0.0842 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 52/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0134 - mean_absolute_error: 0.0842 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 53/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0839 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 54/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0134 - mean_absolute_error: 0.0840 - val_loss: 0.0134 - val_mean_absolute_error: 0.0830\n",
      "Epoch 55/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0134 - mean_absolute_error: 0.0841 - val_loss: 0.0136 - val_mean_absolute_error: 0.0829\n",
      "Epoch 56/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0837 - val_loss: 0.0134 - val_mean_absolute_error: 0.0823\n",
      "Epoch 57/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0838 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 58/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0134 - mean_absolute_error: 0.0839 - val_loss: 0.0135 - val_mean_absolute_error: 0.0826\n",
      "Epoch 59/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0134 - mean_absolute_error: 0.0839 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 60/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 61/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 62/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0133 - mean_absolute_error: 0.0836 - val_loss: 0.0134 - val_mean_absolute_error: 0.0828\n",
      "Epoch 63/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 64/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0136 - val_mean_absolute_error: 0.0833\n",
      "Epoch 65/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0134 - val_mean_absolute_error: 0.0829\n",
      "Epoch 66/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0837 - val_loss: 0.0135 - val_mean_absolute_error: 0.0827\n",
      "Epoch 67/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0133 - mean_absolute_error: 0.0835 - val_loss: 0.0135 - val_mean_absolute_error: 0.0828\n",
      "Epoch 68/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0134 - val_mean_absolute_error: 0.0824\n",
      "Epoch 69/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0832 - val_loss: 0.0136 - val_mean_absolute_error: 0.0832\n",
      "Epoch 70/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0833 - val_loss: 0.0133 - val_mean_absolute_error: 0.0826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0135 - val_mean_absolute_error: 0.0831\n",
      "Epoch 72/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0134 - val_mean_absolute_error: 0.0827\n",
      "Epoch 73/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0822\n",
      "Epoch 74/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 75/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0831 - val_loss: 0.0133 - val_mean_absolute_error: 0.0826\n",
      "Epoch 76/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0832 - val_loss: 0.0134 - val_mean_absolute_error: 0.0826\n",
      "Epoch 77/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 78/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0132 - mean_absolute_error: 0.0835 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 79/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 80/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0832 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 81/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 82/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 83/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 84/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0823\n",
      "Epoch 85/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0135 - val_mean_absolute_error: 0.0833\n",
      "Epoch 86/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0130 - mean_absolute_error: 0.0829 - val_loss: 0.0134 - val_mean_absolute_error: 0.0825\n",
      "Epoch 87/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0133 - val_mean_absolute_error: 0.0825\n",
      "Epoch 88/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0829 - val_loss: 0.0132 - val_mean_absolute_error: 0.0825\n",
      "Epoch 89/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 90/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 91/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0828 - val_loss: 0.0132 - val_mean_absolute_error: 0.0825\n",
      "Epoch 92/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0131 - mean_absolute_error: 0.0830 - val_loss: 0.0133 - val_mean_absolute_error: 0.0823\n",
      "Epoch 93/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 94/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0132 - val_mean_absolute_error: 0.0820\n",
      "Epoch 95/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0130 - mean_absolute_error: 0.0829 - val_loss: 0.0133 - val_mean_absolute_error: 0.0828\n",
      "Epoch 96/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0830 - val_loss: 0.0132 - val_mean_absolute_error: 0.0819\n",
      "Epoch 97/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0130 - mean_absolute_error: 0.0828 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 98/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0129 - mean_absolute_error: 0.0827 - val_loss: 0.0134 - val_mean_absolute_error: 0.0829\n",
      "Epoch 99/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0133 - val_mean_absolute_error: 0.0827\n",
      "Epoch 100/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0824 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 101/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0826 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 102/150\n",
      "17351/17351 [==============================] - 0s 13us/step - loss: 0.0130 - mean_absolute_error: 0.0829 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 103/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0824 - val_loss: 0.0132 - val_mean_absolute_error: 0.0817\n",
      "Epoch 104/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0828 - val_loss: 0.0131 - val_mean_absolute_error: 0.0820\n",
      "Epoch 105/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0131 - val_mean_absolute_error: 0.0820\n",
      "Epoch 106/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0130 - mean_absolute_error: 0.0827 - val_loss: 0.0133 - val_mean_absolute_error: 0.0824\n",
      "Epoch 107/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0131 - val_mean_absolute_error: 0.0821\n",
      "Epoch 108/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 109/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0823 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 110/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0825 - val_loss: 0.0132 - val_mean_absolute_error: 0.0818\n",
      "Epoch 111/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0132 - val_mean_absolute_error: 0.0823\n",
      "Epoch 112/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0822 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 113/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0821 - val_loss: 0.0132 - val_mean_absolute_error: 0.0821\n",
      "Epoch 114/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0826 - val_loss: 0.0130 - val_mean_absolute_error: 0.0817\n",
      "Epoch 115/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0129 - mean_absolute_error: 0.0824 - val_loss: 0.0131 - val_mean_absolute_error: 0.0819\n",
      "Epoch 116/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0821 - val_loss: 0.0131 - val_mean_absolute_error: 0.0818\n",
      "Epoch 117/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0819 - val_loss: 0.0130 - val_mean_absolute_error: 0.0814\n",
      "Epoch 118/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0822 - val_loss: 0.0129 - val_mean_absolute_error: 0.0813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/150\n",
      "17351/17351 [==============================] - 0s 15us/step - loss: 0.0129 - mean_absolute_error: 0.0825 - val_loss: 0.0132 - val_mean_absolute_error: 0.0822\n",
      "Epoch 120/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0822 - val_loss: 0.0129 - val_mean_absolute_error: 0.0813\n",
      "Epoch 121/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0825 - val_loss: 0.0130 - val_mean_absolute_error: 0.0814\n",
      "Epoch 122/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0134 - val_mean_absolute_error: 0.0830\n",
      "Epoch 123/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0131 - val_mean_absolute_error: 0.0817\n",
      "Epoch 124/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0129 - val_mean_absolute_error: 0.0812\n",
      "Epoch 125/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0821 - val_loss: 0.0130 - val_mean_absolute_error: 0.0815\n",
      "Epoch 126/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0130 - val_mean_absolute_error: 0.0815\n",
      "Epoch 127/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0128 - mean_absolute_error: 0.0823 - val_loss: 0.0130 - val_mean_absolute_error: 0.0814\n",
      "Epoch 128/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0821 - val_loss: 0.0129 - val_mean_absolute_error: 0.0811\n",
      "Epoch 129/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0820 - val_loss: 0.0129 - val_mean_absolute_error: 0.0812\n",
      "Epoch 130/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0819 - val_loss: 0.0130 - val_mean_absolute_error: 0.0813\n",
      "Epoch 131/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0127 - mean_absolute_error: 0.0820 - val_loss: 0.0128 - val_mean_absolute_error: 0.0806\n",
      "Epoch 132/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0817 - val_loss: 0.0128 - val_mean_absolute_error: 0.0805\n",
      "Epoch 133/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0817 - val_loss: 0.0130 - val_mean_absolute_error: 0.0813\n",
      "Epoch 134/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0818 - val_loss: 0.0128 - val_mean_absolute_error: 0.0815\n",
      "Epoch 135/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0816 - val_loss: 0.0127 - val_mean_absolute_error: 0.0804\n",
      "Epoch 136/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0815 - val_loss: 0.0127 - val_mean_absolute_error: 0.0805\n",
      "Epoch 137/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0818 - val_loss: 0.0129 - val_mean_absolute_error: 0.0812\n",
      "Epoch 138/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0817 - val_loss: 0.0127 - val_mean_absolute_error: 0.0804\n",
      "Epoch 139/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0815 - val_loss: 0.0127 - val_mean_absolute_error: 0.0807\n",
      "Epoch 140/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0126 - mean_absolute_error: 0.0818 - val_loss: 0.0129 - val_mean_absolute_error: 0.0813\n",
      "Epoch 141/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0815 - val_loss: 0.0127 - val_mean_absolute_error: 0.0805\n",
      "Epoch 142/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0812 - val_loss: 0.0127 - val_mean_absolute_error: 0.0801\n",
      "Epoch 143/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0810 - val_loss: 0.0126 - val_mean_absolute_error: 0.0799\n",
      "Epoch 144/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0814 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 145/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0125 - mean_absolute_error: 0.0813 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "Epoch 146/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0812 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 147/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0811 - val_loss: 0.0126 - val_mean_absolute_error: 0.0800\n",
      "Epoch 148/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0808 - val_loss: 0.0126 - val_mean_absolute_error: 0.0797\n",
      "Epoch 149/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0123 - mean_absolute_error: 0.0808 - val_loss: 0.0125 - val_mean_absolute_error: 0.0797\n",
      "Epoch 150/150\n",
      "17351/17351 [==============================] - 0s 14us/step - loss: 0.0124 - mean_absolute_error: 0.0811 - val_loss: 0.0125 - val_mean_absolute_error: 0.0796\n",
      "get predictions of model...\n",
      "8760/8760 [==============================] - 2s 197us/step\n",
      "yhat shape:  (8760, 10)\n",
      "First 2 scaled predictions\n",
      "[[-121.196205 -161.86708  -138.93344   -93.725044  -96.906815 -111.7979\n",
      "  -105.678856 -101.58039   -94.00922   -88.036316]\n",
      " [ -91.48851  -131.17897  -131.85806  -136.33878   -68.85123   -88.495544\n",
      "   -94.7088    -99.036644  -93.83028   -66.62094 ]]\n",
      "Shape of predictions: (8760, 10)\n",
      "Invert Differencing of multivariate predictions...\n",
      "predictions preview:\n",
      "                            237         161        230           79  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  392.803795  209.132919  57.066559  1074.274956   \n",
      "2011-01-01 01:00:00  238.511490  187.821030 -85.858063   460.661224   \n",
      "2011-01-01 02:00:00  310.338436  140.588417 -17.713257   481.531525   \n",
      "2011-01-01 03:00:00  221.189800  126.194054  -5.141113   387.984482   \n",
      "2011-01-01 04:00:00  114.335281  114.034317  25.203072   399.061569   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2011-01-01 00:00:00  471.093185  329.202103  450.321144  545.419609   \n",
      "2011-01-01 01:00:00  266.148773  172.504456  358.291199  339.963356   \n",
      "2011-01-01 02:00:00  353.895645  278.289024  457.221806  250.104904   \n",
      "2011-01-01 03:00:00  268.598000  254.665417  400.163181  128.695763   \n",
      "2011-01-01 04:00:00  133.050938  239.032337  357.153706  128.233414   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2011-01-01 00:00:00  485.990784  264.963684  \n",
      "2011-01-01 01:00:00  263.169724  111.379059  \n",
      "2011-01-01 02:00:00  225.928810  158.757118  \n",
      "2011-01-01 03:00:00  252.976105  128.803856  \n",
      "2011-01-01 04:00:00  336.112137  167.161709  \n",
      "RMSE per TS 0 for model: validation_set: 93.22029091952957\n",
      "RMSE per TS 1 for model: validation_set: 103.42675619653245\n",
      "RMSE per TS 2 for model: validation_set: 100.85220386089644\n",
      "RMSE per TS 3 for model: validation_set: 121.54178071517238\n",
      "RMSE per TS 4 for model: validation_set: 90.09859839632927\n",
      "RMSE per TS 5 for model: validation_set: 94.5460778738856\n",
      "RMSE per TS 6 for model: validation_set: 78.87472934003003\n",
      "RMSE per TS 7 for model: validation_set: 80.868387389675\n",
      "RMSE per TS 8 for model: validation_set: 84.90586762563497\n",
      "RMSE per TS 9 for model: validation_set: 96.84313825775656\n",
      "Avg.RMSE for multivariate model: validation_set: 94.51778305754422\n",
      "get predictions of model...\n",
      "8784/8784 [==============================] - 0s 33us/step\n",
      "yhat shape:  (8784, 10)\n",
      "First 2 scaled predictions\n",
      "[[-127.426384 -130.49753   -66.1388     14.129628 -103.15182  -117.470505\n",
      "   -93.90209   -67.72022   -37.723743  -53.818718]\n",
      " [ -94.551704 -119.55707   -99.217415  -77.04313   -69.67196  -103.87145\n",
      "   -94.68981   -85.2324    -71.26051   -63.684246]]\n",
      "Shape of predictions: (8784, 10)\n",
      "Invert Differencing of multivariate predictions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions preview:\n",
      "                            237         161         230           79  \\\n",
      "date                                                                   \n",
      "2012-01-01 00:00:00  418.573616  346.502472  115.861198  1325.129628   \n",
      "2012-01-01 01:00:00  357.448296  248.442932   -1.217415   624.956871   \n",
      "2012-01-01 02:00:00  428.143951  155.423538  -17.010376   590.210312   \n",
      "2012-01-01 03:00:00  284.621765  169.799332  -23.464325   510.283081   \n",
      "2012-01-01 04:00:00  135.226704  161.031380   80.276512   391.604805   \n",
      "\n",
      "                            236         162         170         234  \\\n",
      "date                                                                  \n",
      "2012-01-01 00:00:00  483.848183  337.529495  558.097908  667.279778   \n",
      "2012-01-01 01:00:00  312.328041  205.128548  417.310188  428.767601   \n",
      "2012-01-01 02:00:00  388.564461  261.020020  463.310867  232.156052   \n",
      "2012-01-01 03:00:00  334.962860  286.120903  451.496971  179.735504   \n",
      "2012-01-01 04:00:00  179.687802  194.493263  425.432392  174.600700   \n",
      "\n",
      "                             48         186  \n",
      "date                                         \n",
      "2012-01-01 00:00:00  770.276257  356.181282  \n",
      "2012-01-01 01:00:00  414.739487  181.315754  \n",
      "2012-01-01 02:00:00  314.119766  200.299362  \n",
      "2012-01-01 03:00:00  303.623817  209.544342  \n",
      "2012-01-01 04:00:00  426.326851  176.719231  \n",
      "RMSE per TS 0 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 95.74582837954415\n",
      "RMSE per TS 1 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 103.88538954435094\n",
      "RMSE per TS 2 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 99.46396189986406\n",
      "RMSE per TS 3 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 125.0619852522699\n",
      "RMSE per TS 4 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 93.51524642243089\n",
      "RMSE per TS 5 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 98.33249847593525\n",
      "RMSE per TS 6 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 77.80408540179735\n",
      "RMSE per TS 7 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 85.1854321215903\n",
      "RMSE per TS 8 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 89.20991568809262\n",
      "RMSE per TS 9 for model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 99.83611015991605\n",
      "Avg.RMSE for multivariate model: UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012: 96.80404533457917\n"
     ]
    }
   ],
   "source": [
    "'Experiments UBER Autoencoder'\n",
    "\n",
    "'''\n",
    "Different experiments are run with the UBER Autoencoder architecture: \n",
    "\n",
    "   - variation of hidden units\n",
    "   - variation of input dimensions: matrix ('single') vs tensor ('multivariate')\n",
    "\n",
    "'''\n",
    "\n",
    "#create dict to store results, history, models...\n",
    "uber_autoencod_dict_results = {}\n",
    "\n",
    "\n",
    "#set parameters, valid for all experiments:\n",
    "start_train_set_year = '2009'\n",
    "last_train_set_year = '2010'\n",
    "validation_set_year = '2011'\n",
    "test_set_year = '2012'\n",
    "\n",
    "#area 237 training: 2009-2010, validation:2011 test:2012\n",
    "ts_series_multivar = ts_10largest[start_train_set_year:'2013'].iloc[:,:].copy() #Note: we have to set end of data set to '2013' -> this way the future values of last entry of '2012' can be extracted of 2013\n",
    "ts_series_single = ts_10largest[start_train_set_year:'2013'].iloc[:,0].copy()\n",
    "\n",
    "#set parameters for model:\n",
    "n_batch_size_autoencoder = 512\n",
    "n_batch_size_pred_model = 512\n",
    "dropout_rate = 0.3\n",
    "n_epochs = 150\n",
    "\n",
    "shuffle_flag = True\n",
    "\n",
    "stacked_encoder_flag = False\n",
    "\n",
    "early_stopping_flag = False\n",
    "\n",
    "scale_range = (-1,1) \n",
    "standardizing_flag = False\n",
    "\n",
    "\n",
    "#tuples: (n_timesteps_T, n_timesteps_F, n_preds, multivariate_flag, uber_flag, n_hidden1_units_autoencod,n_hidden2_units_autoencod,n_hidden1_units_pred,n_hidden2_units_pred,n_hidden3_units_pred)\n",
    "experiment_settings = [(168,24,1,False,True,256,128,128,64,16),\n",
    "                       (168,24,1,False,True,256,128,256,128,16),\n",
    "                       (168,24,1,False,True,512,128,128,64,16),\n",
    "                       (168,24,1,False,True,512,256,512,128,16),\n",
    "                       (168,24,10,True,True,256,128,128,64,16),\n",
    "                       (168,24,10,True,True,256,128,256,128,16),\n",
    "                       (168,24,10,True,True,512,128,128,64,16),\n",
    "                       (168,24,10,True,True,512,256,512,128,16)]\n",
    "\n",
    "                                                             \n",
    "#get model & predictions:\n",
    "for i in range(len(experiment_settings)):\n",
    "      \n",
    "    #get parameters:\n",
    "    n_timesteps_T, n_timesteps_F, n_preds, multivariate_flag, uber_flag, n_hidden1_units_autoencod,n_hidden2_units_autoencod,n_hidden1_units_pred, n_hidden2_units_pred, n_hidden3_units_pred = experiment_settings[i]\n",
    "    \n",
    "    \n",
    "    #set model_name:    \n",
    "    model_name = str(n_hidden1_units_autoencod) + '_1D_' + str(n_hidden2_units_autoencod) + '_predict_model_3H_' + str(n_hidden1_units_pred) + '_' + str(n_hidden2_units_pred) + '_' + str(n_hidden3_units_pred) + '_batch512_drop03_scaling_tanh_W' + str(n_timesteps_T)\n",
    "\n",
    "    prefix = 'UBER_auto_encoder_'\n",
    "        \n",
    "    if multivariate_flag == True:\n",
    "        postfix = '_multivariate_10largest_areas__y2012'       \n",
    "    else:\n",
    "        postfix = '_area237__y2012'\n",
    "        \n",
    "    if uber_flag == True:\n",
    "        uber_fix = '_F{}'.format(n_timesteps_F)\n",
    "    else:\n",
    "        uber_fix = ''\n",
    "        \n",
    "    \n",
    "    #final name:\n",
    "    model_name = prefix + model_name  + uber_fix + postfix\n",
    "    \n",
    "    print('### START Experiment with ', model_name)\n",
    "    \n",
    "    \n",
    "    #set dataset:\n",
    "    if multivariate_flag == True:\n",
    "        ts_series = ts_series_multivar\n",
    "    else: \n",
    "        ts_series = ts_series_single \n",
    "        \n",
    "    #add key to dict:\n",
    "    uber_autoencod_dict_results[model_name] = []\n",
    "        \n",
    "    #call function to generate model and receive predictions:\n",
    "    all_results_model_i = get_full_autoencoder_pred_model(ts_series, multivariate_flag, last_train_set_year, \n",
    "                                                          validation_set_year, test_set_year, model_name, \n",
    "                                                          n_timesteps_T, n_timesteps_F, n_preds, scale_range, \n",
    "                                                          standardizing_flag, stacked_encoder_flag, \n",
    "                                                          n_hidden1_units_autoencod, n_hidden2_units_autoencod, \n",
    "                                                          n_hidden1_units_pred, n_hidden2_units_pred, \n",
    "                                                          n_hidden3_units_pred, n_batch_size_autoencoder, \n",
    "                                                          n_batch_size_pred_model, dropout_rate, n_epochs, \n",
    "                                                          shuffle_flag, early_stopping_flag, uber_flag)      \n",
    "\n",
    "    #append results in dict:\n",
    "    uber_autoencod_dict_results[model_name].append(all_results_model_i)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store results of key:  UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "Add Actuals to df for single area...\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "Add Actuals to df for single area...\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "Add Actuals to df for single area...\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_area237__y2012\n",
      "Add Actuals to df for single area...\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_256_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_256_1D_128_predict_model_3H_256_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_512_1D_128_predict_model_3H_128_64_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Store results of key:  UBER_auto_encoder_512_1D_256_predict_model_3H_512_128_16_batch512_drop03_scaling_tanh_W168_F24_multivariate_10largest_areas__y2012\n",
      "creation of preds_df done\n",
      "creation of history_dfs done\n",
      "Save df on disk done\n",
      "Saved model to disk\n",
      "Saved model to disk\n",
      "All dfs & models of dict stored!\n"
     ]
    }
   ],
   "source": [
    "#store results of UBER Autoencoder experiments on disk:\n",
    "\n",
    "\n",
    "#prepare input_parameters:\n",
    "validation_set_year = '2011'\n",
    "test_set_year = '2012'\n",
    "\n",
    "#store results on disk:\n",
    "df_Store_PATH = '/media/...'\n",
    "Model_Store_PATH = '/media/...'\n",
    "RMSE_Store_PATH = '/media/...'\n",
    "RMSE_df_name = 'UBER_autoencoder_various_hidden_units'\n",
    "\n",
    "\n",
    "#call function to store results:\n",
    "store_results_of_dicts_on_disk(uber_autoencod_dict_results, validation_set_year, test_set_year, df_Store_PATH, Model_Store_PATH, RMSE_Store_PATH, RMSE_df_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
